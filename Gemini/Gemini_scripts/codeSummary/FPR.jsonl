{"code_tokens": "def sina_xml_to_url_list ( xml_data ) : rawurl = [ ] dom = parseString ( xml_data ) for node in dom . getElementsByTagName ( 'durl' ) : url = node . getElementsByTagName (  'url' ) [ 0 ] rawurl . append ( url . childNodes [ 0 ] . data ) return rawurl"}
{"code_tokens": "def dailymotion_download ( url , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : html = get_content ( rebuilt_url ( url ) ) info = json . loads ( match1 ( html , r'qualities\":({.+?}),\"' ) ) title = match1 ( html , r'\"video_title\"\\s*:\\s*\"([^\"]+)\"' ) or match1 ( html , r'\"title\"\\s*:\\s*\"([^\"]+)\"' ) title = unicodize ( title ) for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : try : real_url = info [ quality ] [ 1 ] [ \"url\" ] if real_url : break except KeyError : pass mime , ext , size = url_info ( real_url ) print_info ( site_info , title , mime , size ) if not info_only : download_urls (  [ real_url ] , title , ext , size , output_dir = output_dir , merge = merge )"}
{"code_tokens": "def sina_download ( url , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : if 'news.sina.com.cn/zxt' in url : sina_zxt ( url , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) return vid = match1 ( url , r'vid=(\\d+)' ) if vid is None : video_page = get_content ( url ) vid = hd_vid = match1 ( video_page , r'hd_vid\\s*:\\s*\\'([^\\']+)\\'' ) if hd_vid == '0' : vids = match1 ( video_page , r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'' ) . split ( '|' ) vid = vids [ - 1 ] if vid is  None : vid = match1 ( video_page , r'vid:\"?(\\d+)\"?' ) if vid : sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) else : vkey = match1 ( video_page , r'vkey\\s*:\\s*\"([^\"]+)\"' ) if vkey is None : vid = match1 ( url , r'#(\\d+)' ) sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) return title = match1 ( video_page , r'title\\s*:\\s*\"([^\"]+)\"' ) sina_download_by_vkey ( vkey , title = title , output_dir = output_dir , merge = merge , info_only = info_only )"}
{"code_tokens": "def sprint ( text , * colors ) : return \"\\33[{}m{content}\\33[{}m\" . format ( \";\" . join ( [ str ( color ) for color in colors ] ) , RESET , content = text ) if IS_ANSI_TERMINAL and colors else text "}
{"code_tokens": "def print_log ( text , * colors ) : sys . stderr . write ( sprint ( \"{}: {}\"  . format ( script_name , text ) , * colors ) + \"\\n\" )"}
{"code_tokens": "def e ( message , exit_code = None ) : print_log ( message , YELLOW , BOLD ) if exit_code is not None : sys .  exit ( exit_code )"}
{"code_tokens": "def wtf ( message , exit_code = 1 ) : print_log ( message , RED , BOLD )  if exit_code is not None : sys . exit ( exit_code )"}
{"code_tokens": "def detect_os ( ) : syst = system ( ) . lower ( ) os = 'unknown' if 'cygwin' in syst : os = 'cygwin' elif 'darwin' in syst : os = 'mac' elif 'linux' in syst : os = 'linux' try : with open ( '/proc/version' , 'r' ) as f : if  'microsoft' in f . read ( ) . lower ( ) : os = 'wsl' except : pass elif 'windows' in syst : os = 'windows' elif 'bsd' in syst : os = 'bsd' return os"}
{"code_tokens": "def vimeo_download_by_channel ( url , output_dir = '.' , merge = False , info_only = False , ** kwargs ) : channel_id =  match1 ( url , r'http://vimeo.com/channels/(\\w+)' ) vimeo_download_by_channel_id ( channel_id , output_dir , merge , info_only , ** kwargs )"}
{"code_tokens": "def ckplayer_get_info_by_xml ( ckinfo ) : e = ET . XML ( ckinfo ) video_dict = { 'title' : '' , 'links' : [ ] , 'size' : 0 , 'flashvars' : '' , } dictified = dictify ( e ) [ 'ckplayer' ] if 'info' in dictified : if '_text' in dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] : video_dict [ 'title' ] = dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] [ '_text' ] . strip ( ) if '_text' in dictified [ 'video' ] [ 0 ] [ 'size' ] [ 0 ] : video_dict [ 'size' ] = sum ( [ int ( i [ 'size' ] [ 0 ] [ '_text' ] ) for i in dictified [ 'video' ] ] ) if '_text' in dictified [ 'video' ] [ 0 ] [ 'file' ] [ 0 ] : video_dict [ 'links' ] = [ i  [ 'file' ] [ 0 ] [ '_text' ] . strip ( ) for i in dictified [ 'video' ] ] if '_text' in dictified [ 'flashvars' ] [ 0 ] : video_dict [ 'flashvars' ] = dictified [ 'flashvars' ] [ 0 ] [ '_text' ] . strip ( ) return video_dict"}
{"code_tokens": "def get_video_url_from_video_id ( video_id ) : data = [ \"\" ] * 256 for index , _ in enumerate ( data ) : t = index for i in range ( 8 ) : t = - 306674912 ^ unsigned_right_shitf ( t , 1 ) if 1 & t else unsigned_right_shitf ( t , 1 ) data [ index ] = t def tmp ( ) : rand_num = random . random ( ) path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\" . format ( video_id = video_id , random_num = str ( rand_num ) [ 2 : ] ) e = o = r = - 1 i , a = 0 , len ( path ) while i < a : e = ord ( path [ i ] ) i += 1 if e < 128 : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ e ) ] else : if e < 2048 : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 192 | e >> 6 & 31 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] else : if 55296 <= e < 57344 : e = ( 1023 & e ) + 64 i += 1 o = 1023 & t . url ( i ) r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 240 | e >> 8 & 7 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 2 & 63 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | o >> 6 & 15 | ( 3 & e ) << 4 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & o ) ) ] else : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 224 | e >> 12 & 15 ) ) ] r = unsigned_right_shitf ( r  , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 6 & 63 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] return \"https://ib.365yg.com{path}&s={param}\" . format ( path = path , param = unsigned_right_shitf ( r ^ - 1 , 0 ) ) while 1 : url = tmp ( ) if url . split ( \"=\" ) [ - 1 ] [ 0 ] != \"-\" : return url"}
{"code_tokens": "def get_mgtv_real_url ( url ) : content = loads ( get_content ( url ) ) m3u_url = content [ 'info' ] split = urlsplit ( m3u_url ) base_url = \"{scheme}://{netloc}{path}/\" . format ( scheme = split [ 0 ] , netloc = split [ 1 ] , path = dirname ( split [ 2 ] ) ) content = get_content ( content [ 'info' ] ) segment_list = [ ] segments_size = 0 for i in content . split ( ) : if not i . startswith ( '#' ) : segment_list . append ( base_url + i ) elif i . startswith ( '#EXT-MGTV-File-SIZE:' ) : segments_size += int ( i [ i  . rfind ( ':' ) + 1 : ] ) return m3u_url , segments_size , segment_list"}
{"code_tokens": "def legitimize ( text , os = detect_os ( ) ) : text = text . translate ( { 0 : None , ord ( '/' ) : '-' , ord ( '|' ) : '-' , } ) if os == 'windows' or os == 'cygwin' or os == 'wsl' : text = text . translate ( { ord ( ':' ) : '-' , ord ( '*' ) : '-' , ord ( '?' ) : '-' , ord ( '\\\\' ) : '-' , ord ( '\\\"' ) : '\\'' , ord ( '+' ) : '-' , ord ( '<' ) : '-' , ord ( '>' ) : '-' , ord ( '[' ) : '(' , ord ( ']' ) : ')' , ord ( '\\t' ) : ' ' , } ) else : if os == 'mac' : text = text . translate ( { ord ( ':' ) : '-' , } ) if text . startswith ( \".\" ) : text = text  [ 1 : ] text = text [ : 80 ] return text"}
{"code_tokens": "def cbs_download ( url , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : html = get_content ( url ) pid = match1 ( html , r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'' ) title = match1 ( html  , r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"' ) theplatform_download_by_pid ( pid , title , output_dir = output_dir , merge = merge , info_only = info_only )"}
{"code_tokens": "def download ( self , ** kwargs ) : if 'json_output' in kwargs and kwargs [ 'json_output' ] : json_output . output ( self ) elif 'info_only' in kwargs and kwargs [ 'info_only' ] : if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : stream_id = kwargs [ 'stream_id' ] if 'index' not in kwargs : self . p ( stream_id ) else : self . p_i ( stream_id ) else : if 'index' not in kwargs : self . p ( [ ] ) else : stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] self . p_i ( stream_id ) else : if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : stream_id = kwargs [ 'stream_id' ] else : stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] if 'index' not in kwargs : self . p ( stream_id ) else : self . p_i ( stream_id ) if stream_id in self . streams : urls = self . streams [ stream_id ] [ 'src' ] ext = self . streams [ stream_id ] [ 'container' ] total_size = self  . streams [ stream_id ] [ 'size' ] else : urls = self . dash_streams [ stream_id ] [ 'src' ] ext = self . dash_streams [ stream_id ] [ 'container' ] total_size = self . dash_streams [ stream_id ] [ 'size' ] if not urls : log . wtf ( '[Failed] Cannot extract video source.' ) download_url_ffmpeg ( urls [ 0 ] , self . title , 'mp4' , output_dir = kwargs [ 'output_dir' ] , merge = kwargs [ 'merge' ] , stream = False ) if not kwargs [ 'caption' ] : print ( 'Skipping captions.' ) return for lang in self . caption_tracks : filename = '%s.%s.srt' % ( get_filename ( self . title ) , lang ) print ( 'Saving %s ... ' % filename , end = \"\" , flush = True ) srt = self . caption_tracks [ lang ] with open ( os . path . join ( kwargs [ 'output_dir' ] , filename ) , 'w' , encoding = 'utf-8' ) as x : x . write ( srt ) print ( 'Done.' )"}
{"code_tokens": "def acfun_download_by_vid ( vid , title , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : info = json . loads ( get_content ( 'http://www.acfun.cn/video/getVideo.aspx?id=' + vid ) ) sourceType = info [ 'sourceType' ] if 'sourceId' in info : sourceId = info [ 'sourceId' ] if sourceType == 'sina' : sina_download_by_vid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'youku' : youku_download_by_vid ( sourceId , title = title , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) elif sourceType == 'tudou' : tudou_download_by_iid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'qq' : qq_download_by_vid ( sourceId , title , True , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'letv' : letvcloud_download_by_vu ( sourceId , '2d8c027396' , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'zhuzhan' : url = 'http://www.acfun.cn/v/ac' + vid yk_streams = youku_acfun_proxy ( info [ 'sourceId' ] , info [ 'encode' ] , url ) seq = [ 'mp4hd3' , 'mp4hd2' , 'mp4hd' , 'flvhd' ] for t in seq : if yk_streams . get ( t ) : preferred = yk_streams [ t ] break size = 0 for url in preferred [ 0 ] : _ , _ , seg_size = url_info ( url ) size += seg_size if re . search ( r'fid=[0-9A-Z\\-]*.flv' , preferred [ 0 ] [ 0 ] ) : ext = 'flv' else : ext = 'mp4' print_info ( site_info , title , ext , size ) if not info_only : download_urls ( preferred [ 0 ] , title , ext , size , output_dir = output_dir , merge = merge ) else : raise NotImplementedError ( sourceType ) if not info_only and not dry_run : if not kwargs [ 'caption' ] : print ( 'Skipping danmaku.' ) return try : title = get_filename ( title ) print ( 'Downloading %s ...\\n' % ( title + '.cmt.json' ) ) cmt = get_srt_json ( vid ) with open ( os . path . join ( output_dir , title + '.cmt.json' ) , 'w' , encoding = 'utf-8' )  as x : x . write ( cmt ) except : pass"}
{"code_tokens": "def matchall ( text , patterns ) : ret = [ ] for pattern in patterns : match = re . findall ( pattern ,  text ) ret += match return ret"}
{"code_tokens": "def parse_query_param ( url , param ) : try : return parse . parse_qs ( parse .  urlparse ( url ) . query ) [ param ] [ 0 ] except : return None"}
{"code_tokens": "def get_content ( url , headers = { } , decoded = True ) : logging . debug ( 'get_content: %s' % url ) req = request . Request ( url , headers = headers ) if cookies : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) response = urlopen_with_retry ( req ) data = response . read ( ) content_encoding = response . getheader ( 'Content-Encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) if decoded : charset = match1 ( response . getheader ( 'Content-Type' , '' ) , r'charset=([\\w-]+)' ) if charset is not None : data = data . decode ( charset , 'ignore'  ) else : data = data . decode ( 'utf-8' , 'ignore' ) return data"}
{"code_tokens": "def post_content ( url , headers = { } , post_data = { } , decoded = True , ** kwargs ) : if kwargs . get ( 'post_data_raw' ) : logging . debug ( 'post_content: %s\\npost_data_raw: %s' % ( url , kwargs [ 'post_data_raw' ] ) ) else : logging . debug ( 'post_content: %s\\npost_data: %s' % ( url , post_data ) ) req = request . Request ( url , headers = headers ) if cookies : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) if kwargs . get ( 'post_data_raw' ) : post_data_enc = bytes ( kwargs [ 'post_data_raw' ] , 'utf-8' ) else : post_data_enc = bytes ( parse . urlencode  ( post_data ) , 'utf-8' ) response = urlopen_with_retry ( req , data = post_data_enc ) data = response . read ( ) content_encoding = response . getheader ( 'Content-Encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) if decoded : charset = match1 ( response . getheader ( 'Content-Type' ) , r'charset=([\\w-]+)' ) if charset is not None : data = data . decode ( charset ) else : data = data . decode ( 'utf-8' ) return data"}
{"code_tokens": "def parse_host ( host ) : if re . match ( r'^(\\d+)$' , host ) is not None : return ( \"0.0.0.0\" , int ( host ) ) if re . match ( r'^(\\w+)://' , host ) is None : host = \"//\" + host o = parse . urlparse (  host ) hostname = o . hostname or \"0.0.0.0\" port = o . port or 0 return ( hostname , port )"}
{"code_tokens": "def showroom_get_roomid_by_room_url_key ( room_url_key ) : fake_headers_mobile = { 'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8' , 'Accept-Charset' : 'UTF-8,*;q=0.5' , 'Accept-Encoding' : 'gzip,deflate,sdch' , 'Accept-Language' : 'en-US,en;q=0.8' , 'User-Agent' : 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114  Mobile Safari/537.36' } webpage_url = 'https://www.showroom-live.com/' + room_url_key html = get_content ( webpage_url , headers = fake_headers_mobile ) roomid = match1 ( html , r'room\\?room_id\\=(\\d+)' ) assert roomid return roomid"}
{"code_tokens": "def _wanmen_get_title_by_json_topic_part ( json_content , tIndex , pIndex ) : return '_' . join ( [ json_content [ 0 ] [ 'name' ] , json_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'name' ] , json_content [ 0 ] [ 'Topics'  ] [ tIndex ] [ 'Parts' ] [ pIndex ] [ 'name' ] ] )"}
{"code_tokens": "def wanmen_download_by_course ( json_api_content , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : for tIndex in range ( len ( json_api_content [ 0 ] [ 'Topics' ] ) ) : for pIndex in range  ( len ( json_api_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] ) ) : wanmen_download_by_course_topic_part ( json_api_content , tIndex , pIndex , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs )"}
{"code_tokens": "def wanmen_download_by_course_topic_part ( json_api_content , tIndex , pIndex , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : html = json_api_content title = _wanmen_get_title_by_json_topic_part ( html , tIndex , pIndex ) bokeccID = _wanmen_get_boke_id_by_json_topic_part ( html , tIndex , pIndex ) bokecc_download_by_id ( vid = bokeccID , title = title , output_dir  = output_dir , merge = merge , info_only = info_only , ** kwargs )"}
{"code_tokens": "def has_task ( self , task_instance ) : if task_instance . key in  self . queued_tasks or task_instance . key in self . running : return True"}
{"code_tokens": "def get_event_buffer ( self , dag_ids = None ) : cleared_events = dict ( ) if dag_ids is None : cleared_events = self . event_buffer self . event_buffer = dict ( ) else : for key in list ( self . event_buffer . keys ( ) ) : dag_id , _  , _ , _ = key if dag_id in dag_ids : cleared_events [ key ] = self . event_buffer . pop ( key ) return cleared_events"}
{"code_tokens": "def get_conn ( self ) : conn_config = self . _get_conn_params ( ) conn = snowflake . connector . connect ( ** conn_config ) return conn "}
{"code_tokens": "def _get_aws_credentials ( self ) : if self . snowflake_conn_id : connection_object = self . get_connection ( self . snowflake_conn_id ) if 'aws_secret_access_key' in connection_object . extra_dejson  : aws_access_key_id = connection_object . extra_dejson . get ( 'aws_access_key_id' ) aws_secret_access_key = connection_object . extra_dejson . get ( 'aws_secret_access_key' ) return aws_access_key_id , aws_secret_access_key"}
{"code_tokens": "def _get_field ( self , field_name , default = None ) : full_field_name = 'extra__grpc__{}' . format ( field_name ) if full_field_name in self . extras : return self . extras  [ full_field_name ] else : return default"}
{"code_tokens": "def copy_expert ( self , sql , filename , open = open ) : if not os . path . isfile ( filename ) : with open ( filename , 'w' ) : pass with open ( filename , 'r+' ) as f : with closing ( self . get_conn ( ) ) as conn : with closing ( conn . cursor ( ) ) as cur : cur .  copy_expert ( sql , f ) f . truncate ( f . tell ( ) ) conn . commit ( )"}
{"code_tokens": "def bulk_dump ( self , table , tmp_file ) : self . copy_expert ( \"COPY {table} TO STDOUT\" . format (  table = table ) , tmp_file )"}
{"code_tokens": "def execute ( self , context ) : hook = GoogleCloudStorageHook ( google_cloud_storage_conn_id = self . google_cloud_storage_conn_id , delegate_to = self . delegate_to ) hook . upload ( bucket_name = self . bucket , object_name = self . dst  , mime_type = self . mime_type , filename = self . src , gzip = self . gzip , )"}
{"code_tokens": "def max_partition ( table , schema = \"default\" , field = None , filter_map = None , metastore_conn_id = 'metastore_default' ) : from airflow . hooks . hive_hooks import HiveMetastoreHook if '.' in table : schema , table = table . split ( '.' ) hh = HiveMetastoreHook ( metastore_conn_id = metastore_conn_id ) return hh . max_partition (  schema = schema , table_name = table , field = field , filter_map = filter_map )"}
{"code_tokens": "def get_conn ( self ) : conn = self . get_connection ( self . mysql_conn_id ) conn_config = { \"user\" : conn . login , \"passwd\" : conn . password or '' , \"host\" : conn . host or 'localhost' , \"db\" : self . schema or conn . schema or '' } if not conn . port : conn_config [ \"port\" ] = 3306 else : conn_config [ \"port\" ] = int ( conn . port ) if conn . extra_dejson . get ( 'charset' , False ) : conn_config [ \"charset\" ] = conn . extra_dejson [ \"charset\" ] if ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf8' or ( conn_config [ \"charset\" ] ) . lower ( ) == 'utf-8' : conn_config [ \"use_unicode\" ] = True if conn . extra_dejson . get ( 'cursor' , False ) : if ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'sscursor' : conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSCursor elif ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'dictcursor' : conn_config [ \"cursorclass\" ] = MySQLdb . cursors . DictCursor elif ( conn . extra_dejson [ \"cursor\" ] ) . lower ( ) == 'ssdictcursor' : conn_config [ \"cursorclass\" ] = MySQLdb . cursors . SSDictCursor local_infile = conn . extra_dejson . get ( 'local_infile' , False ) if conn . extra_dejson . get ( 'ssl' , False ) : dejson_ssl = conn . extra_dejson [ 'ssl' ] if isinstance ( dejson_ssl , six . string_types ) : dejson_ssl = json . loads ( dejson_ssl ) conn_config [ 'ssl' ] = dejson_ssl if conn . extra_dejson . get ( 'unix_socket' ) : conn_config [ 'unix_socket' ] =  conn . extra_dejson [ 'unix_socket' ] if local_infile : conn_config [ \"local_infile\" ] = 1 conn = MySQLdb . connect ( ** conn_config ) return conn"}
{"code_tokens": "def task_state ( args ) : dag = get_dag ( args ) task = dag . get_task ( task_id = args . task_id ) ti = TaskInstance ( task , args .  execution_date ) print ( ti . current_state ( ) )"}
{"code_tokens": "def restart_workers ( gunicorn_master_proc , num_workers_expected , master_timeout ) : def wait_until_true ( fn , timeout = 0 ) : t = time . time ( ) while not fn ( ) : if 0 < timeout <= time . time ( ) - t : raise AirflowWebServerTimeout ( \"No response from gunicorn master within {0} seconds\" . format ( timeout ) ) time . sleep ( 0.1 ) def start_refresh ( gunicorn_master_proc ) : batch_size = conf . getint ( 'webserver' , 'worker_refresh_batch_size' ) log . debug ( '%s doing a refresh of %s workers' , state , batch_size ) sys . stdout . flush ( ) sys . stderr . flush ( ) excess = 0 for _ in range ( batch_size ) : gunicorn_master_proc . send_signal ( signal . SIGTTIN ) excess += 1 wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) try : wait_until_true ( lambda : num_workers_expected == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) while True : num_workers_running = get_num_workers_running ( gunicorn_master_proc ) num_ready_workers_running = get_num_ready_workers_running ( gunicorn_master_proc ) state = '[{0} / {1}]' . format ( num_ready_workers_running , num_workers_running ) if num_ready_workers_running < num_workers_running : log . debug ( '%s some workers are starting up, waiting...' , state ) sys . stdout . flush ( ) time . sleep ( 1 ) elif num_workers_running > num_workers_expected : excess = num_workers_running - num_workers_expected log . debug ( '%s killing %s workers' , state , excess ) for _ in range ( excess ) : gunicorn_master_proc . send_signal ( signal . SIGTTOU ) excess -= 1 wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) elif num_workers_running == num_workers_expected : refresh_interval = conf . getint ( 'webserver' , 'worker_refresh_interval' ) log . debug ( '%s sleeping for %ss starting doing a refresh...' , state , refresh_interval ) time . sleep ( refresh_interval ) start_refresh ( gunicorn_master_proc ) else : log . error ( ( \"%s some workers seem to have died and gunicorn\" \"did not restart them as expected\" ) , state ) time . sleep ( 10 ) if len ( psutil . Process ( gunicorn_master_proc . pid ) . children ( ) ) < num_workers_expected : start_refresh ( gunicorn_master_proc ) except ( AirflowWebServerTimeout , OSError ) as err : log . error ( err ) log . error  ( \"Shutting down webserver\" ) try : gunicorn_master_proc . terminate ( ) gunicorn_master_proc . wait ( ) finally : sys . exit ( 1 )"}
{"code_tokens": "def get_conn ( self ) : if not self . _client : self . _client = Client ( credentials =  self . _get_credentials ( ) ) return self . _client"}
{"code_tokens": "def translate ( self , values , target_language , format_ = None , source_language = None , model = None ) : client = self . get_conn ( ) return client . translate ( values = values , target_language = target_language , format_ =  format_ , source_language = source_language , model = model , )"}
{"code_tokens": "def get_instance ( self , instance , project_id = None ) : return self . get_conn ( ) . instances (  ) . get ( project = project_id , instance = instance ) . execute ( num_retries = self . num_retries )"}
{"code_tokens": "def create_instance ( self , body , project_id = None ) : response = self . get_conn ( ) . instances ( ) . insert ( project = project_id , body =  body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )"}
{"code_tokens": "def patch_instance ( self , body , instance , project_id = None ) : response = self . get_conn ( ) . instances ( ) . patch ( project = project_id , instance =  instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )"}
{"code_tokens": "def delete_instance ( self , instance , project_id = None ) : response = self . get_conn ( ) . instances ( ) . delete ( project = project_id , instance = instance , )  . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )"}
{"code_tokens": "def get_database ( self , instance , database , project_id = None ) : return self . get_conn ( ) . databases ( ) . get ( project = project_id , instance = instance , database = database ) . execute ( num_retries = self  . num_retries )"}
{"code_tokens": "def create_database ( self , instance , body , project_id = None ) : response = self . get_conn ( ) . databases ( ) . insert ( project = project_id , instance = instance , body = body ) . execute (  num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )"}
{"code_tokens": "def patch_database ( self , instance , database , body , project_id = None ) : response = self . get_conn ( ) . databases ( ) . patch ( project = project_id , instance = instance , database = database ,  body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )"}
{"code_tokens": "def delete_database ( self , instance , database , project_id = None ) : response = self . get_conn ( ) . databases ( ) . delete ( project = project_id , instance = instance , database = database )  . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )"}
{"code_tokens": "def export_instance ( self , instance , body , project_id = None ) : try : response = self . get_conn ( ) . instances ( ) . export ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name  ) except HttpError as ex : raise AirflowException ( 'Exporting instance {} failed: {}' . format ( instance , ex . content ) )"}
{"code_tokens": "def start_proxy ( self ) : self . _download_sql_proxy_if_needed ( ) if self . sql_proxy_process : raise AirflowException ( \"The sql proxy is already running: {}\" . format ( self . sql_proxy_process ) ) else : command_to_run = [ self . sql_proxy_path ] command_to_run . extend ( self . command_line_parameters ) try : self . log . info ( \"Creating directory %s\" , self . cloud_sql_proxy_socket_directory ) os . makedirs ( self . cloud_sql_proxy_socket_directory ) except OSError : pass command_to_run . extend ( self . _get_credential_parameters ( ) ) self . log . info ( \"Running the command: `%s`\" , \" \" . join ( command_to_run ) ) self . sql_proxy_process = Popen ( command_to_run , stdin = PIPE , stdout = PIPE , stderr = PIPE ) self .  log . info ( \"The pid of cloud_sql_proxy: %s\" , self . sql_proxy_process . pid ) while True : line = self . sql_proxy_process . stderr . readline ( ) . decode ( 'utf-8' ) return_code = self . sql_proxy_process . poll ( ) if line == '' and return_code is not None : self . sql_proxy_process = None raise AirflowException ( \"The cloud_sql_proxy finished early with return code {}!\" . format ( return_code ) ) if line != '' : self . log . info ( line ) if \"googleapi: Error\" in line or \"invalid instance name:\" in line : self . stop_proxy ( ) raise AirflowException ( \"Error when starting the cloud_sql_proxy {}!\" . format ( line ) ) if \"Ready for new connections\" in line : return"}
{"code_tokens": "def stop_proxy ( self ) : if not self . sql_proxy_process : raise AirflowException ( \"The sql proxy is not started yet\" ) else : self . log . info ( \"Stopping the cloud_sql_proxy pid: %s\" , self . sql_proxy_process . pid ) self . sql_proxy_process . kill ( ) self . sql_proxy_process = None self . log . info ( \"Removing the socket directory: %s\" , self . cloud_sql_proxy_socket_directory ) shutil . rmtree ( self . cloud_sql_proxy_socket_directory , ignore_errors = True ) if self . sql_proxy_was_downloaded : self . log . info ( \"Removing downloaded proxy: %s\" , self . sql_proxy_path ) try : os . remove ( self . sql_proxy_path ) except OSError as e : if not e . errno == errno . ENOENT : raise else : self . log . info ( \"Skipped removing proxy -  it was not downloaded: %s\" , self . sql_proxy_path ) if os . path . isfile ( self . credentials_path ) : self . log . info ( \"Removing generated credentials file %s\" , self . credentials_path ) os . remove ( self . credentials_path )"}
{"code_tokens": "def get_proxy_version ( self ) : self . _download_sql_proxy_if_needed ( ) command_to_run = [ self . sql_proxy_path ] command_to_run . extend ( [ '--version' ] ) command_to_run . extend ( self . _get_credential_parameters ( ) ) result = subprocess . check_output ( command_to_run ) . decode ( 'utf-8' ) pattern = re . compile ( \"^.*[V|v]ersion ([^;]*);.*$\" ) m = pattern . match ( result ) if m : return m . group ( 1 ) else : return None "}
{"code_tokens": "def create_connection ( self , session = None ) : connection = Connection ( conn_id = self . db_conn_id ) uri = self . _generate_connection_uri ( ) self . log . info ( \"Creating connection %s\" , self . db_conn_id ) connection . parse_from_uri ( uri ) session . add (  connection ) session . commit ( )"}
{"code_tokens": "def retrieve_connection ( self , session = None ) : self . log . info ( \"Retrieving connection %s\" , self . db_conn_id ) connections = session . query ( Connection ) . filter ( Connection . conn_id == self . db_conn_id ) if connections  . count ( ) : return connections [ 0 ] return None"}
{"code_tokens": "def delete_connection ( self , session = None ) : self . log . info ( \"Deleting connection %s\" , self . db_conn_id ) connections = session . query ( Connection ) . filter ( Connection . conn_id == self . db_conn_id ) if connections . count ( ) : connection = connections [ 0 ] session . delete ( connection ) session . commit ( ) else : self . log . info ( \"Connection was already  deleted!\" )"}
{"code_tokens": "def get_sqlproxy_runner ( self ) : if not self . use_proxy : raise AirflowException ( \"Proxy runner can only be retrieved in case of use_proxy = True\" ) return CloudSqlProxyRunner ( path_prefix = self . sql_proxy_unique_path , instance_specification = self . _get_sqlproxy_instance_specification ( )  , project_id = self . project_id , sql_proxy_version = self . sql_proxy_version , sql_proxy_binary_path = self . sql_proxy_binary_path )"}
{"code_tokens": "def get_database_hook ( self ) : if self . database_type == 'postgres' : self . db_hook = PostgresHook ( postgres_conn_id = self . db_conn_id , schema = self . database ) else : self . db_hook = MySqlHook ( mysql_conn_id = self .  db_conn_id , schema = self . database ) return self . db_hook"}
{"code_tokens": "def cleanup_database_hook ( self ) : if self . database_type == 'postgres' : if hasattr ( self . db_hook , 'conn' ) and self . db_hook . conn and self .  db_hook . conn . notices : for output in self . db_hook . conn . notices : self . log . info ( output )"}
{"code_tokens": "def reserve_free_tcp_port ( self ) : self . reserved_tcp_socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) self . reserved_tcp_socket . bind ( ( '127.0.0.1' , 0 ) ) self . sql_proxy_tcp_port = self . reserved_tcp_socket . getsockname ( ) [ 1  ]"}
{"code_tokens": "def _normalize_mlengine_job_id ( job_id ) : match = re . search ( r'\\d|\\{{2}' , job_id ) if match and match . start ( ) == 0 : job = 'z_{}' . format ( job_id ) else : job = job_id tracker = 0 cleansed_job_id = '' for m in re . finditer ( r'\\{{2}.+?\\}{2}' , job ) : cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : m .  start ( ) ] ) cleansed_job_id += job [ m . start ( ) : m . end ( ) ] tracker = m . end ( ) cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : ] ) return cleansed_job_id"}
{"code_tokens": "def _get_error_code ( self , e ) : try : matches = self . error_code_pattern . match ( str ( e ) ) code = int ( matches . group (  0 ) ) return code except ValueError : return e"}
{"code_tokens": "def clear_dag_runs ( ) : session = settings . Session ( ) drs = session . query ( DagRun ) . filter ( DagRun . dag_id . in_ ( DAG_IDS ) , ) . all ( ) for dr in drs : logging . info ( 'Deleting DagRun :: {}' . format ( dr ) ) session .  delete ( dr )"}
{"code_tokens": "def clear_dag_task_instances ( ) : session = settings . Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) for ti  in tis : logging . info ( 'Deleting TaskInstance :: {}' . format ( ti ) ) session . delete ( ti ) session . commit ( )"}
{"code_tokens": "def set_dags_paused_state ( is_paused ) : session = settings . Session ( ) dms = session . query ( DagModel ) . filter ( DagModel . dag_id . in_ ( DAG_IDS ) ) for  dm in dms : logging . info ( 'Setting DAG :: {} is_paused={}' . format ( dm , is_paused ) ) dm . is_paused = is_paused session . commit ( )"}
{"code_tokens": "def print_stats ( self ) : session = settings . Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) successful_tis = [ x for x in tis if x . state == State . SUCCESS ] ti_perf = [ ( ti . dag_id , ti . task_id , ti . execution_date , ( ti . queued_dttm - self . start_date ) . total_seconds ( ) , ( ti . start_date - self . start_date ) . total_seconds ( ) , ( ti . end_date - self . start_date ) . total_seconds ( ) , ti . duration ) for ti in successful_tis ] ti_perf_df = pd . DataFrame ( ti_perf , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'queue_delay' , 'start_delay' , 'land_time' , 'duration' ] ) print ( 'Performance Results' )  print ( '###################' ) for dag_id in DAG_IDS : print ( 'DAG {}' . format ( dag_id ) ) print ( ti_perf_df [ ti_perf_df [ 'dag_id' ] == dag_id ] ) print ( '###################' ) if len ( tis ) > len ( successful_tis ) : print ( \"WARNING!! The following task instances haven't completed\" ) print ( pd . DataFrame ( [ ( ti . dag_id , ti . task_id , ti . execution_date , ti . state ) for ti in filter ( lambda x : x . state != State . SUCCESS , tis ) ] , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'state' ] ) ) session . commit ( )"}
{"code_tokens": "def heartbeat ( self ) : super ( SchedulerMetricsJob , self ) . heartbeat ( ) session = settings . Session ( ) TI = TaskInstance successful_tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . filter ( TI . state . in_ ( [ State . SUCCESS ] ) ) . all ( ) ) session . commit ( ) dagbag = DagBag ( SUBDIR ) dags = [ dagbag . dags [ dag_id ] for dag_id in DAG_IDS ] num_task_instances = sum ( [ ( timezone . utcnow ( ) - task . start_date ) . days for dag in dags for task in dag . tasks ] ) if ( len ( successful_tis ) == num_task_instances or ( timezone . utcnow ( ) - self . start_date ) . total_seconds ( ) > MAX_RUNTIME_SECS ) : if len ( successful_tis ) == num_task_instances : self . log . info ( \"All tasks processed! Printing stats.\" ) else  : self . log . info ( \"Test timeout reached. Printing available stats.\" ) self . print_stats ( ) set_dags_paused_state ( True ) sys . exit ( )"}
{"code_tokens": "def invoke_lambda ( self , payload ) : awslambda_conn = self . get_conn ( ) response = awslambda_conn . invoke ( FunctionName = self . function_name , InvocationType = self . invocation_type , LogType = self . log_type , Payload = payload , Qualifier =  self . qualifier ) return response"}
{"code_tokens": "def create_evaluate_ops ( task_prefix , data_format , input_paths , prediction_path , metric_fn_and_keys , validate_fn , batch_prediction_job_id = None , project_id = None , region = None , dataflow_options = None , model_uri = None , model_name = None , version_name = None , dag = None ) : if not re . match ( r\"^[a-zA-Z][-A-Za-z0-9]*$\" , task_prefix ) : raise AirflowException ( \"Malformed task_id for DataFlowPythonOperator (only alphanumeric \" \"and hyphens are allowed but got: \" + task_prefix ) metric_fn , metric_keys = metric_fn_and_keys if not callable ( metric_fn ) : raise AirflowException ( \"`metric_fn` param must be callable.\" ) if not callable ( validate_fn ) : raise AirflowException ( \"`validate_fn` param must be callable.\" ) if dag is not None and dag . default_args is not None : default_args = dag . default_args project_id = project_id or default_args . get ( 'project_id' ) region = region or default_args . get ( 'region' ) model_name = model_name or default_args . get ( 'model_name' ) version_name = version_name or default_args . get ( 'version_name' ) dataflow_options = dataflow_options or default_args . get ( 'dataflow_default_options' ) evaluate_prediction = MLEngineBatchPredictionOperator ( task_id = ( task_prefix + \"-prediction\" ) , project_id = project_id , job_id = batch_prediction_job_id , region = region , data_format = data_format , input_paths = input_paths , output_path = prediction_path , uri = model_uri , model_name = model_name , version_name = version_name , dag = dag  ) metric_fn_encoded = base64 . b64encode ( dill . dumps ( metric_fn , recurse = True ) ) evaluate_summary = DataFlowPythonOperator ( task_id = ( task_prefix + \"-summary\" ) , py_options = [ \"-m\" ] , py_file = \"airflow.contrib.utils.mlengine_prediction_summary\" , dataflow_default_options = dataflow_options , options = { \"prediction_path\" : prediction_path , \"metric_fn_encoded\" : metric_fn_encoded , \"metric_keys\" : ',' . join ( metric_keys ) } , dag = dag ) evaluate_summary . set_upstream ( evaluate_prediction ) def apply_validate_fn ( * args , ** kwargs ) : prediction_path = kwargs [ \"templates_dict\" ] [ \"prediction_path\" ] scheme , bucket , obj , _ , _ = urlsplit ( prediction_path ) if scheme != \"gs\" or not bucket or not obj : raise ValueError ( \"Wrong format prediction_path: %s\" , prediction_path ) summary = os . path . join ( obj . strip ( \"/\" ) , \"prediction.summary.json\" ) gcs_hook = GoogleCloudStorageHook ( ) summary = json . loads ( gcs_hook . download ( bucket , summary ) ) return validate_fn ( summary ) evaluate_validation = PythonOperator ( task_id = ( task_prefix + \"-validation\" ) , python_callable = apply_validate_fn , provide_context = True , templates_dict = { \"prediction_path\" : prediction_path } , dag = dag ) evaluate_validation . set_upstream ( evaluate_summary ) return evaluate_prediction , evaluate_summary , evaluate_validation"}
{"code_tokens": "def mkdirs ( path , mode ) : try : o_umask = os . umask ( 0 ) os . makedirs ( path , mode ) except OSError : if not os . path . isdir ( path ) :  raise finally : os . umask ( o_umask )"}
{"code_tokens": "def _convert_to_float_if_possible ( s ) : try : ret = float ( s ) except ( ValueError , TypeError )  : ret = s return ret"}
{"code_tokens": "def make_aware ( value , timezone = None ) : if timezone is None : timezone = TIMEZONE if is_localized ( value ) : raise ValueError ( \"make_aware expects a naive datetime, got %s\" % value ) if hasattr ( value , 'fold' ) : value = value . replace ( fold = 1 ) if hasattr ( timezone , 'localize' ) : return timezone . localize ( value ) elif  hasattr ( timezone , 'convert' ) : return timezone . convert ( value ) else : return value . replace ( tzinfo = timezone )"}
{"code_tokens": "def make_naive ( value , timezone = None ) : if timezone is None : timezone = TIMEZONE if is_naive ( value ) : raise ValueError ( \"make_naive() cannot be applied to a naive datetime\" ) o = value . astimezone ( timezone ) naive = dt . datetime ( o . year , o . month , o . day , o . hour , o . minute , o . second , o . microsecond ) return  naive"}
{"code_tokens": "def datetime ( * args , ** kwargs ) : if 'tzinfo' not in kwargs : kwargs [ 'tzinfo' ] = TIMEZONE return dt . datetime ( * args ,  ** kwargs )"}
{"code_tokens": "def get_conn ( self ) : conn = self . get_connection ( self . druid_broker_conn_id ) druid_broker_conn = connect ( host = conn . host , port = conn . port , path = conn . extra_dejson . get ( 'endpoint' , '/druid/v2/sql' ) , scheme  = conn . extra_dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to druid broker on %s' , conn . host ) return druid_broker_conn"}
{"code_tokens": "def get_conn ( self , headers = None ) : session = requests . Session ( ) if self . http_conn_id : conn = self . get_connection ( self . http_conn_id ) if \"://\" in conn . host : self . base_url = conn . host else : schema = conn . schema if conn . schema else \"http\" self . base_url = schema + \"://\" + conn . host if conn . port : self . base_url = self . base_url + \":\" + str ( conn . port ) if conn . login : session . auth = ( conn . login , conn . password ) if conn . extra : try : session . headers . update ( conn . extra_dejson ) except TypeError : self . log . warn ( 'Connection to %s has invalid extra field.' ,  conn . host ) if headers : session . headers . update ( headers ) return session"}
{"code_tokens": "def run ( self , endpoint , data = None , headers = None , extra_options = None ) : extra_options = extra_options or { } session = self . get_conn ( headers ) if self . base_url and not self . base_url . endswith ( '/' ) and endpoint and not endpoint . startswith ( '/' ) : url = self . base_url + '/' + endpoint else : url = ( self . base_url or '' ) + ( endpoint or '' ) req = None if self . method == 'GET' : req = requests . Request ( self . method , url , params = data , headers = headers ) elif self . method == 'HEAD' : req = requests . Request ( self . method , url , headers = headers ) else : req = requests . Request ( self . method , url , data = data , headers = headers ) prepped_request = session . prepare_request ( req ) self . log . info ( \"Sending '%s' to url: %s\" , self . method , url )  return self . run_and_check ( session , prepped_request , extra_options )"}
{"code_tokens": "def check_response ( self , response ) : try : response . raise_for_status ( ) except requests . exceptions . HTTPError : self . log . error ( \"HTTP error: %s\" , response . reason ) if self . method not in [ 'GET' , 'HEAD' ] : self . log . error ( response  . text ) raise AirflowException ( str ( response . status_code ) + \":\" + response . reason )"}
{"code_tokens": "def run_and_check ( self , session , prepped_request , extra_options ) : extra_options = extra_options or { } try : response = session . send ( prepped_request , stream = extra_options . get ( \"stream\" , False ) , verify = extra_options . get ( \"verify\" , True ) , proxies = extra_options . get ( \"proxies\" , { } ) , cert = extra_options . get ( \"cert\" ) , timeout = extra_options . get ( \"timeout\" ) ,  allow_redirects = extra_options . get ( \"allow_redirects\" , True ) ) if extra_options . get ( 'check_response' , True ) : self . check_response ( response ) return response except requests . exceptions . ConnectionError as ex : self . log . warn ( str ( ex ) + ' Tenacity will retry to execute the operation' ) raise ex"}
{"code_tokens": "def create_session ( ) : session = settings . Session ( ) try : yield session session . commit ( ) except Exception : session . rollback ( ) raise finally : session . close (  )"}
{"code_tokens": "def provide_session ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : arg_session = 'session' func_params = func . __code__ . co_varnames session_in_args = arg_session in func_params and func_params . index ( arg_session ) < len ( args ) session_in_kwargs = arg_session in kwargs if session_in_kwargs or session_in_args : return func ( * args , ** kwargs ) else : with create_session ( ) as session : kwargs [ arg_session ] = session return func (  * args , ** kwargs ) return wrapper"}
{"code_tokens": "def resetdb ( ) : from airflow import models from alembic . migration import MigrationContext log . info ( \"Dropping tables that exist\" ) models . base . Base . metadata . drop_all ( settings . engine ) mc = MigrationContext . configure ( settings . engine ) if mc . _version . exists ( settings . engine ) : mc . _version . drop  ( settings . engine ) from flask_appbuilder . models . sqla import Base Base . metadata . drop_all ( settings . engine ) initdb ( )"}
{"code_tokens": "def _get_pretty_exception_message ( e ) : if ( hasattr ( e , 'message' ) and 'errorName' in e . message and 'message' in e . message ) : return ( '{name}: {message}' . format ( name = e . message [ 'errorName' ] , message = e . message [ 'message' ] ) ) else : return str ( e  )"}
{"code_tokens": "def get_records ( self , hql , parameters = None ) : try : return super ( ) . get_records ( self . _strip_sql ( hql ) , parameters ) except DatabaseError as e : raise PrestoException ( self . _get_pretty_exception_message ( e ) ) "}
{"code_tokens": "def get_pandas_df ( self , hql , parameters = None ) : import pandas cursor = self . get_cursor ( ) try : cursor . execute ( self . _strip_sql ( hql ) , parameters ) data = cursor . fetchall ( ) except DatabaseError as e : raise PrestoException ( self . _get_pretty_exception_message ( e ) ) column_descriptions = cursor . description if data : df = pandas . DataFrame ( data ) df . columns = [ c [ 0 ]  for c in column_descriptions ] else : df = pandas . DataFrame ( ) return df"}
{"code_tokens": "def run ( self , hql , parameters = None ) : return super ( ) . run ( self . _strip_sql ( hql ) , parameters  )"}
{"code_tokens": "def insert_rows ( self , table , rows , target_fields = None ) : super ( ) . insert_rows ( table , rows  , target_fields , 0 )"}
{"code_tokens": "def get_conn ( self ) : if self . cosmos_client is not None : return self . cosmos_client self . cosmos_client = cosmos_client . CosmosClient ( self  . endpoint_uri , { 'masterKey' : self . master_key } ) return self . cosmos_client"}
{"code_tokens": "def does_collection_exist ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" :  collection_name } ] } ) ) if len ( existing_container ) == 0 : return False return True"}
{"code_tokens": "def create_collection ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" :  \"@id\" , \"value\" : collection_name } ] } ) ) if len ( existing_container ) == 0 : self . get_conn ( ) . CreateContainer ( get_database_link ( self . __get_database_name ( database_name ) ) , { \"id\" : collection_name } )"}
{"code_tokens": "def does_database_exist ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) existing_database = list ( self . get_conn ( ) . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" ,  \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database_name } ] } ) ) if len ( existing_database ) == 0 : return False return True"}
{"code_tokens": "def create_database ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) existing_database = list ( self . get_conn ( ) . QueryDatabases ( { \"query\" : \"SELECT * FROM r WHERE r.id=@id\" , \"parameters\" : [ { \"name\" : \"@id\" , \"value\" : database_name } ] } ) ) if len (  existing_database ) == 0 : self . get_conn ( ) . CreateDatabase ( { \"id\" : database_name } )"}
{"code_tokens": "def delete_database ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( \"Database name cannot be None.\" ) self . get_conn  ( ) . DeleteDatabase ( get_database_link ( database_name ) )"}
{"code_tokens": "def delete_collection ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( \"Collection name cannot be None.\" ) self . get_conn ( )  . DeleteContainer ( get_collection_link ( self . __get_database_name ( database_name ) , collection_name ) )"}
{"code_tokens": "def insert_documents ( self , documents , database_name = None , collection_name = None ) : if documents is None : raise AirflowBadRequest ( \"You cannot insert empty documents\" ) created_documents = [ ] for single_document in documents : created_documents . append ( self . get_conn  ( ) . CreateItem ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , single_document ) ) return created_documents"}
{"code_tokens": "def delete_document ( self , document_id , database_name = None , collection_name = None ) : if document_id is None : raise AirflowBadRequest ( \"Cannot delete a document without an id\" ) self . get_conn ( )  . DeleteItem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) )"}
{"code_tokens": "def get_document ( self , document_id , database_name = None , collection_name = None ) : if document_id is None : raise AirflowBadRequest ( \"Cannot get a document without an id\" ) try : return self . get_conn ( ) . ReadItem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name  ) , document_id ) ) except HTTPFailure : return None"}
{"code_tokens": "def get_documents ( self , sql_string , database_name = None , collection_name = None , partition_key = None ) : if sql_string is None : raise AirflowBadRequest ( \"SQL query string cannot be None\" ) query = { 'query' : sql_string } try : result_iterable = self . get_conn ( ) . QueryItems ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name  ( collection_name ) ) , query , partition_key ) return list ( result_iterable ) except HTTPFailure : return None"}
{"code_tokens": "def get_function ( self , name ) : return self . get_conn ( ) . projects ( ) . locations ( ) . functions ( )  . get ( name = name ) . execute ( num_retries = self . num_retries )"}
{"code_tokens": "def create_new_function ( self , location , body , project_id = None ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . create ( location = self . _full_location ( project_id , location ) , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( operation_name  = operation_name )"}
{"code_tokens": "def update_function ( self , name , body , update_mask ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . patch ( updateMask = \",\" . join ( update_mask ) , name = name , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete ( operation_name  = operation_name )"}
{"code_tokens": "def upload_function_zip ( self , location , zip_path , project_id = None ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . generateUploadUrl ( parent = self . _full_location ( project_id , location ) ) . execute ( num_retries = self . num_retries ) upload_url = response . get ( 'uploadUrl' ) with open ( zip_path , 'rb' ) as fp : requests . put ( url = upload_url , data = fp , headers = { 'Content-type'  : 'application/zip' , 'x-goog-content-length-range' : '0,104857600' , } ) return upload_url"}
{"code_tokens": "def delete_function ( self , name ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . delete ( name = name ) . execute ( num_retries = self . num_retries ) operation_name = response [ \"name\" ] self . _wait_for_operation_to_complete  ( operation_name = operation_name )"}
{"code_tokens": "def get_dep_statuses ( self , ti , session , dep_context = None ) : from airflow . ti_deps . dep_context import DepContext if dep_context is None : dep_context = DepContext ( ) if self . IGNOREABLE and dep_context . ignore_all_deps : yield self . _passing_status ( reason = \"Context specified all dependencies should  be ignored.\" ) return if self . IS_TASK_DEP and dep_context . ignore_task_deps : yield self . _passing_status ( reason = \"Context specified all task dependencies should be ignored.\" ) return for dep_status in self . _get_dep_statuses ( ti , session , dep_context ) : yield dep_status"}
{"code_tokens": "def is_met ( self , ti , session , dep_context = None ) : return all ( status .  passed for status in self . get_dep_statuses ( ti , session , dep_context ) )"}
{"code_tokens": "def get_failure_reasons ( self , ti , session , dep_context = None ) : for dep_status in self . get_dep_statuses  ( ti , session , dep_context ) : if not dep_status . passed : yield dep_status . reason"}
{"code_tokens": "def _parse_s3_config ( config_file_name , config_format = 'boto' , profile = None ) : config = configparser . ConfigParser ( ) if config . read ( config_file_name ) : sections = config . sections ( ) else : raise AirflowException ( \"Couldn't read {0}\" . format ( config_file_name ) ) if config_format is None : config_format = 'boto' conf_format = config_format . lower ( ) if conf_format == 'boto' : if profile is not None and 'profile ' + profile in sections : cred_section = 'profile ' + profile else : cred_section = 'Credentials' elif conf_format == 'aws' and profile is not None : cred_section = profile else : cred_section = 'default' if conf_format in ( 'boto' , 'aws' ) : key_id_option = 'aws_access_key_id' secret_key_option = 'aws_secret_access_key' else : key_id_option = 'access_key' secret_key_option = 'secret_key' if cred_section not in sections : raise AirflowException ( \"This config file format is not recognized\" ) else : try  : access_key = config . get ( cred_section , key_id_option ) secret_key = config . get ( cred_section , secret_key_option ) except Exception : logging . warning ( \"Option Error in parsing s3 config file\" ) raise return access_key , secret_key"}
{"code_tokens": "def get_credentials ( self , region_name = None ) : session , _ = self  . _get_credentials ( region_name ) return session . get_credentials ( ) . get_frozen_credentials ( )"}
{"code_tokens": "def get_conn ( self ) : conn = self . get_connection ( self . vertica_conn_id ) conn_config = { \"user\" : conn . login , \"password\" : conn . password or '' , \"database\" : conn . schema , \"host\" : conn . host or 'localhost' } if not conn . port :  conn_config [ \"port\" ] = 5433 else : conn_config [ \"port\" ] = int ( conn . port ) conn = connect ( ** conn_config ) return conn"}
{"code_tokens": "def flush ( self ) : if len ( self . _buffer ) > 0 : self . logger . log ( self . level , self . _buffer ) self . _buffer = str (  )"}
{"code_tokens": "def correct_maybe_zipped ( fileloc ) : _ , archive , filename = re . search ( r'((.*\\.zip){})?(.*)' . format ( re . escape ( os . sep ) ) , fileloc ) . groups ( ) if archive  and zipfile . is_zipfile ( archive ) : return archive else : return fileloc"}
{"code_tokens": "def list_py_file_paths ( directory , safe_mode = True , include_examples = None ) : if include_examples is None : include_examples = conf . getboolean ( 'core' , 'LOAD_EXAMPLES' ) file_paths = [ ] if directory is None : return [ ] elif os . path . isfile ( directory ) : return [ directory ] elif os . path . isdir ( directory ) : patterns_by_dir = { } for root , dirs , files in os . walk ( directory , followlinks = True ) : patterns = patterns_by_dir . get ( root , [ ] ) ignore_file = os . path . join ( root , '.airflowignore' ) if os . path . isfile ( ignore_file ) : with open ( ignore_file , 'r' ) as f : patterns += [ re . compile ( p ) for p in f . read ( ) . split ( '\\n' ) if p ] dirs [ : ] = [ d for d in dirs if not any ( p . search ( os . path . join ( root , d ) ) for p in patterns ) ] for d in dirs : patterns_by_dir [ os . path . join ( root , d ) ] = patterns for f in files :  try : file_path = os . path . join ( root , f ) if not os . path . isfile ( file_path ) : continue mod_name , file_ext = os . path . splitext ( os . path . split ( file_path ) [ - 1 ] ) if file_ext != '.py' and not zipfile . is_zipfile ( file_path ) : continue if any ( [ re . findall ( p , file_path ) for p in patterns ] ) : continue might_contain_dag = True if safe_mode and not zipfile . is_zipfile ( file_path ) : with open ( file_path , 'rb' ) as fp : content = fp . read ( ) might_contain_dag = all ( [ s in content for s in ( b'DAG' , b'airflow' ) ] ) if not might_contain_dag : continue file_paths . append ( file_path ) except Exception : log = LoggingMixin ( ) . log log . exception ( \"Error while examining %s\" , f ) if include_examples : import airflow . example_dags example_dag_folder = airflow . example_dags . __path__ [ 0 ] file_paths . extend ( list_py_file_paths ( example_dag_folder , safe_mode , False ) ) return file_paths"}
{"code_tokens": "def construct_task_instance ( self , session = None , lock_for_update = False ) : TI = airflow . models . TaskInstance qry = session . query ( TI ) . filter ( TI . dag_id == self . _dag_id , TI . task_id == self . _task_id , TI . execution_date == self . _execution_date ) if lock_for_update  : ti = qry . with_for_update ( ) . first ( ) else : ti = qry . first ( ) return ti"}
{"code_tokens": "def start ( self ) : self . _process = self . _launch_process ( self . _dag_directory , self . _file_paths , self . _max_runs , self . _processor_factory , self . _child_signal_conn , self .  _stat_queue , self . _result_queue , self . _async_mode ) self . log . info ( \"Launched DagFileProcessorManager with pid: %s\" , self . _process . pid )"}
{"code_tokens": "def terminate ( self ) : self . log . info ( \"Sending termination message to manager.\" ) self . _child_signal_conn . send ( DagParsingSignal . TERMINATE_MANAGER  )"}
{"code_tokens": "def _exit_gracefully ( self , signum , frame ) : self . log . info ( \"Exiting gracefully upon receiving signal %s\" , signum ) self . terminate ( ) self . end ( ) self . log . debug ( \"Finished terminating DAG processors.\"  ) sys . exit ( os . EX_OK )"}
{"code_tokens": "def start ( self ) : self . log . info ( \"Processing files using up to %s processes at a time \" , self . _parallelism ) self . log . info ( \"Process each file at most once every %s seconds\" , self . _file_process_interval ) self . log . info ( \"Checking for new files in %s every %s seconds\" , self . _dag_directory , self . dag_dir_list_interval ) if self . _async_mode : self . log . debug ( \"Starting DagFileProcessorManager in async mode\" )  self . start_in_async ( ) else : self . log . debug ( \"Starting DagFileProcessorManager in sync mode\" ) self . start_in_sync ( )"}
{"code_tokens": "def start_in_async ( self ) : while True : loop_start_time = time . time ( ) if self . _signal_conn . poll ( ) : agent_signal = self . _signal_conn . recv ( ) if agent_signal == DagParsingSignal . TERMINATE_MANAGER : self . terminate ( ) break elif agent_signal == DagParsingSignal . END_MANAGER : self . end ( ) sys . exit ( os . EX_OK ) self . _refresh_dag_dir ( ) simple_dags = self . heartbeat ( ) for simple_dag in simple_dags : self . _result_queue . put ( simple_dag ) self . _print_stat ( ) all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) max_runs_reached = self . max_runs_reached ( ) dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , max_runs_reached , all_files_processed , len ( simple_dags ) ) self . _stat_queue . put ( dag_parsing_stat ) if max_runs_reached : self . log . info ( \"Exiting dag parsing loop as all files \" \"have been processed %s times\" , self . _max_runs ) break loop_duration = time . time ( ) - loop_start_time if loop_duration < 1 : sleep_length = 1 - loop_duration self . log . debug ( \"Sleeping  for %.2f seconds to prevent excessive logging\" , sleep_length ) time . sleep ( sleep_length )"}
{"code_tokens": "def start_in_sync ( self ) : while True : agent_signal = self . _signal_conn . recv ( ) if agent_signal == DagParsingSignal . TERMINATE_MANAGER : self . terminate ( ) break elif agent_signal == DagParsingSignal . END_MANAGER : self . end ( ) sys . exit ( os . EX_OK ) elif agent_signal == DagParsingSignal . AGENT_HEARTBEAT : self . _refresh_dag_dir ( ) simple_dags = self . heartbeat ( ) for simple_dag in simple_dags : self . _result_queue . put ( simple_dag ) self . _print_stat ( ) all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) max_runs_reached = self . max_runs_reached ( ) dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , self . max_runs_reached ( ) , all_files_processed , len ( simple_dags ) ) self . _stat_queue . put ( dag_parsing_stat ) self . wait_until_finished ( ) self . _signal_conn . send ( DagParsingSignal . MANAGER_DONE ) if max_runs_reached : self . log . info ( \"Exiting dag parsing loop as all files \" \"have been processed %s times\" , self . _max_runs ) self . _signal_conn . send ( DagParsingSignal . MANAGER_DONE )  break"}
{"code_tokens": "def _refresh_dag_dir ( self ) : elapsed_time_since_refresh = ( timezone . utcnow ( ) - self . last_dag_dir_refresh_time ) . total_seconds ( ) if elapsed_time_since_refresh > self . dag_dir_list_interval : self . log . info ( \"Searching for files in %s\" , self . _dag_directory ) self . _file_paths = list_py_file_paths ( self . _dag_directory ) self . last_dag_dir_refresh_time = timezone . utcnow ( ) self . log . info ( \"There are %s files in %s\" , len ( self . _file_paths ) , self . _dag_directory ) self . set_file_paths ( self . _file_paths ) try : self . log .  debug ( \"Removing old import errors\" ) self . clear_nonexistent_import_errors ( ) except Exception : self . log . exception ( \"Error removing old import errors\" )"}
{"code_tokens": "def _print_stat ( self ) : if ( ( timezone . utcnow ( ) - self . last_stat_print_time ) . total_seconds ( ) > self . print_stats_interval ) : if len ( self . _file_paths ) > 0  : self . _log_file_processing_stats ( self . _file_paths ) self . last_stat_print_time = timezone . utcnow ( )"}
{"code_tokens": "def clear_nonexistent_import_errors ( self , session ) : query = session . query ( errors . ImportError ) if self . _file_paths : query = query . filter ( ~ errors . ImportError . filename .  in_ ( self . _file_paths ) ) query . delete ( synchronize_session = 'fetch' ) session . commit ( )"}
{"code_tokens": "def _log_file_processing_stats ( self , known_file_paths ) : headers = [ \"File Path\" , \"PID\" , \"Runtime\" , \"Last Runtime\" , \"Last Run\" ] rows = [ ] for file_path in known_file_paths : last_runtime = self . get_last_runtime ( file_path ) file_name = os . path . basename ( file_path ) file_name = os . path . splitext ( file_name ) [ 0 ] . replace ( os . sep , '.' ) if last_runtime : Stats . gauge ( 'dag_processing.last_runtime.{}' . format ( file_name ) , last_runtime ) processor_pid = self . get_pid ( file_path ) processor_start_time = self . get_start_time ( file_path ) runtime = ( ( timezone . utcnow ( ) - processor_start_time ) . total_seconds ( ) if processor_start_time else None ) last_run = self . get_last_finish_time ( file_path ) if last_run : seconds_ago = ( timezone . utcnow ( ) - last_run ) . total_seconds ( ) Stats . gauge ( 'dag_processing.last_run.seconds_ago.{}' . format ( file_name ) , seconds_ago ) rows . append ( ( file_path , processor_pid , runtime , last_runtime , last_run ) ) rows = sorted ( rows , key = lambda x : x [ 3 ] or 0.0 ) formatted_rows = [ ] for file_path , pid , runtime , last_runtime , last_run in rows : formatted_rows . append ( ( file_path , pid , \"{:.2f}s\" . format ( runtime ) if runtime else None , \"{:.2f}s\" . format ( last_runtime ) if last_runtime else None , last_run . strftime ( \"%Y-%m-%dT%H:%M:%S\" ) if last_run else None ) )  log_str = ( \"\\n\" + \"=\" * 80 + \"\\n\" + \"DAG File Processing Stats\\n\\n\" + tabulate ( formatted_rows , headers = headers ) + \"\\n\" + \"=\" * 80 ) self . log . info ( log_str )"}
{"code_tokens": "def set_file_paths ( self , new_file_paths ) : self . _file_paths = new_file_paths self . _file_path_queue = [ x for x in self . _file_path_queue if x in new_file_paths ] filtered_processors = { } for file_path , processor in self . _processors . items ( ) : if file_path in new_file_paths : filtered_processors [ file_path ] = processor else : self . log . warning ( \"Stopping processor for %s\"  , file_path ) processor . terminate ( ) self . _processors = filtered_processors"}
{"code_tokens": "def wait_until_finished ( self ) : for file_path , processor in self . _processors .  items ( ) : while not processor . done : time . sleep ( 0.1 )"}
{"code_tokens": "def heartbeat ( self ) : finished_processors = { } running_processors = { } for file_path , processor in self . _processors . items ( ) : if processor . done : self . log . debug ( \"Processor for %s finished\" , file_path ) now = timezone . utcnow ( ) finished_processors [ file_path ] = processor self . _last_runtime [ file_path ] = ( now - processor . start_time ) . total_seconds ( ) self . _last_finish_time [ file_path ] = now self . _run_count [ file_path ] += 1 else : running_processors [ file_path ] = processor self . _processors = running_processors self . log . debug ( \"%s/%s DAG parsing processes running\" , len ( self . _processors ) , self . _parallelism ) self . log . debug ( \"%s file paths queued for processing\" , len ( self . _file_path_queue ) ) simple_dags = [ ] for file_path , processor in finished_processors . items ( ) : if processor . result is None : self . log . warning ( \"Processor for %s exited with return code %s.\" , processor . file_path , processor . exit_code ) else : for simple_dag in processor . result : simple_dags . append ( simple_dag ) if len ( self . _file_path_queue ) == 0 : file_paths_in_progress = self . _processors . keys ( ) now = timezone . utcnow ( ) file_paths_recently_processed = [ ] for file_path in self . _file_paths : last_finish_time = self . get_last_finish_time ( file_path ) if ( last_finish_time is not None and ( now - last_finish_time ) . total_seconds ( ) < self . _file_process_interval ) : file_paths_recently_processed . append ( file_path ) files_paths_at_run_limit = [ file_path for file_path , num_runs in self . _run_count . items ( ) if num_runs == self . _max_runs ] files_paths_to_queue = list ( set ( self . _file_paths ) - set ( file_paths_in_progress ) - set ( file_paths_recently_processed ) - set ( files_paths_at_run_limit ) ) for file_path , processor in self . _processors . items ( ) : self . log . debug ( \"File path %s is still being processed  (started: %s)\" , processor . file_path , processor . start_time . isoformat ( ) ) self . log . debug ( \"Queuing the following files for processing:\\n\\t%s\" , \"\\n\\t\" . join ( files_paths_to_queue ) ) self . _file_path_queue . extend ( files_paths_to_queue ) zombies = self . _find_zombies ( ) while ( self . _parallelism - len ( self . _processors ) > 0 and len ( self . _file_path_queue ) > 0 ) : file_path = self . _file_path_queue . pop ( 0 ) processor = self . _processor_factory ( file_path , zombies ) processor . start ( ) self . log . debug ( \"Started a process (PID: %s) to generate tasks for %s\" , processor . pid , file_path ) self . _processors [ file_path ] = processor self . _run_count [ self . _heart_beat_key ] += 1 return simple_dags"}
{"code_tokens": "def end ( self ) : pids_to_kill = self . get_all_pids ( ) if len ( pids_to_kill ) > 0 : this_process = psutil . Process ( os . getpid ( ) ) child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] for child in child_processes : self . log . info ( \"Terminating child PID: %s\" , child . pid ) child . terminate ( ) timeout = 5 self . log . info ( \"Waiting up to %s seconds for processes to exit...\" , timeout ) try : psutil . wait_procs ( child_processes , timeout = timeout , callback = lambda x : self . log . info ( 'Terminated PID %s' , x . pid ) ) except psutil . TimeoutExpired : self . log . debug ( \"Ran out of time while waiting for processes to exit\" ) child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] if  len ( child_processes ) > 0 : self . log . info ( \"SIGKILL processes that did not terminate gracefully\" ) for child in child_processes : self . log . info ( \"Killing child PID: %s\" , child . pid ) child . kill ( ) child . wait ( )"}
{"code_tokens": "def get_conn ( self ) : self . log . debug ( 'Creating SSH client for conn_id: %s' , self . ssh_conn_id ) client = paramiko . SSHClient ( ) if not self . allow_host_key_change : self . log . warning ( 'Remote Identification Change is not verified. ' 'This wont protect against Man-In-The-Middle attacks' ) client . load_system_host_keys ( ) if self . no_host_key_check : self . log . warning ( 'No Host Key Verification. This wont protect ' 'against Man-In-The-Middle attacks' ) client . set_missing_host_key_policy ( paramiko . AutoAddPolicy ( ) ) if self . password and self . password . strip ( ) : client . connect ( hostname = self . remote_host , username = self . username , password = self . password , key_filename = self . key_file , timeout = self . timeout , compress = self . compress , port = self . port , sock = self . host_proxy ) else : client . connect ( hostname = self .  remote_host , username = self . username , key_filename = self . key_file , timeout = self . timeout , compress = self . compress , port = self . port , sock = self . host_proxy ) if self . keepalive_interval : client . get_transport ( ) . set_keepalive ( self . keepalive_interval ) self . client = client return client"}
{"code_tokens": "def create_transfer_job ( self , body ) : body = self . _inject_project_id ( body , BODY , PROJECT_ID ) return self . get_conn ( ) . transferJobs ( ) . create ( body = body ) . execute ( num_retries = self  . num_retries )"}
{"code_tokens": "def get_transfer_job ( self , job_name , project_id = None ) : return ( self . get_conn ( ) . transferJobs ( ) . get ( jobName = job_name , projectId = project_id ) . execute ( num_retries = self . num_retries  ) )"}
{"code_tokens": "def list_transfer_job ( self , filter ) : conn = self . get_conn ( ) filter = self . _inject_project_id ( filter , FILTER , FILTER_PROJECT_ID ) request = conn . transferJobs ( ) . list ( filter = json . dumps ( filter ) ) jobs = [ ] while request is not None : response = request  . execute ( num_retries = self . num_retries ) jobs . extend ( response [ TRANSFER_JOBS ] ) request = conn . transferJobs ( ) . list_next ( previous_request = request , previous_response = response ) return jobs"}
{"code_tokens": "def update_transfer_job ( self , job_name , body ) : body = self . _inject_project_id ( body , BODY , PROJECT_ID ) return ( self . get_conn ( ) . transferJobs ( ) . patch ( jobName = job_name , body = body ) . execute ( num_retries  = self . num_retries ) )"}
{"code_tokens": "def delete_transfer_job ( self , job_name , project_id ) : return ( self . get_conn ( ) . transferJobs ( ) . patch ( jobName = job_name , body = { PROJECT_ID : project_id , TRANSFER_JOB : { STATUS1 : GcpTransferJobsStatus . DELETED } , TRANSFER_JOB_FIELD_MASK : STATUS1 , } , ) .  execute ( num_retries = self . num_retries ) )"}
{"code_tokens": "def cancel_transfer_operation ( self , operation_name ) : self . get_conn ( ) . transferOperations ( ) .  cancel ( name = operation_name ) . execute ( num_retries = self . num_retries )"}
{"code_tokens": "def pause_transfer_operation ( self , operation_name ) : self . get_conn ( ) . transferOperations ( ) . pause ( name  = operation_name ) . execute ( num_retries = self . num_retries )"}
{"code_tokens": "def resume_transfer_operation ( self , operation_name ) : self . get_conn ( ) . transferOperations ( ) . resume ( name =  operation_name ) . execute ( num_retries = self . num_retries )"}
{"code_tokens": "def wait_for_transfer_job ( self , job , expected_statuses = ( GcpTransferOperationStatus . SUCCESS , ) , timeout = 60 ) : while timeout > 0 : operations = self . list_transfer_operations ( filter = { FILTER_PROJECT_ID : job [ PROJECT_ID ] , FILTER_JOB_NAMES : [ job [ NAME ] ] } ) if GCPTransferServiceHook . operations_contain_expected_statuses ( operations , expected_statuses ) : return time  . sleep ( TIME_TO_SLEEP_IN_SECONDS ) timeout -= TIME_TO_SLEEP_IN_SECONDS raise AirflowException ( \"Timeout. The operation could not be completed within the allotted time.\" )"}
{"code_tokens": "def find_for_task_instance ( task_instance , session ) : TR = TaskReschedule return ( session . query ( TR ) . filter ( TR . dag_id == task_instance . dag_id , TR . task_id == task_instance . task_id , TR . execution_date == task_instance . execution_date , TR . try_number == task_instance . try_number ) . order_by ( asc  ( TR . id ) ) . all ( ) )"}
{"code_tokens": "def open_slots ( self , session ) : from airflow . models . taskinstance import TaskInstance as TI used_slots = session . query ( func . count ( ) ) . filter ( TI . pool == self . pool ) . filter ( TI . state . in_ ( [ State . RUNNING , State . QUEUED ] ) ) . scalar ( ) return self . slots  - used_slots"}
{"code_tokens": "def run_command ( command ) : process = subprocess . Popen ( shlex . split ( command ) , stdout = subprocess . PIPE , stderr = subprocess . PIPE , close_fds = True ) output , stderr = [ stream . decode ( sys . getdefaultencoding ( ) , 'ignore' ) for stream in process . communicate ( ) ] if process . returncode != 0 :  raise AirflowConfigException ( \"Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}\" . format ( command , process . returncode , output , stderr ) ) return output"}
{"code_tokens": "def remove_option ( self , section , option , remove_default = True ) : if super ( ) . has_option ( section , option ) : super ( ) . remove_option ( section , option )  if self . airflow_defaults . has_option ( section , option ) and remove_default : self . airflow_defaults . remove_option ( section , option )"}
{"code_tokens": "def getsection ( self , section ) : if ( section not in self . _sections and section not in self . airflow_defaults . _sections ) : return None _section = copy . deepcopy ( self . airflow_defaults . _sections [ section ] ) if section in self . _sections : _section . update ( copy . deepcopy ( self . _sections [ section ] ) ) section_prefix = 'AIRFLOW__{S}__' . format ( S = section . upper ( ) ) for env_var in sorted ( os . environ . keys ( ) ) : if env_var . startswith ( section_prefix ) : key = env_var . replace ( section_prefix , ''  ) . lower ( ) _section [ key ] = self . _get_env_var_option ( section , key ) for key , val in iteritems ( _section ) : try : val = int ( val ) except ValueError : try : val = float ( val ) except ValueError : if val . lower ( ) in ( 't' , 'true' ) : val = True elif val . lower ( ) in ( 'f' , 'false' ) : val = False _section [ key ] = val return _section"}
{"code_tokens": "def allocate_ids ( self , partial_keys ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . allocateIds ( projectId = self . project_id  , body = { 'keys' : partial_keys } ) . execute ( num_retries = self . num_retries ) ) return resp [ 'keys' ]"}
{"code_tokens": "def begin_transaction ( self ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . beginTransaction ( projectId = self . project_id , body = { } ) . execute ( num_retries = self . num_retries ) ) return  resp [ 'transaction' ]"}
{"code_tokens": "def commit ( self , body ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . commit ( projectId = self . project_id , body = body ) . execute ( num_retries = self . num_retries  ) ) return resp"}
{"code_tokens": "def lookup ( self , keys , read_consistency = None , transaction = None ) : conn = self . get_conn ( ) body = { 'keys' : keys } if read_consistency : body [ 'readConsistency' ] = read_consistency if transaction : body [ 'transaction' ] = transaction resp = ( conn . projects ( ) . lookup ( projectId = self . project_id , body = body ) . execute ( num_retries = self . num_retries )  ) return resp"}
{"code_tokens": "def rollback ( self , transaction ) : conn = self . get_conn ( ) conn . projects ( ) . rollback ( projectId = self  . project_id , body = { 'transaction' : transaction } ) . execute ( num_retries = self . num_retries )"}
{"code_tokens": "def run_query ( self , body ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . runQuery ( projectId = self . project_id , body = body ) .  execute ( num_retries = self . num_retries ) ) return resp [ 'batch' ]"}
{"code_tokens": "def get_operation ( self , name ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . operations ( ) . get ( name =  name ) . execute ( num_retries = self . num_retries ) ) return resp"}
{"code_tokens": "def delete_operation ( self , name ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . operations ( ) . delete ( name = name ) . execute ( num_retries = self . num_retries  ) ) return resp"}
{"code_tokens": "def poll_operation_until_done ( self , name , polling_interval_in_seconds ) : while True : result = self . get_operation ( name ) state = result [ 'metadata' ] [ 'common' ] [ 'state' ] if state == 'PROCESSING' : self . log . info ( 'Operation is processing. Re-polling state in {} seconds' . format ( polling_interval_in_seconds )  ) time . sleep ( polling_interval_in_seconds ) else : return result"}
{"code_tokens": "def export_to_storage_bucket ( self , bucket , namespace = None , entity_filter = None , labels = None ) : admin_conn = self . get_conn ( ) output_uri_prefix = 'gs://' + '/' . join ( filter ( None , [ bucket , namespace ] ) ) if not entity_filter : entity_filter = { } if not labels : labels = { } body = { 'outputUrlPrefix' : output_uri_prefix , 'entityFilter' : entity_filter , 'labels' : labels , } resp = ( admin_conn . projects ( ) . export ( projectId = self  . project_id , body = body ) . execute ( num_retries = self . num_retries ) ) return resp"}
{"code_tokens": "def import_from_storage_bucket ( self , bucket , file , namespace = None , entity_filter = None , labels = None ) : admin_conn = self . get_conn ( ) input_url = 'gs://' + '/' . join ( filter ( None , [ bucket , namespace , file ] ) ) if not entity_filter : entity_filter = { } if not labels : labels = { } body = { 'inputUrl' : input_url , 'entityFilter' : entity_filter , 'labels' : labels , } resp = ( admin_conn . projects ( ) . import_ ( projectId = self . project_id , body = body ) . execute ( num_retries = self .  num_retries ) ) return resp"}
{"code_tokens": "def publish_to_target ( self , target_arn , message ) : conn = self . get_conn ( ) messages = { 'default' : message } return conn . publish ( TargetArn = target_arn , Message = json . dumps ( messages ) , MessageStructure =  'json' )"}
{"code_tokens": "def get_hostname ( ) : try : callable_path = conf . get ( 'core' , 'hostname_callable' ) except AirflowConfigException : callable_path = None if not callable_path : return socket . getfqdn ( ) module_path , attr_name = callable_path . split ( ':' )  module = importlib . import_module ( module_path ) callable = getattr ( module , attr_name ) return callable ( )"}
{"code_tokens": "def get_conn ( self ) : if not self . _conn : self . _conn = LanguageServiceClient ( credentials = self . _get_credentials ( ) ) return  self . _conn"}
{"code_tokens": "def analyze_entities ( self , document , encoding_type = None , retry = None , timeout = None , metadata = None ) : client = self . get_conn ( ) return client . analyze_entities ( document = document , encoding_type  = encoding_type , retry = retry , timeout = timeout , metadata = metadata )"}
{"code_tokens": "def annotate_text ( self , document , features , encoding_type = None , retry = None , timeout = None , metadata = None ) : client = self . get_conn ( ) return client . annotate_text ( document = document , features = features , encoding_type = encoding_type , retry = retry , timeout  = timeout , metadata = metadata , )"}
{"code_tokens": "def classify_text ( self , document , retry = None , timeout = None , metadata = None ) : client = self . get_conn ( ) return client . classify_text (  document = document , retry = retry , timeout = timeout , metadata = metadata )"}
{"code_tokens": "def get_template_field ( env , fullname ) : modname , classname = fullname . rsplit ( \".\" , 1 ) try : with mock ( env . config . autodoc_mock_imports ) : mod = import_module ( modname ) except ImportError : raise RoleException ( \"Error loading %s module.\" % ( modname , ) ) clazz = getattr ( mod , classname ) if not clazz : raise RoleException ( \"Error finding %s class in %s module.\" % ( classname , modname ) ) template_fields = getattr ( clazz , \"template_fields\" ) if not  template_fields : raise RoleException ( \"Could not find the template fields for %s class in %s module.\" % ( classname , modname ) ) return list ( template_fields )"}
{"code_tokens": "def template_field_role ( app , typ , rawtext , text , lineno , inliner , options = { } , content = [ ] ) : text = utils . unescape ( text ) try : template_fields = get_template_field ( app . env , text ) except RoleException as e : msg = inliner . reporter . error ( \"invalid class name %s \\n%s\" % ( text , e , ) , line = lineno ) prb = inliner . problematic ( rawtext , rawtext , msg ) return [ prb ] , [ msg ] node = nodes . inline ( rawtext = rawtext ) for i , field in enumerate ( template_fields ) : if i != 0 : node += nodes . Text ( \", \" ) node  += nodes . literal ( field , \"\" , nodes . Text ( field ) ) return [ node ] , [ ]"}
{"code_tokens": "def dispose_orm ( ) : log . debug ( \"Disposing DB connection pool (PID %s)\" , os . getpid ( ) ) global engine global Session if Session : Session . remove ( ) Session = None if engine : engine . dispose  ( ) engine = None"}
{"code_tokens": "def prepare_classpath ( ) : if DAGS_FOLDER not in sys . path : sys . path . append ( DAGS_FOLDER ) config_path = os . path . join ( AIRFLOW_HOME , 'config' ) if config_path not in sys . path : sys . path . append ( config_path ) if PLUGINS_FOLDER not in sys .  path : sys . path . append ( PLUGINS_FOLDER )"}
{"code_tokens": "def _check_task_id ( self , context ) : ti = context [ 'ti' ] celery_result = ti . xcom_pull  ( task_ids = self . target_task_id ) return celery_result . ready ( )"}
{"code_tokens": "def detect_conf_var ( ) : ticket_cache = configuration . conf . get ( 'kerberos' , 'ccache' ) with open ( ticket_cache , 'rb' ) as f : return b'X-CACHECONF:' in  f . read ( )"}
{"code_tokens": "def alchemy_to_dict ( obj ) : if not obj : return None d = { } for c in obj . __table__ . columns : value = getattr ( obj , c . name ) if type (  value ) == datetime : value = value . isoformat ( ) d [ c . name ] = value return d"}
{"code_tokens": "def chunks ( items , chunk_size ) : if chunk_size <= 0 : raise ValueError ( 'Chunk size must be a positive integer' ) for i in range ( 0 , len ( items ) , chunk_size ) : yield items [ i  : i + chunk_size ]"}
{"code_tokens": "def reduce_in_chunks ( fn , iterable , initializer , chunk_size = 0 ) : if len ( iterable ) == 0 : return initializer if chunk_size == 0 : chunk_size = len ( iterable  ) return reduce ( fn , chunks ( iterable , chunk_size ) , initializer )"}
{"code_tokens": "def chain ( * tasks ) : for up_task , down_task in zip ( tasks [ : - 1 ] , tasks [ 1 : ]  ) : up_task . set_downstream ( down_task )"}
{"code_tokens": "def pprinttable ( rows ) : if not rows : return if hasattr ( rows [ 0 ] , '_fields' ) : headers = rows [ 0 ] . _fields else : headers = [ \"col{}\" . format ( i ) for i in range ( len ( rows [ 0 ] ) ) ] lens = [ len ( s ) for s in headers ] for row in rows : for i in range ( len ( rows [ 0 ] ) ) : slenght = len ( \"{}\" . format ( row [ i ] ) ) if slenght > lens [ i ] : lens [ i ] = slenght formats = [ ] hformats = [ ] for i in range ( len ( rows [ 0 ] ) ) : if isinstance ( rows [ 0 ] [ i ] , int ) : formats . append ( \"%%%dd\" % lens [ i ] ) else : formats . append ( \"%%-%ds\" % lens [ i ] ) hformats . append ( \"%%-%ds\" % lens [ i ] ) pattern =  \" | \" . join ( formats ) hpattern = \" | \" . join ( hformats ) separator = \"-+-\" . join ( [ '-' * n for n in lens ] ) s = \"\" s += separator + '\\n' s += ( hpattern % tuple ( headers ) ) + '\\n' s += separator + '\\n' def f ( t ) : return \"{}\" . format ( t ) if isinstance ( t , basestring ) else t for line in rows : s += pattern % tuple ( f ( t ) for t in line ) + '\\n' s += separator + '\\n' return s"}
{"code_tokens": "def render_log_filename ( ti , try_number , filename_template ) : filename_template , filename_jinja_template = parse_template_string ( filename_template ) if filename_jinja_template : jinja_context = ti . get_template_context ( ) jinja_context [ 'try_number' ] = try_number return filename_jinja_template . render ( ** jinja_context ) return filename_template . format ( dag_id = ti . dag_id , task_id = ti . task_id , execution_date = ti . execution_date .  isoformat ( ) , try_number = try_number )"}
{"code_tokens": "def get_conn ( self ) : http_authorized = self . _authorize ( ) return build ( 'dataproc' , self . api_version , http = http_authorized , cache_discovery  = False )"}
{"code_tokens": "def wait ( self , operation ) : submitted = _DataProcOperation ( self . get_conn ( ) , operation  , self . num_retries ) submitted . wait_for_done ( )"}
{"code_tokens": "def _deep_string_coerce ( content , json_path = 'json' ) : c = _deep_string_coerce if isinstance ( content , six . string_types ) : return content elif isinstance ( content , six . integer_types + ( float , ) ) : return str ( content ) elif isinstance ( content , ( list , tuple ) ) : return [ c ( e , '{0}[{1}]' . format ( json_path , i ) ) for i , e in enumerate ( content ) ] elif isinstance ( content , dict ) : return { k : c ( v , '{0}[{1}]' . format ( json_path , k ) ) for k , v in list ( content . items ( ) ) } else : param_type = type (  content ) msg = 'Type {0} used for parameter {1} is not a number or a string' . format ( param_type , json_path ) raise AirflowException ( msg )"}
{"code_tokens": "def _handle_databricks_operator_execution ( operator , hook , log , context ) : if operator . do_xcom_push : context [ 'ti' ] . xcom_push ( key = XCOM_RUN_ID_KEY , value = operator . run_id ) log . info ( 'Run submitted with run_id: %s' , operator . run_id ) run_page_url = hook . get_run_page_url ( operator . run_id ) if operator . do_xcom_push : context [ 'ti' ] . xcom_push ( key = XCOM_RUN_PAGE_URL_KEY , value = run_page_url ) log . info ( 'View run status, Spark UI, and logs at %s' , run_page_url ) while True : run_state = hook . get_run_state ( operator . run_id ) if run_state . is_terminal : if run_state . is_successful : log . info ( '%s completed successfully.' , operator . task_id ) log . info ( 'View run status, Spark UI, and logs at %s' , run_page_url ) return else : error_message = '{t} failed with terminal state: {s}' . format ( t = operator . task_id , s = run_state ) raise AirflowException ( error_message ) else : log  . info ( '%s in run state: %s' , operator . task_id , run_state ) log . info ( 'View run status, Spark UI, and logs at %s' , run_page_url ) log . info ( 'Sleeping for %s seconds.' , operator . polling_period_seconds ) time . sleep ( operator . polling_period_seconds )"}
{"code_tokens": "def run_cli ( self , pig , verbose = True ) : with TemporaryDirectory ( prefix = 'airflow_pigop_' ) as tmp_dir : with NamedTemporaryFile ( dir = tmp_dir ) as f : f . write ( pig . encode ( 'utf-8' ) ) f . flush ( ) fname = f . name pig_bin = 'pig' cmd_extra = [ ] pig_cmd = [ pig_bin , '-f' , fname ] + cmd_extra if self . pig_properties : pig_properties_list = self . pig_properties . split ( ) pig_cmd . extend ( pig_properties_list ) if verbose : self . log . info ( \"%s\" , \" \" . join ( pig_cmd ) ) sp = subprocess . Popen ( pig_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , cwd = tmp_dir , close_fds = True ) self . sp = sp stdout = '' for line in iter ( sp . stdout . readline , b'' ) :  stdout += line . decode ( 'utf-8' ) if verbose : self . log . info ( line . strip ( ) ) sp . wait ( ) if sp . returncode : raise AirflowException ( stdout ) return stdout"}
{"code_tokens": "def fetch_celery_task_state ( celery_task ) : try : with timeout ( seconds = 2 ) : res = ( celery_task [ 0 ] , celery_task [ 1 ] . state ) except Exception as e : exception_traceback = \"Celery Task ID: {}\\n{}\" . format ( celery_task [ 0 ] , traceback . format_exc ( )  ) res = ExceptionWithTraceback ( e , exception_traceback ) return res"}
{"code_tokens": "def _num_tasks_per_send_process ( self , to_send_count ) : return max ( 1 , int  ( math . ceil ( 1.0 * to_send_count / self . _sync_parallelism ) ) )"}
{"code_tokens": "def _num_tasks_per_fetch_process ( self ) : return max ( 1 , int ( math . ceil ( 1.0 * len ( self . tasks ) / self . _sync_parallelism ) )  )"}
{"code_tokens": "def setdefault ( cls , key , default , deserialize_json = False ) : obj = Variable . get ( key , default_var = None , deserialize_json = deserialize_json ) if obj is None : if default is not None : Variable . set ( key , default  , serialize_json = deserialize_json ) return default else : raise ValueError ( 'Default Value must be set' ) else : return obj"}
{"code_tokens": "def get_conn ( self ) : authed_http = self . _authorize ( ) return build ( 'ml' , 'v1' , http  = authed_http , cache_discovery = False )"}
{"code_tokens": "def create_job ( self , project_id , job , use_existing_job_fn = None ) : request = self . _mlengine . projects ( ) . jobs ( ) . create ( parent = 'projects/{}' . format ( project_id ) , body = job ) job_id = job [ 'jobId' ] try : request . execute ( ) except HttpError as e : if e . resp . status == 409 : if use_existing_job_fn is not None : existing_job = self . _get_job ( project_id , job_id ) if not use_existing_job_fn ( existing_job ) : self . log . error ( 'Job with job_id %s already exist, but it does ' 'not match our expectation: %s' , job_id , existing_job ) raise self . log . info ( 'Job with job_id %s already exist. Will waiting for it to finish' , job_id ) else : self . log . error ( 'Failed to create MLEngine job: {}' . format ( e )  ) raise return self . _wait_for_job_done ( project_id , job_id )"}
{"code_tokens": "def _get_job ( self , project_id , job_id ) : job_name = 'projects/{}/jobs/{}' . format ( project_id , job_id ) request = self . _mlengine . projects ( ) . jobs ( ) . get ( name = job_name ) while True : try : return request . execute ( )  except HttpError as e : if e . resp . status == 429 : time . sleep ( 30 ) else : self . log . error ( 'Failed to get MLEngine job: {}' . format ( e ) ) raise"}
{"code_tokens": "def _wait_for_job_done ( self , project_id , job_id , interval = 30 ) : if interval <= 0 : raise ValueError ( \"Interval must be > 0\" ) while True : job = self . _get_job ( project_id , job_id ) if job [ 'state' ] in [ 'SUCCEEDED' , 'FAILED' , 'CANCELLED' ] : return job time .  sleep ( interval )"}
{"code_tokens": "def create_version ( self , project_id , model_name , version_spec ) : parent_name = 'projects/{}/models/{}' . format ( project_id , model_name ) create_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . create ( parent = parent_name , body = version_spec ) response = create_request . execute ( ) get_request = self . _mlengine . projects ( ) . operations ( ) . get  ( name = response [ 'name' ] ) return _poll_with_exponential_delay ( request = get_request , max_n = 9 , is_done_func = lambda resp : resp . get ( 'done' , False ) , is_error_func = lambda resp : resp . get ( 'error' , None ) is not None )"}
{"code_tokens": "def set_default_version ( self , project_id , model_name , version_name ) : full_version_name = 'projects/{}/models/{}/versions/{}' . format ( project_id , model_name , version_name ) request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . setDefault ( name = full_version_name , body = { } ) try : response = request . execute ( ) self . log . info ( 'Successfully set version: %s to default' , response ) return response except HttpError as e : self . log . error ( 'Something went wrong:  %s' , e ) raise"}
{"code_tokens": "def list_versions ( self , project_id , model_name ) : result = [ ] full_parent_name = 'projects/{}/models/{}' . format ( project_id , model_name ) request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full_parent_name , pageSize = 100 ) response = request . execute ( ) next_page_token = response . get ( 'nextPageToken' , None ) result . extend ( response . get ( 'versions' , [ ] ) ) while next_page_token is not None : next_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full_parent_name , pageToken = next_page_token , pageSize = 100 ) response = next_request . execute ( ) next_page_token = response . get ( 'nextPageToken' , None ) result . extend ( response . get ( 'versions' , [ ] ) ) time . sleep ( 5 ) return result "}
{"code_tokens": "def delete_version ( self , project_id , model_name , version_name ) : full_name = 'projects/{}/models/{}/versions/{}' . format ( project_id , model_name , version_name ) delete_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . delete ( name = full_name ) response = delete_request . execute ( ) get_request = self . _mlengine . projects ( ) . operations ( ) . get ( name = response [ 'name' ] ) return _poll_with_exponential_delay ( request = get_request , max_n = 9 ,  is_done_func = lambda resp : resp . get ( 'done' , False ) , is_error_func = lambda resp : resp . get ( 'error' , None ) is not None )"}
{"code_tokens": "def create_model ( self , project_id , model ) : if not model [ 'name' ] : raise ValueError ( \"Model name must be provided and \" \"could not be an empty string\" ) project = 'projects/{}' . format ( project_id ) request = self  . _mlengine . projects ( ) . models ( ) . create ( parent = project , body = model ) return request . execute ( )"}
{"code_tokens": "def get_model ( self , project_id , model_name ) : if not model_name : raise ValueError ( \"Model name must be provided and \" \"it could not be an empty string\" ) full_model_name = 'projects/{}/models/{}' . format ( project_id , model_name ) request = self . _mlengine . projects ( ) . models ( ) . get ( name = full_model_name )  try : return request . execute ( ) except HttpError as e : if e . resp . status == 404 : self . log . error ( 'Model was not found: %s' , e ) return None raise"}
{"code_tokens": "def write_batch_data ( self , items ) : dynamodb_conn = self . get_conn ( ) try : table = dynamodb_conn . Table ( self . table_name ) with table . batch_writer ( overwrite_by_pkeys = self . table_keys ) as batch : for item in items : batch . put_item ( Item = item ) return True except Exception as general_error : raise AirflowException  ( 'Failed to insert items in dynamodb, error: {error}' . format ( error = str ( general_error ) ) )"}
{"code_tokens": "def _integrate_plugins ( ) : from airflow . plugins_manager import executors_modules for executors_module in executors_modules : sys . modules [ executors_module . __name__ ]  = executors_module globals ( ) [ executors_module . _name ] = executors_module"}
{"code_tokens": "def get_default_executor ( ) : global DEFAULT_EXECUTOR if DEFAULT_EXECUTOR is not None : return DEFAULT_EXECUTOR executor_name = configuration . conf . get ( 'core' , 'EXECUTOR' ) DEFAULT_EXECUTOR = _get_executor ( executor_name ) log = LoggingMixin ( ) .  log log . info ( \"Using executor %s\" , executor_name ) return DEFAULT_EXECUTOR"}
{"code_tokens": "def _get_executor ( executor_name ) : if executor_name == Executors . LocalExecutor : return LocalExecutor ( ) elif executor_name == Executors . SequentialExecutor : return SequentialExecutor ( ) elif executor_name == Executors . CeleryExecutor : from airflow . executors . celery_executor import CeleryExecutor return CeleryExecutor ( ) elif executor_name == Executors . DaskExecutor : from airflow . executors . dask_executor import DaskExecutor return DaskExecutor ( ) elif executor_name == Executors . KubernetesExecutor : from airflow . contrib . executors . kubernetes_executor import KubernetesExecutor return KubernetesExecutor ( ) else : _integrate_plugins ( ) executor_path = executor_name . split ( '.' ) if len ( executor_path ) != 2 : raise AirflowException ( \"Executor {0} not supported: \" \"please specify in format plugin_module.executor\" . format ( executor_name ) ) if executor_path [ 0 ] in globals ( ) : return globals ( ) [ executor_path [ 0 ] ] .  __dict__ [ executor_path [ 1 ] ] ( ) else : raise AirflowException ( \"Executor {0} not supported.\" . format ( executor_name ) )"}
{"code_tokens": "def on_error ( self , error , items ) : self . log . error ( 'Encountered Segment error: {segment_error} with ' 'items: {with_items}' . format ( segment_error = error , with_items = items ) ) raise AirflowException ( 'Segment error:  {}' . format ( error ) )"}
{"code_tokens": "def get_conn ( self ) : conn = self . get_connection ( self . mssql_conn_id ) conn = pymssql . connect ( server = conn . host , user = conn . login , password = conn . password , database = self . schema or conn . schema , port = conn .  port ) return conn"}
{"code_tokens": "def trigger_dag ( dag_id ) : data = request . get_json ( force = True ) run_id = None if 'run_id' in data : run_id = data [ 'run_id' ] conf = None if 'conf' in data : conf = data [ 'conf' ] execution_date = None if 'execution_date' in data and data [ 'execution_date' ] is not None : execution_date = data [ 'execution_date' ] try : execution_date = timezone . parse ( execution_date ) except ValueError : error_message = ( 'Given execution date, {}, could not be identified ' 'as a date. Example date format: 2015-11-16T14:34:15+00:00' . format ( execution_date ) ) _log . info ( error_message ) response = jsonify ( { 'error' : error_message } ) response . status_code = 400 return response try : dr = trigger . trigger_dag ( dag_id , run_id , conf , execution_date ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status_code = err . status_code return response if getattr ( g , 'user' , None ) : _log .  info ( \"User %s created %s\" , g . user , dr ) response = jsonify ( message = \"Created {}\" . format ( dr ) ) return response"}
{"code_tokens": "def delete_dag ( dag_id ) : try : count = delete . delete_dag ( dag_id ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = \"{}\"  . format ( err ) ) response . status_code = err . status_code return response return jsonify ( message = \"Removed {} record(s)\" . format ( count ) , count = count )"}
{"code_tokens": "def task_info ( dag_id , task_id ) : try : info = get_task ( dag_id , task_id ) except AirflowException as err : _log . info ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status_code = err . status_code return response fields = { k : str ( v ) for k , v in vars ( info ) . items ( )  if not k . startswith ( '_' ) } return jsonify ( fields )"}
{"code_tokens": "def get_pools ( ) : try : pools = pool_api . get_pools ( ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = \"{}\" . format ( err ) ) response . status_code = err . status_code return response else :  return jsonify ( [ p . to_json ( ) for p in pools ] )"}
{"code_tokens": "def create_pool ( ) : params = request . get_json ( force = True ) try : pool = pool_api . create_pool ( ** params ) except AirflowException as err : _log . error ( err ) response =  jsonify ( error = \"{}\" . format ( err ) ) response . status_code = err . status_code return response else : return jsonify ( pool . to_json ( ) )"}
{"code_tokens": "def delete_pool ( name ) : try : pool = pool_api . delete_pool ( name = name ) except AirflowException as err : _log . error ( err ) response = jsonify ( error =  \"{}\" . format ( err ) ) response . status_code = err . status_code return response else : return jsonify ( pool . to_json ( ) )"}
{"code_tokens": "def create_or_update ( self , resource_group , name , container_group ) : self . connection . container_groups . create_or_update (  resource_group , name , container_group )"}
{"code_tokens": "def get_state_exitcode_details ( self , resource_group , name ) : current_state = self . _get_instance_view ( resource_group , name ) . current_state return ( current_state .  state , current_state . exit_code , current_state . detail_status )"}
{"code_tokens": "def get_messages ( self , resource_group , name ) : instance_view = self . _get_instance_view ( resource_group , name ) return [ event . message for event in instance_view .  events ]"}
{"code_tokens": "def get_logs ( self , resource_group , name , tail = 1000 ) : logs = self . connection . container . list_logs ( resource_group , name , name , tail = tail ) return logs  . content . splitlines ( True )"}
{"code_tokens": "def delete ( self , resource_group , name ) : self . connection . container_groups .  delete ( resource_group , name )"}
{"code_tokens": "def exists ( self , resource_group , name ) : for container in self . connection .  container_groups . list_by_resource_group ( resource_group ) : if container . name == name : return True return False"}
{"code_tokens": "def apply_defaults ( func ) : sig_cache = signature ( func ) non_optional_args = { name for ( name , param ) in sig_cache . parameters . items ( ) if param . default == param . empty and param . name != 'self' and param . kind not in ( param . VAR_POSITIONAL , param . VAR_KEYWORD ) } @ wraps ( func ) def wrapper ( * args , ** kwargs ) : if len ( args ) > 1 : raise AirflowException ( \"Use keyword arguments when initializing operators\" ) dag_args = { } dag_params = { } dag = kwargs . get ( 'dag' , None ) or settings . CONTEXT_MANAGER_DAG if dag : dag_args = copy ( dag . default_args ) or { } dag_params = copy ( dag . params ) or { } params = { } if 'params' in kwargs : params = kwargs [ 'params' ] dag_params . update ( params ) default_args = { } if 'default_args' in kwargs : default_args = kwargs [ 'default_args' ] if 'params' in default_args : dag_params . update ( default_args [ 'params' ] ) del default_args [ 'params' ] dag_args . update ( default_args ) default_args = dag_args for arg in sig_cache . parameters : if arg not in kwargs and arg in default_args : kwargs [ arg ] = default_args [ arg ] missing_args = list ( non_optional_args - set ( kwargs ) ) if missing_args :  msg = \"Argument {0} is required\" . format ( missing_args ) raise AirflowException ( msg ) kwargs [ 'params' ] = dag_params result = func ( * args , ** kwargs ) return result return wrapper"}
{"code_tokens": "def construct_ingest_query ( self , static_path , columns ) : num_shards = self . num_shards target_partition_size = self . target_partition_size if self . target_partition_size == - 1 : if self . num_shards == - 1 : target_partition_size = DEFAULT_TARGET_PARTITION_SIZE else : num_shards = - 1 metric_names = [ m [ 'fieldName' ] for m in self . metric_spec if m [ 'type' ] != 'count' ] dimensions = [ c for c in columns if c not in metric_names and c != self . ts_dim ] ingest_query_dict = { \"type\" : \"index_hadoop\" , \"spec\" : { \"dataSchema\" : { \"metricsSpec\" : self . metric_spec , \"granularitySpec\" : { \"queryGranularity\" : self . query_granularity , \"intervals\" : self . intervals , \"type\" : \"uniform\" , \"segmentGranularity\" : self . segment_granularity , } , \"parser\" : { \"type\" : \"string\" , \"parseSpec\" : { \"columns\" : columns , \"dimensionsSpec\" : { \"dimensionExclusions\" : [ ] , \"dimensions\" : dimensions , \"spatialDimensions\" : [ ] } , \"timestampSpec\" : { \"column\" : self . ts_dim , \"format\" : \"auto\" } , \"format\" : \"tsv\" } } , \"dataSource\" : self . druid_datasource } , \"tuningConfig\" : { \"type\" : \"hadoop\" , \"jobProperties\" : { \"mapreduce.job.user.classpath.first\" : \"false\" , \"mapreduce.map.output.compress\" : \"false\" , \"mapreduce.output.fileoutputformat.compress\" : \"false\" , } , \"partitionsSpec\" : { \"type\" : \"hashed\" , \"targetPartitionSize\" : target_partition_size , \"numShards\" : num_shards , } , } , \"ioConfig\" : { \"inputSpec\" : { \"paths\" : static_path , \"type\" : \"static\" } , \"type\" : \"hadoop\" } } } if self . job_properties : ingest_query_dict [ 'spec' ] [ 'tuningConfig' ] [ 'jobProperties' ] . update ( self . job_properties ) if self . hadoop_dependency_coordinates : ingest_query_dict [  'hadoopDependencyCoordinates' ] = self . hadoop_dependency_coordinates return ingest_query_dict"}
{"code_tokens": "def poke ( self , context ) : self . log . info ( 'RedisPubSubSensor checking for message on channels: %s' , self . channels ) message = self . pubsub . get_message ( ) self . log . info ( 'Message %s from channel %s' , message , self . channels ) if message and message [ 'type' ] == 'message' : context [ 'ti' ] . xcom_push ( key = 'message' , value = message ) self . pubsub . unsubscribe ( self .  channels ) return True return False"}
{"code_tokens": "def find ( dag_id = None , run_id = None , execution_date = None , state = None , external_trigger = None , no_backfills = False , session = None ) : DR = DagRun qry = session . query ( DR ) if dag_id : qry = qry . filter ( DR . dag_id == dag_id ) if run_id : qry = qry . filter ( DR . run_id == run_id ) if execution_date : if isinstance ( execution_date , list ) : qry = qry . filter ( DR . execution_date . in_ ( execution_date ) ) else : qry = qry . filter ( DR . execution_date == execution_date ) if state : qry = qry . filter ( DR . state == state ) if external_trigger is not None : qry = qry . filter ( DR . external_trigger == external_trigger ) if no_backfills : from airflow . jobs import BackfillJob qry = qry . filter ( DR . run_id . notlike ( BackfillJob  . ID_PREFIX + '%' ) ) dr = qry . order_by ( DR . execution_date ) . all ( ) return dr"}
{"code_tokens": "def get_task_instances ( self , state = None , session = None ) : from airflow . models . taskinstance import TaskInstance tis = session . query ( TaskInstance ) . filter ( TaskInstance . dag_id == self . dag_id , TaskInstance . execution_date == self . execution_date , ) if state : if isinstance ( state , six . string_types ) : tis = tis . filter ( TaskInstance . state == state ) else : if None in state : tis = tis . filter ( or_ ( TaskInstance . state . in_ ( state ) , TaskInstance . state . is_ ( None ) ) ) else : tis = tis . filter ( TaskInstance . state . in_ ( state ) ) if self . dag and self . dag . partial : tis = tis . filter  ( TaskInstance . task_id . in_ ( self . dag . task_ids ) ) return tis . all ( )"}
{"code_tokens": "def get_task_instance ( self , task_id , session = None ) : from airflow . models . taskinstance import TaskInstance TI = TaskInstance ti = session . query ( TI ) . filter ( TI . dag_id == self . dag_id , TI  . execution_date == self . execution_date , TI . task_id == task_id ) . first ( ) return ti"}
{"code_tokens": "def get_previous_dagrun ( self , session = None ) : return session . query ( DagRun ) . filter ( DagRun . dag_id == self . dag_id , DagRun . execution_date  < self . execution_date ) . order_by ( DagRun . execution_date . desc ( ) ) . first ( )"}
{"code_tokens": "def get_previous_scheduled_dagrun ( self , session = None ) : dag = self . get_dag ( ) return session . query ( DagRun ) . filter ( DagRun . dag_id == self . dag_id , DagRun . execution_date == dag  . previous_schedule ( self . execution_date ) ) . first ( )"}
{"code_tokens": "def update_state ( self , session = None ) : dag = self . get_dag ( ) tis = self . get_task_instances ( session = session ) self . log . debug ( \"Updating state for %s considering %s task(s)\" , self , len ( tis ) ) for ti in list ( tis ) : if ti . state == State . REMOVED : tis . remove ( ti ) else : ti . task = dag . get_task ( ti . task_id ) start_dttm = timezone . utcnow ( ) unfinished_tasks = self . get_task_instances ( state = State . unfinished ( ) , session = session ) none_depends_on_past = all ( not t . task . depends_on_past for t in unfinished_tasks ) none_task_concurrency = all ( t . task . task_concurrency is None for t in unfinished_tasks ) if unfinished_tasks and none_depends_on_past and none_task_concurrency : no_dependencies_met = True for ut in unfinished_tasks : old_state = ut . state deps_met = ut . are_dependencies_met ( dep_context = DepContext ( flag_upstream_failed = True , ignore_in_retry_period = True , ignore_in_reschedule_period = True ) , session = session ) if deps_met or old_state != ut . current_state ( session = session ) : no_dependencies_met = False break duration = ( timezone . utcnow ( ) - start_dttm ) . total_seconds ( ) * 1000 Stats . timing ( \"dagrun.dependency-check.{}\" . format ( self . dag_id ) , duration ) root_ids = [ t . task_id for t in dag . roots ] roots = [ t for t in tis if t . task_id in root_ids ] if ( not unfinished_tasks and any ( r . state in ( State . FAILED , State . UPSTREAM_FAILED ) for r in roots ) ) : self . log . info ( 'Marking run %s failed' , self ) self . set_state ( State . FAILED ) dag . handle_callback ( self , success = False , reason = 'task_failure' , session = session ) elif not unfinished_tasks and all ( r . state in ( State .  SUCCESS , State . SKIPPED ) for r in roots ) : self . log . info ( 'Marking run %s successful' , self ) self . set_state ( State . SUCCESS ) dag . handle_callback ( self , success = True , reason = 'success' , session = session ) elif ( unfinished_tasks and none_depends_on_past and none_task_concurrency and no_dependencies_met ) : self . log . info ( 'Deadlock; marking run %s failed' , self ) self . set_state ( State . FAILED ) dag . handle_callback ( self , success = False , reason = 'all_tasks_deadlocked' , session = session ) else : self . set_state ( State . RUNNING ) self . _emit_duration_stats_for_finished_state ( ) session . merge ( self ) session . commit ( ) return self . state"}
{"code_tokens": "def verify_integrity ( self , session = None ) : from airflow . models . taskinstance import TaskInstance dag = self . get_dag ( ) tis = self . get_task_instances ( session = session ) task_ids = [ ] for ti in tis : task_ids . append ( ti . task_id ) task = None try : task = dag . get_task ( ti . task_id ) except AirflowException : if ti . state == State . REMOVED : pass elif self . state is not State . RUNNING and not dag . partial : self . log . warning ( \"Failed to get task '{}' for dag '{}'. \" \"Marking it as removed.\" . format ( ti , dag ) ) Stats . incr ( \"task_removed_from_dag.{}\" . format ( dag . dag_id ) , 1 , 1 ) ti . state = State . REMOVED is_task_in_dag  = task is not None should_restore_task = is_task_in_dag and ti . state == State . REMOVED if should_restore_task : self . log . info ( \"Restoring task '{}' which was previously \" \"removed from DAG '{}'\" . format ( ti , dag ) ) Stats . incr ( \"task_restored_to_dag.{}\" . format ( dag . dag_id ) , 1 , 1 ) ti . state = State . NONE for task in six . itervalues ( dag . task_dict ) : if task . start_date > self . execution_date and not self . is_backfill : continue if task . task_id not in task_ids : Stats . incr ( \"task_instance_created-{}\" . format ( task . __class__ . __name__ ) , 1 , 1 ) ti = TaskInstance ( task , self . execution_date ) session . add ( ti ) session . commit ( )"}
{"code_tokens": "def jenkins_request_with_headers ( jenkins_server , req ) : try : response = jenkins_server . jenkins_request ( req ) response_body = response . content response_headers = response . headers if response_body is None : raise jenkins . EmptyResponseException ( \"Error communicating with server[%s]: \" \"empty response\" % jenkins_server . server ) return { 'body' : response_body . decode ( 'utf-8' ) , 'headers' : response_headers } except HTTPError as e : if e . code in [ 401 , 403 , 500 ] : raise JenkinsException ( 'Error in request. ' + 'Possibly authentication failed [%s]: %s' % ( e . code , e . msg ) ) elif e . code == 404 : raise jenkins . NotFoundException ( 'Requested  item could not be found' ) else : raise except socket . timeout as e : raise jenkins . TimeoutException ( 'Error in request: %s' % e ) except URLError as e : if str ( e . reason ) == \"timed out\" : raise jenkins . TimeoutException ( 'Error in request: %s' % e . reason ) raise JenkinsException ( 'Error in request: %s' % e . reason )"}
{"code_tokens": "def context_to_airflow_vars ( context , in_env_var_format = False ) : params = dict ( ) if in_env_var_format : name_format = 'env_var_format' else : name_format = 'default' task_instance = context . get ( 'task_instance' ) if task_instance and task_instance . dag_id : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_DAG_ID' ] [ name_format ] ] = task_instance . dag_id if task_instance and task_instance . task_id : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_TASK_ID' ] [ name_format ] ] = task_instance . task_id if task_instance and task_instance . execution_date : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_EXECUTION_DATE' ] [ name_format ] ] = task_instance . execution_date . isoformat ( ) dag_run = context . get ( 'dag_run' ) if dag_run and dag_run . run_id : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_DAG_RUN_ID' ] [ name_format ] ] = dag_run . run_id  return params"}
{"code_tokens": "def conditionally_trigger ( context , dag_run_obj ) : c_p = context [ 'params' ] [ 'condition_param' ] print ( \"Controller DAG : conditionally_trigger = {}\" . format ( c_p ) ) if context [ 'params' ] [ 'condition_param' ] : dag_run_obj . payload = { 'message' : context [ 'params' ] [ 'message' ] } pp . pprint ( dag_run_obj . payload ) return  dag_run_obj"}
{"code_tokens": "def send_metric ( self , metric_name , datapoint , tags = None , type_ = None , interval = None ) : response = api . Metric . send ( metric = metric_name , points = datapoint , host = self . host , tags = tags , type = type_ , interval = interval ) self . validate_response ( response )  return response"}
{"code_tokens": "def query_metric ( self , query , from_seconds_ago , to_seconds_ago ) : now = int ( time . time ( ) ) response = api . Metric . query (  start = now - from_seconds_ago , end = now - to_seconds_ago , query = query ) self . validate_response ( response ) return response"}
{"code_tokens": "def get_dag ( self , dag_id ) : from airflow . models . dag import DagModel root_dag_id = dag_id if dag_id in self . dags : dag = self . dags [ dag_id ] if dag . is_subdag : root_dag_id = dag . parent_dag . dag_id orm_dag = DagModel . get_current ( root_dag_id ) if orm_dag and ( root_dag_id not in self . dags or ( orm_dag . last_expired and dag . last_loaded < orm_dag . last_expired ) ) : found_dags = self . process_file  ( filepath = orm_dag . fileloc , only_if_updated = False ) if found_dags and dag_id in [ found_dag . dag_id for found_dag in found_dags ] : return self . dags [ dag_id ] elif dag_id in self . dags : del self . dags [ dag_id ] return self . dags . get ( dag_id )"}
{"code_tokens": "def kill_zombies ( self , zombies , session = None ) : from airflow . models . taskinstance import TaskInstance for zombie in zombies : if zombie . dag_id in self . dags : dag = self . dags [ zombie . dag_id ] if zombie . task_id in dag . task_ids : task = dag . get_task ( zombie . task_id ) ti = TaskInstance ( task , zombie . execution_date ) ti . start_date = zombie . start_date ti . end_date = zombie . end_date ti . try_number = zombie . try_number ti . state = zombie . state ti . test_mode = configuration . getboolean ( 'core' , 'unit_test_mode' ) ti . handle_failure ( \"{} detected as zombie\" .  format ( ti ) , ti . test_mode , ti . get_template_context ( ) ) self . log . info ( 'Marked zombie job %s as %s' , ti , ti . state ) Stats . incr ( 'zombies_killed' ) session . commit ( )"}
{"code_tokens": "def bag_dag ( self , dag , parent_dag , root_dag ) : dag . test_cycle ( ) dag . resolve_template_files ( ) dag . last_loaded = timezone . utcnow ( ) for task in dag . tasks : settings . policy ( task ) subdags = dag . subdags try : for subdag in subdags : subdag . full_filepath = dag . full_filepath subdag . parent_dag = dag subdag . is_subdag = True self . bag_dag ( subdag , parent_dag = dag , root_dag = root_dag ) self . dags [ dag . dag_id ] = dag self . log . debug ( 'Loaded DAG %s' , dag ) except AirflowDagCycleException as cycle_exception : self . log .  exception ( 'Exception bagging dag: %s' , dag . dag_id ) if dag == root_dag : for subdag in subdags : if subdag . dag_id in self . dags : del self . dags [ subdag . dag_id ] raise cycle_exception"}
{"code_tokens": "def collect_dags ( self , dag_folder = None , only_if_updated = True , include_examples = configuration . conf . getboolean ( 'core' , 'LOAD_EXAMPLES' ) , safe_mode = configuration . conf . getboolean ( 'core' , 'DAG_DISCOVERY_SAFE_MODE' ) ) : start_dttm = timezone . utcnow ( ) dag_folder = dag_folder or self . dag_folder stats = [ ] FileLoadStat = namedtuple ( 'FileLoadStat' , \"file duration dag_num task_num dags\" ) dag_folder = correct_maybe_zipped ( dag_folder ) for filepath in list_py_file_paths ( dag_folder , safe_mode = safe_mode , include_examples = include_examples ) : try : ts = timezone . utcnow ( ) found_dags = self . process_file ( filepath , only_if_updated = only_if_updated , safe_mode = safe_mode ) td = timezone . utcnow ( ) - ts td = td . total_seconds ( ) + ( float ( td . microseconds ) / 1000000 ) stats . append ( FileLoadStat ( filepath . replace ( dag_folder , '' ) , td , len ( found_dags ) , sum ( [ len ( dag . tasks ) for dag in found_dags ] ) , str ( [ dag . dag_id for dag in found_dags ] )  , ) ) except Exception as e : self . log . exception ( e ) Stats . gauge ( 'collect_dags' , ( timezone . utcnow ( ) - start_dttm ) . total_seconds ( ) , 1 ) Stats . gauge ( 'dagbag_size' , len ( self . dags ) , 1 ) Stats . gauge ( 'dagbag_import_errors' , len ( self . import_errors ) , 1 ) self . dagbag_stats = sorted ( stats , key = lambda x : x . duration , reverse = True )"}
{"code_tokens": "def dagbag_report ( self ) : report = textwrap . dedent ( ) stats = self . dagbag_stats return report . format ( dag_folder = self . dag_folder , duration = sum ( [ o . duration for o in stats ]  ) , dag_num = sum ( [ o . dag_num for o in stats ] ) , task_num = sum ( [ o . task_num for o in stats ] ) , table = pprinttable ( stats ) , )"}
{"code_tokens": "def ds_add ( ds , days ) : ds = datetime . strptime ( ds , '%Y-%m-%d' ) if days : ds = ds + timedelta ( days )  return ds . isoformat ( ) [ : 10 ]"}
{"code_tokens": "def ds_format ( ds , input_format , output_format ) : return datetime  . strptime ( ds , input_format ) . strftime ( output_format )"}
{"code_tokens": "def poke ( self , context ) : sb = self . hook ( self . hdfs_conn_id ) . get_conn ( ) self . log . info ( 'Poking for %s to be a directory with files matching %s' , self . filepath , self . regex . pattern ) result = [ f for f in sb . ls ( [ self . filepath ] , include_toplevel = False ) if f [ 'file_type' ] == 'f' and self . regex . match ( f [ 'path' ] . replace ( '%s/' % self . filepath , '' ) ) ] result = self . filter_for_ignored_ext ( result , self . ignored_ext , self . ignore_copying ) result = self  . filter_for_filesize ( result , self . file_size ) return bool ( result )"}
{"code_tokens": "def poke ( self , context ) : sb = self . hook ( self . hdfs_conn_id ) . get_conn ( ) result = [ f for f in sb . ls ( [ self . filepath ] , include_toplevel = True ) ] result = self . filter_for_ignored_ext ( result , self . ignored_ext , self . ignore_copying ) result = self . filter_for_filesize ( result , self . file_size ) if self . be_empty : self . log . info ( 'Poking for filepath %s to a empty directory' , self . filepath ) return len ( result ) == 1 and result [ 0 ] [ 'path' ] == self . filepath else : self . log . info ( 'Poking for filepath %s to a non empty directory' , self . filepath ) result . pop ( 0 ) return bool ( result )  and result [ 0 ] [ 'file_type' ] == 'f'"}
{"code_tokens": "def clear_task_instances ( tis , session , activate_dag_runs = True , dag = None , ) : job_ids = [ ] for ti in tis : if ti . state == State . RUNNING : if ti . job_id : ti . state = State . SHUTDOWN job_ids . append ( ti . job_id ) else : task_id = ti . task_id if dag and dag . has_task ( task_id ) : task = dag . get_task ( task_id ) task_retries = task . retries ti . max_tries = ti . try_number + task_retries - 1 else : ti . max_tries = max ( ti . max_tries , ti . try_number - 1 ) ti . state = State . NONE session . merge ( ti ) if job_ids : from airflow . jobs import BaseJob as BJ for job in session . query ( BJ ) . filter ( BJ . id . in_ (  job_ids ) ) . all ( ) : job . state = State . SHUTDOWN if activate_dag_runs and tis : from airflow . models . dagrun import DagRun drs = session . query ( DagRun ) . filter ( DagRun . dag_id . in_ ( { ti . dag_id for ti in tis } ) , DagRun . execution_date . in_ ( { ti . execution_date for ti in tis } ) , ) . all ( ) for dr in drs : dr . state = State . RUNNING dr . start_date = timezone . utcnow ( )"}
{"code_tokens": "def try_number ( self ) : if self . state == State .  RUNNING : return self . _try_number return self . _try_number + 1"}
{"code_tokens": "def generate_command ( dag_id , task_id , execution_date , mark_success = False , ignore_all_deps = False , ignore_depends_on_past = False , ignore_task_deps = False , ignore_ti_state = False , local = False , pickle_id = None , file_path = None , raw = False , job_id = None , pool = None , cfg_path = None ) : iso = execution_date . isoformat ( ) cmd = [ \"airflow\" , \"run\" , str ( dag_id ) , str ( task_id ) , str ( iso ) ] cmd . extend ( [ \"--mark_success\" ] ) if mark_success else None cmd . extend ( [ \"--pickle\" , str ( pickle_id ) ] ) if pickle_id else None cmd . extend ( [ \"--job_id\" , str ( job_id ) ] ) if job_id else  None cmd . extend ( [ \"-A\" ] ) if ignore_all_deps else None cmd . extend ( [ \"-i\" ] ) if ignore_task_deps else None cmd . extend ( [ \"-I\" ] ) if ignore_depends_on_past else None cmd . extend ( [ \"--force\" ] ) if ignore_ti_state else None cmd . extend ( [ \"--local\" ] ) if local else None cmd . extend ( [ \"--pool\" , pool ] ) if pool else None cmd . extend ( [ \"--raw\" ] ) if raw else None cmd . extend ( [ \"-sd\" , file_path ] ) if file_path else None cmd . extend ( [ \"--cfg_path\" , cfg_path ] ) if cfg_path else None return cmd"}
{"code_tokens": "def current_state ( self , session = None ) : TI = TaskInstance ti = session . query ( TI ) . filter ( TI . dag_id == self . dag_id , TI . task_id == self . task_id , TI . execution_date == self . execution_date , ) . all ( ) if ti  : state = ti [ 0 ] . state else : state = None return state"}
{"code_tokens": "def error ( self , session = None ) : self . log . error ( \"Recording the task instance as FAILED\" ) self . state = State . FAILED session . merge ( self ) session . commit (  )"}
{"code_tokens": "def refresh_from_db ( self , session = None , lock_for_update = False ) : TI = TaskInstance qry = session . query ( TI ) . filter ( TI . dag_id == self . dag_id , TI . task_id == self . task_id , TI . execution_date == self . execution_date ) if lock_for_update : ti = qry . with_for_update ( ) . first ( ) else : ti = qry . first ( ) if ti : self . state = ti . state self . start_date = ti . start_date self . end_date = ti . end_date self . try_number = ti . _try_number self . max_tries = ti . max_tries self . hostname = ti . hostname self . pid = ti . pid self .  executor_config = ti . executor_config else : self . state = None"}
{"code_tokens": "def clear_xcom_data ( self , session = None ) : session . query ( XCom ) . filter ( XCom . dag_id == self . dag_id , XCom . task_id == self . task_id , XCom . execution_date == self . execution_date  ) . delete ( ) session . commit ( )"}
{"code_tokens": "def key ( self ) : return self . dag_id , self . task_id , self . execution_date , self  . try_number"}
{"code_tokens": "def are_dependents_done ( self , session = None ) : task = self . task if not task . downstream_task_ids : return True ti = session . query ( func . count ( TaskInstance . task_id ) ) . filter ( TaskInstance . dag_id == self . dag_id , TaskInstance . task_id . in_ ( task . downstream_task_ids ) , TaskInstance .  execution_date == self . execution_date , TaskInstance . state == State . SUCCESS , ) count = ti [ 0 ] [ 0 ] return count == len ( task . downstream_task_ids )"}
{"code_tokens": "def next_retry_datetime ( self ) : delay = self . task . retry_delay if self . task . retry_exponential_backoff : min_backoff = int ( delay . total_seconds ( ) * ( 2 ** ( self . try_number - 2 ) ) ) hash = int ( hashlib . sha1 ( \"{}#{}#{}#{}\" . format ( self . dag_id , self . task_id , self . execution_date , self . try_number ) . encode ( 'utf-8' ) ) . hexdigest ( ) , 16 ) modded_hash = min_backoff + hash % min_backoff delay_backoff_in_seconds = min ( modded_hash  , timedelta . max . total_seconds ( ) - 1 ) delay = timedelta ( seconds = delay_backoff_in_seconds ) if self . task . max_retry_delay : delay = min ( self . task . max_retry_delay , delay ) return self . end_date + delay"}
{"code_tokens": "def ready_for_retry ( self ) : return ( self . state == State .  UP_FOR_RETRY and self . next_retry_datetime ( ) < timezone . utcnow ( ) )"}
{"code_tokens": "def pool_full ( self , session ) : if not self . task . pool : return False pool = ( session . query ( Pool ) . filter ( Pool . pool == self . task . pool ) . first ( ) ) if not pool :  return False open_slots = pool . open_slots ( session = session ) return open_slots <= 0"}
{"code_tokens": "def get_dagrun ( self , session ) : from airflow . models . dagrun import DagRun dr = session . query ( DagRun ) . filter ( DagRun . dag_id == self . dag_id , DagRun . execution_date == self .  execution_date ) . first ( ) return dr"}
{"code_tokens": "def xcom_push ( self , key , value , execution_date = None ) : if execution_date and execution_date < self . execution_date : raise ValueError ( 'execution_date can not be in the past (current ' 'execution_date is {}; received {})' . format  ( self . execution_date , execution_date ) ) XCom . set ( key = key , value = value , task_id = self . task_id , dag_id = self . dag_id , execution_date = execution_date or self . execution_date )"}
{"code_tokens": "def xcom_pull ( self , task_ids = None , dag_id = None , key = XCOM_RETURN_KEY , include_prior_dates = False ) : if dag_id is None : dag_id = self . dag_id pull_fn = functools . partial ( XCom . get_one , execution_date = self . execution_date , key = key , dag_id = dag_id , include_prior_dates = include_prior_dates ) if is_container ( task_ids ) : return tuple ( pull_fn (  task_id = t ) for t in task_ids ) else : return pull_fn ( task_id = task_ids )"}
{"code_tokens": "def init_run_context ( self , raw = False ) : self . raw = raw self .  _set_context ( self )"}
{"code_tokens": "def close ( self ) : if self . closed : return super ( ) . close ( ) if not self . upload_on_close : return local_loc = os . path . join ( self . local_base , self . log_relative_path ) remote_loc = os . path . join ( self . remote_base , self . log_relative_path ) if os . path . exists ( local_loc ) : with open ( local_loc , 'r' ) as logfile : log = logfile . read ( ) self . wasb_write ( log , remote_loc  , append = True ) if self . delete_local_copy : shutil . rmtree ( os . path . dirname ( local_loc ) ) self . closed = True"}
{"code_tokens": "def get_conn ( self ) : if not self . _conn : http_authorized = self . _authorize ( ) self . _conn = build ( 'compute' , self . api_version , http = http_authorized , cache_discovery = False ) return self .  _conn"}
{"code_tokens": "def start_instance ( self , zone , resource_id , project_id = None ) : response = self . get_conn ( ) . instances ( ) . start ( project = project_id , zone = zone , instance = resource_id ) . execute ( num_retries = self . num_retries ) try : operation_name = response [ \"name\" ] except KeyError : raise AirflowException ( \"Wrong response '{}' returned - it should contain  \" \"'name' field\" . format ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name , zone = zone )"}
{"code_tokens": "def set_machine_type ( self , zone , resource_id , body , project_id = None ) : response = self . _execute_set_machine_type ( zone , resource_id , body , project_id ) try : operation_name = response [ \"name\" ] except KeyError  : raise AirflowException ( \"Wrong response '{}' returned - it should contain \" \"'name' field\" . format ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name , zone = zone )"}
{"code_tokens": "def get_instance_template ( self , resource_id , project_id = None ) : response = self . get_conn ( ) . instanceTemplates ( ) . get ( project = project_id ,  instanceTemplate = resource_id ) . execute ( num_retries = self . num_retries ) return response"}
{"code_tokens": "def insert_instance_template ( self , body , request_id = None , project_id = None ) : response = self . get_conn ( ) . instanceTemplates ( ) . insert ( project = project_id , body = body , requestId = request_id ) . execute ( num_retries = self . num_retries ) try : operation_name = response [ \"name\" ] except KeyError : raise AirflowException ( \"Wrong response '{}' returned - it should contain \" \"'name' field\" . format ( response  ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )"}
{"code_tokens": "def get_instance_group_manager ( self , zone , resource_id , project_id = None ) : response = self . get_conn ( ) . instanceGroupManagers ( ) . get ( project = project_id , zone = zone , instanceGroupManager  = resource_id ) . execute ( num_retries = self . num_retries ) return response"}
{"code_tokens": "def patch_instance_group_manager ( self , zone , resource_id , body , request_id = None , project_id = None ) : response = self . get_conn ( ) . instanceGroupManagers ( ) . patch ( project = project_id , zone = zone , instanceGroupManager = resource_id , body = body , requestId = request_id ) . execute ( num_retries = self . num_retries ) try : operation_name = response [ \"name\" ] except KeyError : raise AirflowException ( \"Wrong response '{}' returned - it should contain \" \"'name' field\" . format  ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name , zone = zone )"}
{"code_tokens": "def _wait_for_operation_to_complete ( self , project_id , operation_name , zone = None ) : service = self . get_conn ( ) while True : if zone is None : operation_response = self . _check_global_operation_status ( service , operation_name , project_id ) else : operation_response = self . _check_zone_operation_status ( service , operation_name , project_id , zone , self . num_retries ) if operation_response . get ( \"status\" ) == GceOperationStatus . DONE : error = operation_response . get ( \"error\" ) if error : code = operation_response . get ( \"httpErrorStatusCode\" ) msg = operation_response . get ( \"httpErrorMessage\" ) error_msg = str ( error .  get ( \"errors\" ) ) [ 1 : - 1 ] raise AirflowException ( \"{} {}: \" . format ( code , msg ) + error_msg ) return time . sleep ( TIME_TO_SLEEP_IN_SECONDS )"}
{"code_tokens": "def check_for_bucket ( self , bucket_name ) : try : self . get_conn ( ) . head_bucket ( Bucket = bucket_name ) return True except  ClientError as e : self . log . info ( e . response [ \"Error\" ] [ \"Message\" ] ) return False"}
{"code_tokens": "def create_bucket ( self , bucket_name , region_name = None ) : s3_conn = self . get_conn ( ) if not region_name : region_name = s3_conn . meta . region_name if region_name == 'us-east-1' : self . get_conn ( ) . create_bucket ( Bucket = bucket_name ) else : self . get_conn ( ) . create_bucket ( Bucket = bucket_name , CreateBucketConfiguration  = { 'LocationConstraint' : region_name } )"}
{"code_tokens": "def check_for_prefix ( self , bucket_name , prefix , delimiter ) : prefix = prefix + delimiter if prefix [ - 1 ] != delimiter else prefix prefix_split = re . split ( r'(\\w+[{d}])$' . format ( d = delimiter ) , prefix , 1 ) previous_level = prefix_split [ 0 ] plist = self . list_prefixes ( bucket_name , previous_level , delimiter ) return False if  plist is None else prefix in plist"}
{"code_tokens": "def list_prefixes ( self , bucket_name , prefix = '' , delimiter = '' , page_size = None , max_items = None ) : config = { 'PageSize' : page_size , 'MaxItems' : max_items , } paginator = self . get_conn ( ) . get_paginator ( 'list_objects_v2' ) response = paginator . paginate ( Bucket = bucket_name , Prefix = prefix , Delimiter = delimiter , PaginationConfig = config ) has_results =  False prefixes = [ ] for page in response : if 'CommonPrefixes' in page : has_results = True for p in page [ 'CommonPrefixes' ] : prefixes . append ( p [ 'Prefix' ] ) if has_results : return prefixes"}
{"code_tokens": "def list_keys ( self , bucket_name , prefix = '' , delimiter = '' , page_size = None , max_items = None ) : config = { 'PageSize' : page_size , 'MaxItems' : max_items , } paginator = self . get_conn ( ) . get_paginator ( 'list_objects_v2' ) response = paginator . paginate ( Bucket = bucket_name , Prefix = prefix , Delimiter = delimiter , PaginationConfig = config ) has_results = False keys = [ ] for page in response : if 'Contents' in page : has_results =  True for k in page [ 'Contents' ] : keys . append ( k [ 'Key' ] ) if has_results : return keys"}
{"code_tokens": "def check_for_key ( self , key , bucket_name = None ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) try : self . get_conn ( ) . head_object ( Bucket = bucket_name , Key = key ) return True except ClientError as  e : self . log . info ( e . response [ \"Error\" ] [ \"Message\" ] ) return False"}
{"code_tokens": "def get_key ( self , key , bucket_name = None ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) obj = self . get_resource_type ( 's3' ) . Object ( bucket_name , key ) obj . load ( )  return obj"}
{"code_tokens": "def read_key ( self , key , bucket_name = None ) : obj = self . get_key ( key , bucket_name ) return obj . get ( ) [ 'Body' ] .  read ( ) . decode ( 'utf-8' )"}
{"code_tokens": "def select_key ( self , key , bucket_name = None , expression = 'SELECT * FROM S3Object' , expression_type = 'SQL' , input_serialization = None , output_serialization = None ) : if input_serialization is None : input_serialization = { 'CSV' : { } } if output_serialization is None : output_serialization = { 'CSV' : { } } if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) response = self . get_conn ( ) . select_object_content ( Bucket = bucket_name , Key = key , Expression = expression , ExpressionType = expression_type , InputSerialization = input_serialization , OutputSerialization = output_serialization ) return ''  . join ( event [ 'Records' ] [ 'Payload' ] . decode ( 'utf-8' ) for event in response [ 'Payload' ] if 'Records' in event )"}
{"code_tokens": "def check_for_wildcard_key ( self , wildcard_key , bucket_name = None , delimiter = '' ) : return self . get_wildcard_key ( wildcard_key = wildcard_key  , bucket_name = bucket_name , delimiter = delimiter ) is not None"}
{"code_tokens": "def get_wildcard_key ( self , wildcard_key , bucket_name = None , delimiter = '' ) : if not bucket_name : ( bucket_name , wildcard_key ) = self . parse_s3_url ( wildcard_key ) prefix = re . split ( r'[*]' , wildcard_key , 1 ) [ 0 ] klist = self . list_keys ( bucket_name , prefix = prefix , delimiter = delimiter ) if klist : key_matches =  [ k for k in klist if fnmatch . fnmatch ( k , wildcard_key ) ] if key_matches : return self . get_key ( key_matches [ 0 ] , bucket_name )"}
{"code_tokens": "def load_file ( self , filename , key , bucket_name = None , replace = False , encrypt = False ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) if not replace and self . check_for_key ( key , bucket_name ) : raise ValueError ( \"The key {key} already exists.\" . format ( key = key )  ) extra_args = { } if encrypt : extra_args [ 'ServerSideEncryption' ] = \"AES256\" client = self . get_conn ( ) client . upload_file ( filename , bucket_name , key , ExtraArgs = extra_args )"}
{"code_tokens": "def load_string ( self , string_data , key , bucket_name = None , replace = False , encrypt = False , encoding = 'utf-8' ) : self . load_bytes ( string_data . encode ( encoding  ) , key = key , bucket_name = bucket_name , replace = replace , encrypt = encrypt )"}
{"code_tokens": "def load_bytes ( self , bytes_data , key , bucket_name = None , replace = False , encrypt = False ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) if not replace and self . check_for_key ( key , bucket_name ) : raise ValueError ( \"The key {key} already exists.\" .  format ( key = key ) ) extra_args = { } if encrypt : extra_args [ 'ServerSideEncryption' ] = \"AES256\" filelike_buffer = BytesIO ( bytes_data ) client = self . get_conn ( ) client . upload_fileobj ( filelike_buffer , bucket_name , key , ExtraArgs = extra_args )"}
{"code_tokens": "def load_file_obj ( self , file_obj , key , bucket_name = None , replace = False , encrypt = False ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) if not replace and self . check_for_key ( key , bucket_name ) : raise ValueError ( \"The key {key} already exists.\" . format ( key = key ) ) extra_args = { } if encrypt : extra_args [ 'ServerSideEncryption' ] = \"AES256\" client = self . get_conn ( ) client . upload_fileobj ( file_obj , bucket_name , key ,  ExtraArgs = extra_args )"}
{"code_tokens": "def copy_object ( self , source_bucket_key , dest_bucket_key , source_bucket_name = None , dest_bucket_name = None , source_version_id = None ) : if dest_bucket_name is None : dest_bucket_name , dest_bucket_key = self . parse_s3_url ( dest_bucket_key ) else : parsed_url = urlparse ( dest_bucket_key ) if parsed_url . scheme != '' or parsed_url . netloc != '' : raise AirflowException ( 'If dest_bucket_name is provided, ' + 'dest_bucket_key should be relative path ' + 'from root level, rather than a full s3:// url' ) if source_bucket_name is None : source_bucket_name , source_bucket_key = self . parse_s3_url ( source_bucket_key ) else : parsed_url = urlparse ( source_bucket_key ) if parsed_url . scheme != '' or parsed_url . netloc != '' : raise AirflowException ( 'If source_bucket_name is provided, ' + 'source_bucket_key should be relative path ' + 'from root level, rather than a full s3:// url' ) CopySource = { 'Bucket' : source_bucket_name , 'Key' :  source_bucket_key , 'VersionId' : source_version_id } response = self . get_conn ( ) . copy_object ( Bucket = dest_bucket_name , Key = dest_bucket_key , CopySource = CopySource ) return response"}
{"code_tokens": "def _query_cassandra ( self ) : self . hook = CassandraHook ( cassandra_conn_id = self . cassandra_conn_id ) session = self . hook . get_conn ( ) cursor = session . execute (  self . cql ) return cursor"}
{"code_tokens": "def convert_user_type ( cls , name , value ) : names = value . _fields values = [ cls . convert_value ( name , getattr ( value , name )  ) for name in names ] return cls . generate_data_dict ( names , values )"}
{"code_tokens": "def send_email ( to , subject , html_content , files = None , dryrun = False , cc = None , bcc = None , mime_subtype = 'mixed' , sandbox_mode = False , ** kwargs ) : if files is None : files = [ ] mail = Mail ( ) from_email = kwargs . get ( 'from_email' ) or os . environ . get ( 'SENDGRID_MAIL_FROM' ) from_name = kwargs . get ( 'from_name' ) or os . environ . get ( 'SENDGRID_MAIL_SENDER' ) mail . from_email = Email ( from_email , from_name ) mail . subject = subject mail . mail_settings = MailSettings ( ) if sandbox_mode : mail . mail_settings . sandbox_mode = SandBoxMode ( enable = True ) personalization = Personalization ( ) to = get_email_address_list ( to ) for to_address in to : personalization . add_to ( Email ( to_address ) ) if cc : cc = get_email_address_list ( cc ) for cc_address in cc : personalization . add_cc ( Email ( cc_address ) ) if bcc : bcc = get_email_address_list ( bcc ) for bcc_address in bcc : personalization . add_bcc ( Email ( bcc_address ) ) pers_custom_args = kwargs . get ( 'personalization_custom_args' , None ) if isinstance ( pers_custom_args , dict ) : for key in pers_custom_args . keys ( ) : personalization . add_custom_arg ( CustomArg ( key , pers_custom_args [ key ] ) ) mail .  add_personalization ( personalization ) mail . add_content ( Content ( 'text/html' , html_content ) ) categories = kwargs . get ( 'categories' , [ ] ) for cat in categories : mail . add_category ( Category ( cat ) ) for fname in files : basename = os . path . basename ( fname ) attachment = Attachment ( ) attachment . type = mimetypes . guess_type ( basename ) [ 0 ] attachment . filename = basename attachment . disposition = \"attachment\" attachment . content_id = '<{0}>' . format ( basename ) with open ( fname , \"rb\" ) as f : attachment . content = base64 . b64encode ( f . read ( ) ) . decode ( 'utf-8' ) mail . add_attachment ( attachment ) _post_sendgrid_mail ( mail . get ( ) )"}
{"code_tokens": "def get_conn ( self ) : if not self . _client : self . _client = SpeechClient ( credentials = self . _get_credentials ( ) ) return  self . _client"}
{"code_tokens": "def recognize_speech ( self , config , audio , retry = None , timeout = None ) : client = self . get_conn ( ) response = client . recognize  ( config = config , audio = audio , retry = retry , timeout = timeout ) self . log . info ( \"Recognised speech: %s\" % response ) return response"}
{"code_tokens": "def execute ( self , context ) : self . _hook = SparkSqlHook ( sql = self . _sql , conf = self . _conf , conn_id = self . _conn_id , total_executor_cores = self . _total_executor_cores , executor_cores = self . _executor_cores , executor_memory = self . _executor_memory , keytab = self . _keytab , principal = self . _principal ,  name = self . _name , num_executors = self . _num_executors , master = self . _master , yarn_queue = self . _yarn_queue ) self . _hook . run_query ( )"}
{"code_tokens": "def load_entrypoint_plugins ( entry_points , airflow_plugins ) : for entry_point in entry_points : log . debug ( 'Importing entry_point plugin %s' , entry_point . name ) plugin_obj = entry_point . load ( ) if is_valid_plugin ( plugin_obj , airflow_plugins ) : if callable ( getattr ( plugin_obj , 'on_load' , None ) ) : plugin_obj . on_load ( ) airflow_plugins . append ( plugin_obj )  return airflow_plugins"}
{"code_tokens": "def is_valid_plugin ( plugin_obj , existing_plugins ) : if ( inspect . isclass ( plugin_obj ) and issubclass ( plugin_obj , AirflowPlugin ) and (  plugin_obj is not AirflowPlugin ) ) : plugin_obj . validate ( ) return plugin_obj not in existing_plugins return False"}
{"code_tokens": "def skip ( self , dag_run , execution_date , tasks , session = None ) : if not tasks : return task_ids = [ d . task_id for d in tasks ] now = timezone . utcnow ( ) if dag_run : session . query ( TaskInstance ) . filter ( TaskInstance . dag_id == dag_run . dag_id , TaskInstance . execution_date == dag_run . execution_date , TaskInstance . task_id . in_ ( task_ids ) ) . update ( { TaskInstance . state : State . SKIPPED , TaskInstance . start_date : now , TaskInstance . end_date : now } , synchronize_session = False ) session . commit ( ) else : assert execution_date is not None , \"Execution date is None and no dag run\" self . log . warning ( \"No DAG RUN present this should not happen\" ) for task in tasks : ti = TaskInstance ( task , execution_date = execution_date ) ti . state = State . SKIPPED ti .  start_date = now ti . end_date = now session . merge ( ti ) session . commit ( )"}
{"code_tokens": "def get_conn ( self ) : conn = self . get_connection ( self . conn_id ) service_options = conn . extra_dejson self . account_name = service_options . get ( 'account_name' ) adlCreds = lib . auth ( tenant_id = service_options . get ( 'tenant' ) , client_secret = conn . password , client_id = conn . login ) adlsFileSystemClient = core . AzureDLFileSystem ( adlCreds , store_name = self . account_name  ) adlsFileSystemClient . connect ( ) return adlsFileSystemClient"}
{"code_tokens": "def check_for_file ( self , file_path ) : try : files = self . connection . glob ( file_path , details  = False , invalidate_cache = True ) return len ( files ) == 1 except FileNotFoundError : return False"}
{"code_tokens": "def upload_file ( self , local_path , remote_path , nthreads = 64 , overwrite = True , buffersize = 4194304 , blocksize = 4194304 ) : multithread . ADLUploader ( self . connection , lpath = local_path , rpath = remote_path , nthreads = nthreads , overwrite = overwrite , buffersize = buffersize , blocksize = blocksize ) "}
{"code_tokens": "def list ( self , path ) : if \"*\" in path : return self . connection . glob ( path ) else : return self . connection . walk (  path )"}
{"code_tokens": "def execute ( self , context ) : self . hook = self . get_hook ( ) self . hook . get_conn ( ) self . query_execution_context [ 'Database' ] = self . database self . result_configuration [ 'OutputLocation' ] = self . output_location self . query_execution_id = self . hook . run_query ( self . query , self . query_execution_context , self . result_configuration , self . client_request_token ) query_status = self . hook . poll_query_status ( self . query_execution_id , self . max_tries ) if query_status in AWSAthenaHook . FAILURE_STATES : raise Exception ( 'Final state of Athena job is {}, query_execution_id is {}.'  . format ( query_status , self . query_execution_id ) ) elif not query_status or query_status in AWSAthenaHook . INTERMEDIATE_STATES : raise Exception ( 'Final state of Athena job is {}. ' 'Max tries of poll status exceeded, query_execution_id is {}.' . format ( query_status , self . query_execution_id ) )"}
{"code_tokens": "def uncompress_file ( input_file_name , file_extension , dest_dir ) : if file_extension . lower ( ) not in ( '.gz' , '.bz2' ) : raise NotImplementedError ( \"Received {} format. Only gz and bz2 \" \"files can currently be uncompressed.\" . format ( file_extension ) ) if file_extension . lower ( ) == '.gz' : fmodule = gzip . GzipFile elif file_extension . lower ( ) == '.bz2'  : fmodule = bz2 . BZ2File with fmodule ( input_file_name , mode = 'rb' ) as f_compressed , NamedTemporaryFile ( dir = dest_dir , mode = 'wb' , delete = False ) as f_uncompressed : shutil . copyfileobj ( f_compressed , f_uncompressed ) return f_uncompressed . name"}
{"code_tokens": "def _query_mssql ( self ) : mssql = MsSqlHook ( mssql_conn_id = self . mssql_conn_id ) conn = mssql . get_conn ( ) cursor = conn . cursor ( ) cursor . execute  ( self . sql ) return cursor"}
{"code_tokens": "def action_logging ( f ) : @ functools . wraps ( f ) def wrapper ( * args , ** kwargs ) : assert args assert isinstance ( args [ 0 ] , Namespace ) , \"1st positional argument should be argparse.Namespace instance, \" \"but {}\" . format ( args [ 0 ] ) metrics = _build_metrics ( f . __name__ , args [ 0 ] ) cli_action_loggers . on_pre_execution ( ** metrics ) try : return f ( * args , ** kwargs ) except Exception as e : metrics [ 'error' ] = e raise finally : metrics [ 'end_datetime' ] = datetime  . utcnow ( ) cli_action_loggers . on_post_execution ( ** metrics ) return wrapper"}
{"code_tokens": "def _build_metrics ( func_name , namespace ) : metrics = { 'sub_command' : func_name , 'start_datetime' : datetime . utcnow ( ) , 'full_command' : '{}' . format ( list ( sys . argv ) ) , 'user' : getpass . getuser ( ) } assert isinstance ( namespace , Namespace ) tmp_dic = vars ( namespace ) metrics [ 'dag_id' ] = tmp_dic . get ( 'dag_id' ) metrics [ 'task_id' ] = tmp_dic . get ( 'task_id' ) metrics [ 'execution_date' ] = tmp_dic . get ( 'execution_date' ) metrics [ 'host_name' ] = socket . gethostname ( ) extra = json . dumps ( dict ( ( k , metrics [ k ] ) for k in ( 'host_name' , 'full_command' )  ) ) log = Log ( event = 'cli_{}' . format ( func_name ) , task_instance = None , owner = metrics [ 'user' ] , extra = extra , task_id = metrics . get ( 'task_id' ) , dag_id = metrics . get ( 'dag_id' ) , execution_date = metrics . get ( 'execution_date' ) ) metrics [ 'log' ] = log return metrics"}
{"code_tokens": "def _create_cgroup ( self , path ) : node = trees . Tree ( ) . root path_split = path . split ( os . sep ) for path_element in path_split : name_to_node = { x . name : x for x in node . children } if path_element not in name_to_node : self . log . debug ( \"Creating cgroup %s in %s\" , path_element , node . path ) node = node . create_cgroup ( path_element ) else : self . log . debug ( \"Not creating cgroup %s in %s since it already exists\" , path_element , node .  path ) node = name_to_node [ path_element ] return node"}
{"code_tokens": "def _delete_cgroup ( self , path ) : node = trees . Tree ( ) . root path_split = path . split ( \"/\" ) for path_element in path_split : name_to_node = { x . name : x for x in node . children } if path_element not in name_to_node : self . log . warning ( \"Cgroup does not exist: %s\" , path ) return else  : node = name_to_node [ path_element ] parent = node . parent self . log . debug ( \"Deleting cgroup %s/%s\" , parent , node . name ) parent . delete_cgroup ( node . name )"}
{"code_tokens": "def _parse_host ( host ) : urlparse_host = urlparse . urlparse ( host ) . hostname if urlparse_host : return urlparse_host else  : return host"}
{"code_tokens": "def _do_api_call ( self , endpoint_info , json ) : method , endpoint = endpoint_info url = 'https://{host}/{endpoint}' . format ( host = self . _parse_host ( self . databricks_conn . host ) , endpoint = endpoint ) if 'token' in self . databricks_conn . extra_dejson : self . log . info ( 'Using token auth.' ) auth = _TokenAuth ( self . databricks_conn . extra_dejson [ 'token' ] ) else : self . log . info ( 'Using basic auth.' ) auth = ( self . databricks_conn . login , self . databricks_conn . password ) if method == 'GET' : request_func = requests . get elif method == 'POST' : request_func = requests . post else : raise AirflowException ( 'Unexpected HTTP Method: ' + method ) attempt_num = 1 while True : try : response = request_func ( url , json = json , auth = auth , headers = USER_AGENT_HEADER , timeout = self . timeout_seconds ) response . raise_for_status ( ) return response . json ( ) except requests_exceptions . RequestException as e : if not _retryable_error ( e ) : raise AirflowException ( 'Response: {0}, Status Code:  {1}' . format ( e . response . content , e . response . status_code ) ) self . _log_request_error ( attempt_num , e ) if attempt_num == self . retry_limit : raise AirflowException ( ( 'API requests to Databricks failed {} times. ' + 'Giving up.' ) . format ( self . retry_limit ) ) attempt_num += 1 sleep ( self . retry_delay )"}
{"code_tokens": "def get_conn ( self ) : if not self . conn : connection = self . get_connection ( self . conn_id ) extras = connection . extra_dejson self . conn = Salesforce ( username = connection . login , password = connection . password , security_token = extras [ 'security_token' ] , instance_url  = connection . host , sandbox = extras . get ( 'sandbox' , False ) ) return self . conn"}
{"code_tokens": "def make_query ( self , query ) : conn = self . get_conn ( ) self . log . info ( \"Querying for all objects\" ) query_results = conn . query_all ( query ) self . log . info ( \"Received results: Total size:  %s; Done: %s\" , query_results [ 'totalSize' ] , query_results [ 'done' ] ) return query_results"}
{"code_tokens": "def describe_object ( self , obj ) : conn = self . get_conn ( ) return conn . __getattr__ ( obj ) . describe ( ) "}
{"code_tokens": "def get_available_fields ( self , obj ) : self . get_conn ( ) obj_description = self . describe_object ( obj ) return [ field [ 'name' ] for field in obj_description [ 'fields' ]  ]"}
{"code_tokens": "def get_object_from_salesforce ( self , obj , fields ) : query = \"SELECT {} FROM {}\" . format ( \",\" . join ( fields ) , obj ) self . log . info ( \"Making query to Salesforce: %s\" , query if len ( query ) < 30 else \" ... \" . join ( [ query [ : 15 ] ,  query [ - 15 : ] ] ) ) return self . make_query ( query )"}
{"code_tokens": "def _to_timestamp ( cls , column ) : try : column = pd . to_datetime ( column ) except ValueError : log = LoggingMixin ( ) . log log . warning ( \"Could not convert field to timestamps: %s\" , column . name ) return column converted = [ ] for value in column : try : converted . append  ( value . timestamp ( ) ) except ( ValueError , AttributeError ) : converted . append ( pd . np . NaN ) return pd . Series ( converted , index = column . index )"}
{"code_tokens": "def write_object_to_file ( self , query_results , filename , fmt = \"csv\" , coerce_to_timestamp = False , record_time_added = False ) : fmt = fmt . lower ( ) if fmt not in [ 'csv' , 'json' , 'ndjson' ] : raise ValueError ( \"Format value is not recognized: {}\" . format ( fmt ) ) df = pd . DataFrame . from_records ( query_results , exclude = [ \"attributes\" ] ) df . columns = [ column . lower ( ) for column in df . columns ] if coerce_to_timestamp and df . shape [ 0 ] > 0 : object_name = query_results [ 0 ] [ 'attributes' ] [ 'type' ] self . log . info ( \"Coercing timestamps for: %s\" , object_name ) schema = self . describe_object ( object_name ) possible_timestamp_cols = [ field [ 'name' ] . lower ( ) for field in schema [ 'fields' ] if field [ 'type' ] in [ \"date\" , \"datetime\" ] and field [ 'name' ] . lower ( ) in df . columns ] df [ possible_timestamp_cols ] = df [ possible_timestamp_cols ] . apply ( self . _to_timestamp ) if record_time_added : fetched_time = time . time ( ) df [ \"time_fetched_from_salesforce\" ] = fetched_time if fmt == \"csv\"  : self . log . info ( \"Cleaning data and writing to CSV\" ) possible_strings = df . columns [ df . dtypes == \"object\" ] df [ possible_strings ] = df [ possible_strings ] . apply ( lambda x : x . str . replace ( \"\\r\\n\" , \"\" ) . str . replace ( \"\\n\" , \"\" ) ) df . to_csv ( filename , index = False ) elif fmt == \"json\" : df . to_json ( filename , \"records\" , date_unit = \"s\" ) elif fmt == \"ndjson\" : df . to_json ( filename , \"records\" , lines = True , date_unit = \"s\" ) return df"}
{"code_tokens": "def get_conn ( self ) : if self . client is not None : return self . client options = self . extras if options . get ( 'ssl' , False ) : options . update ( { 'ssl_cert_reqs' : CERT_NONE } ) self . client = MongoClient ( self  . uri , ** options ) return self . client"}
{"code_tokens": "def get_collection ( self , mongo_collection , mongo_db = None ) : mongo_db = mongo_db if mongo_db is not None else self  . connection . schema mongo_conn = self . get_conn ( ) return mongo_conn . get_database ( mongo_db ) . get_collection ( mongo_collection )"}
{"code_tokens": "def replace_many ( self , mongo_collection , docs , filter_docs = None , mongo_db = None , upsert = False , collation = None , ** kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) if not filter_docs : filter_docs = [ { '_id' : doc [ '_id' ] } for doc in docs ] requests = [ ReplaceOne ( filter_docs [ i ] , docs [ i ] ,  upsert = upsert , collation = collation ) for i in range ( len ( docs ) ) ] return collection . bulk_write ( requests , ** kwargs )"}
{"code_tokens": "def has_mail_attachment ( self , name , mail_folder = 'INBOX' , check_regex = False ) : mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only = True ) return len ( mail_attachments ) >  0"}
{"code_tokens": "def retrieve_mail_attachments ( self , name , mail_folder = 'INBOX' , check_regex = False , latest_only = False , not_found_mode = 'raise' ) : mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only ) if not mail_attachments : self . _handle_not_found_mode (  not_found_mode ) return mail_attachments"}
{"code_tokens": "def download_mail_attachments ( self , name , local_output_directory , mail_folder = 'INBOX' , check_regex = False , latest_only = False , not_found_mode = 'raise' ) : mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only ) if not mail_attachments : self . _handle_not_found_mode ( not_found_mode )  self . _create_files ( mail_attachments , local_output_directory )"}
{"code_tokens": "def get_attachments_by_name ( self , name , check_regex , find_first = False ) : attachments = [ ] for part in self . mail . walk ( ) : mail_part = MailPart ( part ) if mail_part . is_attachment ( ) : found_attachment = mail_part . has_matching_name ( name ) if check_regex else mail_part . has_equal_name ( name ) if found_attachment : file_name , file_payload = mail_part . get_file ( ) self . log . info ( 'Found attachment: {}' . format ( file_name ) ) attachments . append ( ( file_name , file_payload ) ) if find_first :  break return attachments"}
{"code_tokens": "def get_file ( self ) : return self . part . get_filename ( ) , self . part . get_payload  ( decode = True )"}
{"code_tokens": "def put_records ( self , records ) : firehose_conn = self . get_conn ( ) response = firehose_conn . put_record_batch (  DeliveryStreamName = self . delivery_stream , Records = records ) return response"}
{"code_tokens": "def _get_dep_statuses ( self , ti , session , dep_context ) : if dep_context . ignore_in_reschedule_period : yield self . _passing_status ( reason = \"The context specified that being in a reschedule period was \" \"permitted.\" ) return if ti . state not in self . RESCHEDULEABLE_STATES : yield self . _passing_status ( reason = \"The task instance is not in State_UP_FOR_RESCHEDULE or NONE state.\" ) return task_reschedules = TaskReschedule . find_for_task_instance ( task_instance = ti ) if not task_reschedules : yield self . _passing_status ( reason = \"There is no reschedule request for this task instance.\" ) return now = timezone . utcnow ( ) next_reschedule_date = task_reschedules [ - 1 ] . reschedule_date if now >= next_reschedule_date : yield self . _passing_status ( reason = \"Task instance id ready for reschedule.\" ) return yield self . _failing_status ( reason = \"Task is not ready for reschedule  yet but will be rescheduled \" \"automatically. Current date is {0} and task will be rescheduled \" \"at {1}.\" . format ( now . isoformat ( ) , next_reschedule_date . isoformat ( ) ) )"}
{"code_tokens": "def send_email ( to , subject , html_content , files = None , dryrun = False , cc = None , bcc = None , mime_subtype = 'mixed' , mime_charset = 'utf-8' , ** kwargs ) : path , attr = configuration . conf . get ( 'email' , 'EMAIL_BACKEND' ) . rsplit ( '.' , 1 ) module = importlib . import_module ( path ) backend = getattr ( module , attr ) to = get_email_address_list ( to ) to = \", \" . join ( to ) return backend  ( to , subject , html_content , files = files , dryrun = dryrun , cc = cc , bcc = bcc , mime_subtype = mime_subtype , mime_charset = mime_charset , ** kwargs )"}
{"code_tokens": "def send_email_smtp ( to , subject , html_content , files = None , dryrun = False , cc = None , bcc = None , mime_subtype = 'mixed' , mime_charset = 'utf-8' , ** kwargs ) : smtp_mail_from = configuration . conf . get ( 'smtp' , 'SMTP_MAIL_FROM' ) to = get_email_address_list ( to ) msg = MIMEMultipart ( mime_subtype ) msg [ 'Subject' ] = subject msg [ 'From' ] = smtp_mail_from msg [ 'To' ] = \", \" . join ( to ) recipients = to if cc : cc = get_email_address_list ( cc ) msg [ 'CC' ] = \", \" . join ( cc ) recipients = recipients + cc if bcc : bcc = get_email_address_list ( bcc ) recipients = recipients + bcc msg  [ 'Date' ] = formatdate ( localtime = True ) mime_text = MIMEText ( html_content , 'html' , mime_charset ) msg . attach ( mime_text ) for fname in files or [ ] : basename = os . path . basename ( fname ) with open ( fname , \"rb\" ) as f : part = MIMEApplication ( f . read ( ) , Name = basename ) part [ 'Content-Disposition' ] = 'attachment; filename=\"%s\"' % basename part [ 'Content-ID' ] = '<%s>' % basename msg . attach ( part ) send_MIME_email ( smtp_mail_from , recipients , msg , dryrun )"}
{"code_tokens": "def process_result_value ( self , value , dialect ) : if value is not None : if value . tzinfo is None : value = value . replace ( tzinfo = utc ) else : value = value  . astimezone ( utc ) return value"}
{"code_tokens": "def check_for_blob ( self , container_name , blob_name , ** kwargs ) : return self . connection . exists ( container_name , blob_name , ** kwargs  )"}
{"code_tokens": "def check_for_prefix ( self , container_name , prefix , ** kwargs ) : matches = self . connection . list_blobs ( container_name , prefix , num_results = 1 , **  kwargs ) return len ( list ( matches ) ) > 0"}
{"code_tokens": "def load_string ( self , string_data , container_name , blob_name , ** kwargs ) : self . connection  . create_blob_from_text ( container_name , blob_name , string_data , ** kwargs )"}
{"code_tokens": "def read_file ( self , container_name , blob_name , ** kwargs ) : return self  . connection . get_blob_to_text ( container_name , blob_name , ** kwargs ) . content"}
{"code_tokens": "def delete_file ( self , container_name , blob_name , is_prefix = False , ignore_if_missing = False , ** kwargs ) : if is_prefix : blobs_to_delete = [ blob . name for blob in self . connection . list_blobs ( container_name , prefix = blob_name , ** kwargs ) ] elif self . check_for_blob ( container_name , blob_name ) : blobs_to_delete = [ blob_name ] else : blobs_to_delete = [ ] if not ignore_if_missing and len ( blobs_to_delete ) == 0 : raise AirflowException ( 'Blob(s) not found:  {}' . format ( blob_name ) ) for blob_uri in blobs_to_delete : self . log . info ( \"Deleting blob: \" + blob_uri ) self . connection . delete_blob ( container_name , blob_uri , delete_snapshots = 'include' , ** kwargs )"}
{"code_tokens": "def mlsd ( conn , path = \"\" , facts = None ) : facts = facts or [ ] if facts : conn . sendcmd ( \"OPTS MLST \" + \";\" . join ( facts ) + \";\" ) if path : cmd = \"MLSD %s\" % path else : cmd = \"MLSD\" lines = [ ] conn . retrlines ( cmd , lines . append ) for line in lines : facts_found , _ , name = line . rstrip ( ftplib . CRLF ) . partition ( ' ' ) entry = { } for fact in facts_found [ : - 1 ] . split ( \";\" ) : key , _ , value = fact . partition ( \"=\" ) entry  [ key . lower ( ) ] = value yield ( name , entry )"}
{"code_tokens": "def get_conn ( self ) : if self . conn is None : params = self . get_connection ( self . ftp_conn_id ) pasv = params . extra_dejson . get ( \"passive\" , True ) self . conn = ftplib . FTP ( params . host , params . login ,  params . password ) self . conn . set_pasv ( pasv ) return self . conn"}
{"code_tokens": "def list_directory ( self , path , nlst = False ) : conn = self . get_conn ( )  conn . cwd ( path ) files = conn . nlst ( ) return files"}
{"code_tokens": "def retrieve_file ( self , remote_full_path , local_full_path_or_buffer , callback = None ) : conn = self . get_conn ( ) is_path = isinstance ( local_full_path_or_buffer , basestring ) if not callback : if is_path : output_handle = open ( local_full_path_or_buffer , 'wb' ) else : output_handle = local_full_path_or_buffer callback = output_handle . write else : output_handle = None remote_path , remote_file_name = os . path . split ( remote_full_path ) conn . cwd ( remote_path ) self . log . info ( 'Retrieving file from FTP: %s' , remote_full_path ) conn . retrbinary ( 'RETR %s' % remote_file_name , callback ) self . log . info ( 'Finished retrieving file from FTP: %s' , remote_full_path ) if  is_path and output_handle : output_handle . close ( )"}
{"code_tokens": "def store_file ( self , remote_full_path , local_full_path_or_buffer ) : conn = self . get_conn ( ) is_path = isinstance ( local_full_path_or_buffer , basestring ) if is_path : input_handle = open ( local_full_path_or_buffer , 'rb' ) else : input_handle = local_full_path_or_buffer remote_path , remote_file_name = os . path . split (  remote_full_path ) conn . cwd ( remote_path ) conn . storbinary ( 'STOR %s' % remote_file_name , input_handle ) if is_path : input_handle . close ( )"}
{"code_tokens": "def get_mod_time ( self , path ) : conn = self . get_conn ( ) ftp_mdtm = conn . sendcmd ( 'MDTM ' + path ) time_val = ftp_mdtm [ 4 : ] try : return datetime . datetime . strptime ( time_val , \"%Y%m%d%H%M%S.%f\" ) except ValueError : return datetime . datetime .  strptime ( time_val , '%Y%m%d%H%M%S' )"}
{"code_tokens": "def execute ( self , context ) : self . hook = DiscordWebhookHook ( self . http_conn_id , self . webhook_endpoint , self . message , self . username ,  self . avatar_url , self . tts , self . proxy ) self . hook . execute ( )"}
{"code_tokens": "def get_conn ( self ) : conn = self . get_connection ( self . conn_id ) service_options = conn . extra_dejson return FileService  ( account_name = conn . login , account_key = conn . password , ** service_options )"}
{"code_tokens": "def check_for_directory ( self , share_name , directory_name , ** kwargs ) : return self . connection . exists ( share_name , directory_name , **  kwargs )"}
{"code_tokens": "def check_for_file ( self , share_name , directory_name , file_name , ** kwargs ) : return self . connection . exists ( share_name , directory_name , file_name ,  ** kwargs )"}
{"code_tokens": "def list_directories_and_files ( self , share_name , directory_name = None , ** kwargs )  : return self . connection . list_directories_and_files ( share_name , directory_name , ** kwargs )"}
{"code_tokens": "def create_directory ( self , share_name , directory_name , ** kwargs ) : return self . connection . create_directory (  share_name , directory_name , ** kwargs )"}
{"code_tokens": "def load_file ( self , file_path , share_name , directory_name , file_name , ** kwargs ) : self . connection . create_file_from_path ( share_name , directory_name , file_name , file_path ,  ** kwargs )"}
{"code_tokens": "def load_string ( self , string_data , share_name , directory_name , file_name , ** kwargs ) : self . connection . create_file_from_text ( share_name , directory_name , file_name , string_data  , ** kwargs )"}
{"code_tokens": "def load_stream ( self , stream , share_name , directory_name , file_name , count , ** kwargs ) : self . connection . create_file_from_stream  ( share_name , directory_name , file_name , stream , count , ** kwargs )"}
{"code_tokens": "def get_conn ( self ) : if not self . _conn : self . _conn = storage . Client ( credentials = self  . _get_credentials ( ) ) return self . _conn"}
{"code_tokens": "def copy ( self , source_bucket , source_object , destination_bucket = None , destination_object = None ) : destination_bucket = destination_bucket or source_bucket destination_object = destination_object or source_object if source_bucket == destination_bucket and source_object == destination_object : raise ValueError ( 'Either source/destination bucket or source/destination object ' 'must be different, not both the same: bucket=%s, object=%s' % ( source_bucket , source_object ) ) if not source_bucket or not source_object : raise ValueError ( 'source_bucket and source_object cannot be empty.' ) client = self . get_conn ( ) source_bucket = client . get_bucket ( source_bucket ) source_object = source_bucket . blob ( source_object ) destination_bucket = client . get_bucket ( destination_bucket ) destination_object = source_bucket . copy_blob ( blob = source_object , destination_bucket = destination_bucket , new_name = destination_object ) self . log . info ( 'Object %s in  bucket %s copied to object %s in bucket %s' , source_object . name , source_bucket . name , destination_object . name , destination_bucket . name )"}
{"code_tokens": "def download ( self , bucket_name , object_name , filename = None ) : client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name ) blob = bucket . blob ( blob_name = object_name ) if filename : blob . download_to_filename ( filename ) self .  log . info ( 'File downloaded to %s' , filename ) return blob . download_as_string ( )"}
{"code_tokens": "def upload ( self , bucket_name , object_name , filename , mime_type = 'application/octet-stream' , gzip = False ) : if gzip : filename_gz = filename + '.gz' with open ( filename , 'rb' ) as f_in : with gz . open ( filename_gz , 'wb' ) as f_out : shutil . copyfileobj ( f_in , f_out ) filename = filename_gz client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . blob ( blob_name = object_name ) blob . upload_from_filename ( filename = filename , content_type =  mime_type ) if gzip : os . remove ( filename ) self . log . info ( 'File %s uploaded to %s in %s bucket' , filename , object_name , bucket_name )"}
{"code_tokens": "def exists ( self , bucket_name , object_name ) : client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . blob ( blob_name  = object_name ) return blob . exists ( )"}
{"code_tokens": "def is_updated_after ( self , bucket_name , object_name , ts ) : client = self . get_conn ( ) bucket = storage . Bucket ( client = client , name = bucket_name ) blob = bucket . get_blob ( blob_name = object_name ) blob . reload ( ) blob_update_time = blob . updated if blob_update_time is not None : import dateutil . tz if not ts  . tzinfo : ts = ts . replace ( tzinfo = dateutil . tz . tzutc ( ) ) self . log . info ( \"Verify object date: %s > %s\" , blob_update_time , ts ) if blob_update_time > ts : return True return False"}
{"code_tokens": "def delete ( self , bucket_name , object_name ) : client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . blob ( blob_name = object_name ) blob . delete ( ) self . log . info ( 'Blob %s deleted.' ,  object_name )"}
{"code_tokens": "def list ( self , bucket_name , versions = None , max_results = None , prefix = None , delimiter = None ) : client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) ids = [ ] pageToken = None while True : blobs = bucket . list_blobs ( max_results = max_results , page_token = pageToken , prefix = prefix , delimiter = delimiter , versions = versions ) blob_names = [ ] for blob in blobs : blob_names . append ( blob . name ) prefixes = blobs . prefixes if prefixes : ids += list ( prefixes ) else : ids += blob_names pageToken = blobs . next_page_token if pageToken  is None : break return ids"}
{"code_tokens": "def get_size ( self , bucket_name , object_name ) : self . log . info ( 'Checking the file size of object: %s in bucket_name: %s' , object_name , bucket_name ) client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name =  bucket_name ) blob = bucket . get_blob ( blob_name = object_name ) blob . reload ( ) blob_size = blob . size self . log . info ( 'The file size of %s is %s bytes.' , object_name , blob_size ) return blob_size"}
{"code_tokens": "def get_crc32c ( self , bucket_name , object_name ) : self . log . info ( 'Retrieving the crc32c checksum of ' 'object_name: %s in bucket_name: %s' , object_name , bucket_name ) client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . get_blob  ( blob_name = object_name ) blob . reload ( ) blob_crc32c = blob . crc32c self . log . info ( 'The crc32c checksum of %s is %s' , object_name , blob_crc32c ) return blob_crc32c"}
{"code_tokens": "def get_md5hash ( self , bucket_name , object_name ) : self . log . info ( 'Retrieving the MD5 hash of ' 'object: %s in bucket: %s' , object_name , bucket_name ) client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . get_blob ( blob_name = object_name  ) blob . reload ( ) blob_md5hash = blob . md5_hash self . log . info ( 'The md5Hash of %s is %s' , object_name , blob_md5hash ) return blob_md5hash"}
{"code_tokens": "def create_bucket ( self , bucket_name , resource = None , storage_class = 'MULTI_REGIONAL' , location = 'US' , project_id = None , labels = None ) : self . log . info ( 'Creating Bucket: %s; Location: %s; Storage Class: %s' , bucket_name , location , storage_class ) client = self . get_conn ( ) bucket = client . bucket ( bucket_name = bucket_name ) bucket_resource = resource or { } for item in bucket_resource : if item != \"name\" : bucket . _patch_property ( name = item , value = resource [ item ] ) bucket . storage_class = storage_class bucket . labels = labels or { } bucket . create ( project = project_id , location = location  ) return bucket . id"}
{"code_tokens": "def compose ( self , bucket_name , source_objects , destination_object ) : if not source_objects or not len ( source_objects ) : raise ValueError ( 'source_objects cannot be empty.' ) if not bucket_name or not destination_object : raise ValueError ( 'bucket_name and destination_object cannot be empty.' ) self . log . info ( \"Composing %s to %s in the bucket %s\" , source_objects , destination_object , bucket_name  ) client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name ) destination_blob = bucket . blob ( destination_object ) destination_blob . compose ( sources = [ bucket . blob ( blob_name = source_object ) for source_object in source_objects ] ) self . log . info ( \"Completed successfully.\" )"}
{"code_tokens": "def secondary_training_status_changed ( current_job_description , prev_job_description ) : current_secondary_status_transitions = current_job_description . get ( 'SecondaryStatusTransitions' ) if current_secondary_status_transitions is None or len ( current_secondary_status_transitions ) == 0 : return False prev_job_secondary_status_transitions = prev_job_description . get ( 'SecondaryStatusTransitions' ) if prev_job_description is not None else None last_message  = prev_job_secondary_status_transitions [ - 1 ] [ 'StatusMessage' ] if prev_job_secondary_status_transitions is not None and len ( prev_job_secondary_status_transitions ) > 0 else '' message = current_job_description [ 'SecondaryStatusTransitions' ] [ - 1 ] [ 'StatusMessage' ] return message != last_message"}
{"code_tokens": "def secondary_training_status_message ( job_description , prev_description ) : if job_description is None or job_description . get ( 'SecondaryStatusTransitions' ) is None or len ( job_description . get ( 'SecondaryStatusTransitions' ) ) == 0 : return '' prev_description_secondary_transitions = prev_description . get ( 'SecondaryStatusTransitions' ) if prev_description is not None else None prev_transitions_num = len ( prev_description [ 'SecondaryStatusTransitions' ] ) if prev_description_secondary_transitions is not None else 0 current_transitions = job_description [ 'SecondaryStatusTransitions' ] transitions_to_print = current_transitions [ - 1 : ] if len ( current_transitions ) == prev_transitions_num else current_transitions [ prev_transitions_num - len ( current_transitions ) : ] status_strs = [ ] for transition in transitions_to_print : message = transition [ 'StatusMessage' ] time_str = timezone . convert_to_utc  ( job_description [ 'LastModifiedTime' ] ) . strftime ( '%Y-%m-%d %H:%M:%S' ) status_strs . append ( '{} {} - {}' . format ( time_str , transition [ 'Status' ] , message ) ) return '\\n' . join ( status_strs )"}
{"code_tokens": "def tar_and_s3_upload ( self , path , key , bucket ) : with tempfile . TemporaryFile ( ) as temp_file : if os . path . isdir ( path ) : files = [ os . path . join ( path , name ) for name in os . listdir ( path ) ] else : files = [ path ] with tarfile . open ( mode = 'w:gz' , fileobj = temp_file ) as tar_file : for f in files : tar_file . add ( f , arcname = os . path . basename ( f ) )  temp_file . seek ( 0 ) self . s3_hook . load_file_obj ( temp_file , key , bucket , replace = True )"}
{"code_tokens": "def configure_s3_resources ( self , config ) : s3_operations = config . pop ( 'S3Operations' , None ) if s3_operations is not None : create_bucket_ops = s3_operations . get ( 'S3CreateBucket' , [ ] ) upload_ops = s3_operations . get ( 'S3Upload' , [ ] ) for op in create_bucket_ops : self . s3_hook . create_bucket ( bucket_name = op [ 'Bucket' ] ) for op in upload_ops : if op [ 'Tar' ] : self . tar_and_s3_upload ( op [ 'Path' ] , op [ 'Key' ] , op [ 'Bucket' ] ) else  : self . s3_hook . load_file ( op [ 'Path' ] , op [ 'Key' ] , op [ 'Bucket' ] )"}
{"code_tokens": "def check_s3_url ( self , s3url ) : bucket , key = S3Hook . parse_s3_url ( s3url ) if not self . s3_hook . check_for_bucket ( bucket_name = bucket ) : raise AirflowException ( \"The input S3 Bucket {} does not exist \" . format ( bucket ) ) if key and not self . s3_hook . check_for_key (  key = key , bucket_name = bucket ) and not self . s3_hook . check_for_prefix ( prefix = key , bucket_name = bucket , delimiter = '/' ) : raise AirflowException ( \"The input S3 Key \" \"or Prefix {} does not exist in the Bucket {}\" . format ( s3url , bucket ) ) return True"}
{"code_tokens": "def get_log_conn ( self ) : config = botocore . config . Config ( retries = { 'max_attempts' : 15 } ) return self . get_client_type ( 'logs'  , config = config )"}
{"code_tokens": "def create_training_job ( self , config , wait_for_completion = True , print_log = True , check_interval = 30 , max_ingestion_time = None ) : self . check_training_config ( config ) response = self . get_conn ( ) . create_training_job ( ** config ) if print_log : self . check_training_status_with_log ( config [ 'TrainingJobName' ] , self . non_terminal_states , self . failed_states , wait_for_completion , check_interval , max_ingestion_time ) elif wait_for_completion : describe_response = self . check_status ( config [ 'TrainingJobName' ] ,  'TrainingJobStatus' , self . describe_training_job , check_interval , max_ingestion_time ) billable_time = ( describe_response [ 'TrainingEndTime' ] - describe_response [ 'TrainingStartTime' ] ) * describe_response [ 'ResourceConfig' ] [ 'InstanceCount' ] self . log . info ( 'Billable seconds:{}' . format ( int ( billable_time . total_seconds ( ) ) + 1 ) ) return response"}
{"code_tokens": "def create_tuning_job ( self , config , wait_for_completion = True , check_interval = 30 , max_ingestion_time = None ) : self . check_tuning_config ( config ) response = self . get_conn ( )  . create_hyper_parameter_tuning_job ( ** config ) if wait_for_completion : self . check_status ( config [ 'HyperParameterTuningJobName' ] , 'HyperParameterTuningJobStatus' , self . describe_tuning_job , check_interval , max_ingestion_time ) return response"}
{"code_tokens": "def create_transform_job ( self , config , wait_for_completion = True , check_interval = 30 , max_ingestion_time = None ) : self . check_s3_url ( config [ 'TransformInput' ] [ 'DataSource' ] [ 'S3DataSource' ] [ 'S3Uri' ] ) response =  self . get_conn ( ) . create_transform_job ( ** config ) if wait_for_completion : self . check_status ( config [ 'TransformJobName' ] , 'TransformJobStatus' , self . describe_transform_job , check_interval , max_ingestion_time ) return response"}
{"code_tokens": "def create_endpoint ( self , config , wait_for_completion = True , check_interval = 30 , max_ingestion_time = None ) : response = self . get_conn ( ) . create_endpoint ( ** config ) if wait_for_completion : self . check_status ( config [ 'EndpointName' ] , 'EndpointStatus' , self . describe_endpoint , check_interval  , max_ingestion_time , non_terminal_states = self . endpoint_non_terminal_states ) return response"}
{"code_tokens": "def describe_training_job_with_log ( self , job_name , positions , stream_names , instance_count , state , last_description , last_describe_job_call ) : log_group = '/aws/sagemaker/TrainingJobs' if len ( stream_names ) < instance_count : logs_conn = self . get_log_conn ( ) try : streams = logs_conn . describe_log_streams ( logGroupName = log_group , logStreamNamePrefix = job_name + '/' , orderBy = 'LogStreamName' , limit = instance_count ) stream_names = [ s [ 'logStreamName' ] for s in streams [ 'logStreams' ] ] positions . update ( [ ( s , Position ( timestamp = 0 , skip = 0 ) ) for s in stream_names if s not in positions ] ) except logs_conn . exceptions . ResourceNotFoundException : pass if len ( stream_names ) > 0 : for idx , event in self . multi_stream_iter ( log_group , stream_names , positions ) : self . log . info ( event [ 'message' ] ) ts , count = positions [ stream_names [ idx ] ] if event [ 'timestamp' ] == ts : positions [ stream_names [ idx ] ] = Position ( timestamp = ts , skip = count + 1 ) else : positions [ stream_names [ idx ] ] = Position ( timestamp = event [ 'timestamp' ] , skip = 1 ) if state == LogState . COMPLETE : return state , last_description , last_describe_job_call if state == LogState . JOB_COMPLETE : state = LogState . COMPLETE elif time . time ( ) - last_describe_job_call >= 30 : description = self . describe_training_job ( job_name ) last_describe_job_call = time . time ( ) if secondary_training_status_changed ( description , last_description ) : self . log . info ( secondary_training_status_message ( description , last_description ) ) last_description = description status =  description [ 'TrainingJobStatus' ] if status not in self . non_terminal_states : state = LogState . JOB_COMPLETE return state , last_description , last_describe_job_call"}
{"code_tokens": "def check_status ( self , job_name , key , describe_function , check_interval , max_ingestion_time , non_terminal_states = None ) : if not non_terminal_states : non_terminal_states = self . non_terminal_states sec = 0 running = True while running : time . sleep ( check_interval ) sec = sec + check_interval try : response = describe_function ( job_name ) status = response [ key ] self . log . info ( 'Job still running for %s seconds... ' 'current status is %s' % ( sec , status ) ) except KeyError : raise AirflowException ( 'Could not get status of the SageMaker job' ) except ClientError : raise AirflowException ( 'AWS request failed, check logs for more info' ) if status in non_terminal_states : running = True elif status in self . failed_states : raise AirflowException ( 'SageMaker job failed because %s' % response [ 'FailureReason' ] ) else : running = False if max_ingestion_time and sec >  max_ingestion_time : raise AirflowException ( 'SageMaker job took more than %s seconds' , max_ingestion_time ) self . log . info ( 'SageMaker Job Compeleted' ) response = describe_function ( job_name ) return response"}
{"code_tokens": "def check_training_status_with_log ( self , job_name , non_terminal_states , failed_states , wait_for_completion , check_interval , max_ingestion_time ) : sec = 0 description = self . describe_training_job ( job_name ) self . log . info ( secondary_training_status_message ( description , None ) ) instance_count = description [ 'ResourceConfig' ] [ 'InstanceCount' ] status = description [ 'TrainingJobStatus' ] stream_names = [ ] positions = { } job_already_completed = status not in non_terminal_states state = LogState . TAILING if wait_for_completion and not job_already_completed else LogState . COMPLETE last_describe_job_call = time . time ( ) last_description = description while True : time . sleep ( check_interval ) sec = sec + check_interval state , last_description , last_describe_job_call = self . describe_training_job_with_log ( job_name , positions , stream_names , instance_count , state , last_description , last_describe_job_call ) if state == LogState . COMPLETE : break if max_ingestion_time and sec > max_ingestion_time : raise AirflowException ( 'SageMaker job took more  than %s seconds' , max_ingestion_time ) if wait_for_completion : status = last_description [ 'TrainingJobStatus' ] if status in failed_states : reason = last_description . get ( 'FailureReason' , '(No reason provided)' ) raise AirflowException ( 'Error training {}: {} Reason: {}' . format ( job_name , status , reason ) ) billable_time = ( last_description [ 'TrainingEndTime' ] - last_description [ 'TrainingStartTime' ] ) * instance_count self . log . info ( 'Billable seconds:{}' . format ( int ( billable_time . total_seconds ( ) ) + 1 ) )"}
{"code_tokens": "def execute ( self , context ) : bucket_helper = GoogleCloudBucketHelper ( self . gcp_conn_id , self . delegate_to ) self . py_file = bucket_helper . google_cloud_to_local ( self . py_file ) hook = DataFlowHook ( gcp_conn_id = self . gcp_conn_id , delegate_to = self . delegate_to , poll_sleep = self . poll_sleep ) dataflow_options = self . dataflow_default_options . copy ( ) dataflow_options . update ( self . options ) camel_to_snake = lambda name : re . sub  ( r'[A-Z]' , lambda x : '_' + x . group ( 0 ) . lower ( ) , name ) formatted_options = { camel_to_snake ( key ) : dataflow_options [ key ] for key in dataflow_options } hook . start_python_dataflow ( self . job_name , formatted_options , self . py_file , self . py_options )"}
{"code_tokens": "def run_migrations_offline ( ) : context . configure ( url = settings . SQL_ALCHEMY_CONN , target_metadata = target_metadata , literal_binds = True , compare_type = COMPARE_TYPE )  with context . begin_transaction ( ) : context . run_migrations ( )"}
{"code_tokens": "def run_migrations_online ( ) : connectable = settings . engine with connectable . connect ( ) as connection : context . configure ( connection = connection , transaction_per_migration = True , target_metadata = target_metadata , compare_type = COMPARE_TYPE , ) with context . begin_transaction  ( ) : context . run_migrations ( )"}
{"code_tokens": "def delete_instance ( self , instance_id , project_id = None ) : instance = self . get_instance ( instance_id = instance_id , project_id = project_id ) if instance : instance . delete ( ) else : self . log . info ( \"The instance '%s' does not exist  in project '%s'. Exiting\" , instance_id , project_id )"}
{"code_tokens": "def create_instance ( self , instance_id , main_cluster_id , main_cluster_zone , project_id = None , replica_cluster_id = None , replica_cluster_zone = None , instance_display_name = None , instance_type = enums . Instance . Type . TYPE_UNSPECIFIED , instance_labels = None , cluster_nodes = None , cluster_storage_type = enums . StorageType . STORAGE_TYPE_UNSPECIFIED , timeout = None ) : cluster_storage_type = enums . StorageType ( cluster_storage_type ) instance_type = enums . Instance . Type ( instance_type ) instance = Instance ( instance_id , self . _get_client ( project_id = project_id ) , instance_display_name , instance_type , instance_labels , ) clusters = [ instance . cluster ( main_cluster_id , main_cluster_zone , cluster_nodes , cluster_storage_type ) ] if replica_cluster_id and replica_cluster_zone : clusters .  append ( instance . cluster ( replica_cluster_id , replica_cluster_zone , cluster_nodes , cluster_storage_type ) ) operation = instance . create ( clusters = clusters ) operation . result ( timeout ) return instance"}
{"code_tokens": "def create_table ( instance , table_id , initial_split_keys = None , column_families = None ) : if column_families is None : column_families = { } if initial_split_keys is None : initial_split_keys = [ ] table = Table ( table_id , instance ) table . create (  initial_split_keys , column_families )"}
{"code_tokens": "def delete_table ( self , instance_id , table_id , project_id = None ) : table = self . get_instance ( instance_id = instance_id , project_id = project_id ) . table ( table_id = table_id ) table . delete (  )"}
{"code_tokens": "def update_cluster ( instance , cluster_id , nodes ) : cluster = Cluster ( cluster_id , instance ) cluster . serve_nodes = nodes cluster  . update ( )"}
{"code_tokens": "def _prepare_cli_cmd ( self ) : conn = self . conn hive_bin = 'hive' cmd_extra = [ ] if self . use_beeline : hive_bin = 'beeline' jdbc_url = \"jdbc:hive2://{host}:{port}/{schema}\" . format ( host = conn . host , port = conn . port , schema = conn . schema ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : template = conn . extra_dejson . get ( 'principal' , \"hive/_HOST@EXAMPLE.COM\" ) if \"_HOST\" in template : template = utils . replace_hostname_pattern ( utils . get_components ( template ) ) proxy_user = \"\" if conn . extra_dejson . get ( 'proxy_user' ) == \"login\" and conn . login : proxy_user = \"hive.server2.proxy.user={0}\" . format ( conn . login ) elif conn  . extra_dejson . get ( 'proxy_user' ) == \"owner\" and self . run_as : proxy_user = \"hive.server2.proxy.user={0}\" . format ( self . run_as ) jdbc_url += \";principal={template};{proxy_user}\" . format ( template = template , proxy_user = proxy_user ) elif self . auth : jdbc_url += \";auth=\" + self . auth jdbc_url = '\"{}\"' . format ( jdbc_url ) cmd_extra += [ '-u' , jdbc_url ] if conn . login : cmd_extra += [ '-n' , conn . login ] if conn . password : cmd_extra += [ '-p' , conn . password ] hive_params_list = self . hive_cli_params . split ( ) return [ hive_bin ] + cmd_extra + hive_params_list"}
{"code_tokens": "def _prepare_hiveconf ( d ) : if not d : return [ ] return as_flattened_list ( zip ( [ \"-hiveconf\" ] * len ( d ) , [ \"{}={}\" . format ( k , v ) for k , v in  d . items ( ) ] ) )"}
{"code_tokens": "def load_df ( self , df , table , field_dict = None , delimiter = ',' , encoding = 'utf8' , pandas_kwargs = None , ** kwargs ) : def _infer_field_types_from_df ( df ) : DTYPE_KIND_HIVE_TYPE = { 'b' : 'BOOLEAN' , 'i' : 'BIGINT' , 'u' : 'BIGINT' , 'f' : 'DOUBLE' , 'c' : 'STRING' , 'M' : 'TIMESTAMP' , 'O' : 'STRING' , 'S' : 'STRING' , 'U' : 'STRING' , 'V' : 'STRING' } d = OrderedDict ( ) for col , dtype in df . dtypes . iteritems ( ) : d [ col ] = DTYPE_KIND_HIVE_TYPE [ dtype . kind ] return d if pandas_kwargs is None : pandas_kwargs = { } with TemporaryDirectory ( prefix = 'airflow_hiveop_' ) as tmp_dir : with NamedTemporaryFile ( dir = tmp_dir , mode = \"w\" ) as f : if field_dict is None : field_dict = _infer_field_types_from_df ( df ) df . to_csv ( path_or_buf = f , sep = delimiter , header = False , index = False , encoding = encoding , date_format = \"%Y-%m-%d %H:%M:%S\" , ** pandas_kwargs  ) f . flush ( ) return self . load_file ( filepath = f . name , table = table , delimiter = delimiter , field_dict = field_dict , ** kwargs )"}
{"code_tokens": "def load_file ( self , filepath , table , delimiter = \",\" , field_dict = None , create = True , overwrite = True , partition = None , recreate = False , tblproperties = None ) : hql = '' if recreate : hql += \"DROP TABLE IF EXISTS {table};\\n\" . format ( table = table ) if create or recreate : if field_dict is None : raise ValueError ( \"Must provide a field dict when creating a table\" ) fields = \",\\n \" . join ( [ k + ' ' + v for k , v in field_dict . items ( ) ] ) hql += \"CREATE TABLE IF NOT EXISTS {table} (\\n{fields})\\n\" . format ( table = table , fields = fields ) if partition : pfields = \",\\n \" . join ( [ p + \" STRING\" for p in partition ] ) hql += \"PARTITIONED BY ({pfields})\\n\" . format ( pfields = pfields ) hql += \"ROW FORMAT DELIMITED\\n\" hql += \"FIELDS TERMINATED BY '{delimiter}'\\n\" . format ( delimiter = delimiter ) hql += \"STORED AS textfile\\n\" if tblproperties is not None : tprops = \", \" . join ( [ \"'{0}'='{1}'\" . format ( k , v ) for k , v in tblproperties . items ( ) ] )  hql += \"TBLPROPERTIES({tprops})\\n\" . format ( tprops = tprops ) hql += \";\" self . log . info ( hql ) self . run_cli ( hql ) hql = \"LOAD DATA LOCAL INPATH '{filepath}' \" . format ( filepath = filepath ) if overwrite : hql += \"OVERWRITE \" hql += \"INTO TABLE {table} \" . format ( table = table ) if partition : pvals = \", \" . join ( [ \"{0}='{1}'\" . format ( k , v ) for k , v in partition . items ( ) ] ) hql += \"PARTITION ({pvals})\" . format ( pvals = pvals ) hql += ';\\n' self . log . info ( hql ) self . run_cli ( hql )"}
{"code_tokens": "def get_metastore_client ( self ) : import hmsclient from thrift . transport import TSocket , TTransport from thrift . protocol import TBinaryProtocol ms = self . metastore_conn auth_mechanism = ms . extra_dejson . get ( 'authMechanism' , 'NOSASL' ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : auth_mechanism = ms . extra_dejson . get ( 'authMechanism' , 'GSSAPI' ) kerberos_service_name = ms . extra_dejson . get ( 'kerberos_service_name' , 'hive' ) socket = TSocket . TSocket ( ms . host , ms . port ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' and auth_mechanism == 'GSSAPI' : try : import saslwrapper as sasl except ImportError : import sasl def sasl_factory ( ) : sasl_client = sasl . Client ( ) sasl_client . setAttr ( \"host\" , ms . host ) sasl_client . setAttr ( \"service\" , kerberos_service_name ) sasl_client . init ( ) return sasl_client from thrift_sasl import TSaslClientTransport transport = TSaslClientTransport ( sasl_factory , \"GSSAPI\" , socket ) else : transport = TTransport . TBufferedTransport ( socket ) protocol = TBinaryProtocol . TBinaryProtocol ( transport ) return hmsclient . HMSClient (  iprot = protocol )"}
{"code_tokens": "def check_for_named_partition ( self , schema , table , partition_name ) : with self . metastore as client : return client . check_for_named_partition (  schema , table , partition_name )"}
{"code_tokens": "def table_exists ( self , table_name , db = 'default' ) : try : self . get_table ( table_name  , db ) return True except Exception : return False"}
{"code_tokens": "def get_conn ( self , schema = None ) : db = self . get_connection ( self . hiveserver2_conn_id ) auth_mechanism = db . extra_dejson . get ( 'authMechanism' , 'NONE' ) if auth_mechanism == 'NONE' and db . login is None : username = 'airflow' kerberos_service_name = None if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : auth_mechanism = db . extra_dejson . get ( 'authMechanism' , 'KERBEROS' ) kerberos_service_name = db . extra_dejson . get ( 'kerberos_service_name' , 'hive' ) if auth_mechanism == 'GSSAPI' : self . log . warning ( \"Detected deprecated 'GSSAPI' for authMechanism \" \"for %s. Please use 'KERBEROS' instead\" , self . hiveserver2_conn_id ) auth_mechanism = 'KERBEROS' from pyhive . hive import connect return connect ( host = db . host , port = db . port , auth = auth_mechanism , kerberos_service_name = kerberos_service_name , username = db . login or username , password  = db . password , database = schema or db . schema or 'default' )"}
{"code_tokens": "def get_results ( self , hql , schema = 'default' , fetch_size = None , hive_conf = None ) : results_iter = self . _get_results ( hql , schema , fetch_size  = fetch_size , hive_conf = hive_conf ) header = next ( results_iter ) results = { 'data' : list ( results_iter ) , 'header' : header } return results"}
{"code_tokens": "def to_csv ( self , hql , csv_filepath , schema = 'default' , delimiter = ',' , lineterminator = '\\r\\n' , output_header = True , fetch_size = 1000 , hive_conf = None ) : results_iter = self . _get_results ( hql , schema , fetch_size = fetch_size , hive_conf = hive_conf ) header = next ( results_iter ) message = None i = 0 with open ( csv_filepath , 'wb' ) as f : writer = csv . writer ( f , delimiter = delimiter , lineterminator = lineterminator , encoding = 'utf-8' ) try : if output_header : self . log . debug ( 'Cursor description is %s' , header ) writer . writerow ( [ c [ 0 ] for c in header ] ) for i , row in enumerate ( results_iter , 1 ) : writer . writerow ( row ) if i % fetch_size == 0 : self . log . info ( \"Written %s  rows so far.\" , i ) except ValueError as exception : message = str ( exception ) if message : os . remove ( csv_filepath ) raise ValueError ( message ) self . log . info ( \"Done. Loaded a total of %s rows.\" , i )"}
{"code_tokens": "def get_records ( self , hql , schema = 'default' , hive_conf = None ) : return self . get_results ( hql , schema = schema , hive_conf = hive_conf ) [ 'data' ] "}
{"code_tokens": "def get_pandas_df ( self , hql , schema = 'default' ) : import pandas as pd res = self . get_results ( hql , schema = schema ) df = pd . DataFrame ( res  [ 'data' ] ) df . columns = [ c [ 0 ] for c in res [ 'header' ] ] return df"}
{"code_tokens": "def get_conn ( self ) : if not self . _client : self . _client = ProductSearchClient ( credentials = self . _get_credentials ( )  ) return self . _client"}
{"code_tokens": "def _get_endpoint ( self ) : conn = self . get_connection ( self . http_conn_id ) token = conn . password if not token :  raise AirflowException ( 'Dingding token is requests but get nothing, ' 'check you conn_id configuration.' ) return 'robot/send?access_token={}' . format ( token )"}
{"code_tokens": "def send ( self ) : support_type = [ 'text' , 'link' , 'markdown' , 'actionCard' , 'feedCard' ] if self . message_type not in support_type : raise ValueError ( 'DingdingWebhookHook only support {} ' 'so far, but receive {}' . format ( support_type , self . message_type ) ) data = self . _build_message ( ) self . log . info ( 'Sending Dingding type %s message %s' , self . message_type ,  data ) resp = self . run ( endpoint = self . _get_endpoint ( ) , data = data , headers = { 'Content-Type' : 'application/json' } ) if int ( resp . json ( ) . get ( 'errcode' ) ) != 0 : raise AirflowException ( 'Send Dingding message failed, receive error ' 'message %s' , resp . text ) self . log . info ( 'Success Send Dingding message' )"}
{"code_tokens": "def _bind_parameters ( operation , parameters ) : string_parameters = { } for ( name , value ) in iteritems ( parameters ) : if value is None : string_parameters [ name ] = 'NULL' elif isinstance ( value , basestring ) : string_parameters [ name ] = \"'\" + _escape ( value ) + \"'\" else : string_parameters [ name ] =  str ( value ) return operation % string_parameters"}
{"code_tokens": "def _escape ( s ) : e = s e = e . replace ( '\\\\' , '\\\\\\\\' ) e = e . replace ( '\\n' , '\\\\n' ) e = e . replace ( '\\r' , '\\\\r' ) e =  e . replace ( \"'\" , \"\\\\'\" ) e = e . replace ( '\"' , '\\\\\"' ) return e"}
{"code_tokens": "def _bq_cast ( string_field , bq_type ) : if string_field is None : return None elif bq_type == 'INTEGER' : return int ( string_field ) elif bq_type == 'FLOAT' or bq_type == 'TIMESTAMP' : return float ( string_field ) elif bq_type == 'BOOLEAN' : if string_field not in [ 'true' , 'false' ] : raise ValueError ( \"{} must have value 'true' or 'false'\" . format ( string_field  ) ) return string_field == 'true' else : return string_field"}
{"code_tokens": "def _validate_value ( key , value , expected_type ) : if not isinstance ( value , expected_type ) : raise TypeError ( \"{} argument must have a  type {} not {}\" . format ( key , expected_type , type ( value ) ) )"}
{"code_tokens": "def get_conn ( self ) : service = self . get_service ( ) project = self . _get_field ( 'project' ) return BigQueryConnection ( service = service , project_id = project , use_legacy_sql = self . use_legacy_sql , location  = self . location , num_retries = self . num_retries )"}
{"code_tokens": "def get_service ( self ) : http_authorized = self . _authorize ( ) return  build ( 'bigquery' , 'v2' , http = http_authorized , cache_discovery = False )"}
{"code_tokens": "def table_exists ( self , project_id , dataset_id , table_id ) : service = self . get_service ( ) try : service . tables ( ) . get ( projectId = project_id , datasetId = dataset_id , tableId = table_id ) . execute ( num_retries = self . num_retries ) return True except HttpError as e : if e . resp [ 'status' ]  == '404' : return False raise"}
{"code_tokens": "def create_empty_table ( self , project_id , dataset_id , table_id , schema_fields = None , time_partitioning = None , cluster_fields = None , labels = None , view = None , num_retries = None ) : project_id = project_id if project_id is not None else self . project_id table_resource = { 'tableReference' : { 'tableId' : table_id } } if schema_fields : table_resource [ 'schema' ] = { 'fields' : schema_fields } if time_partitioning : table_resource [ 'timePartitioning' ] = time_partitioning if cluster_fields : table_resource [ 'clustering' ] = { 'fields' : cluster_fields } if labels : table_resource [ 'labels' ] = labels if view : table_resource [ 'view' ] = view num_retries = num_retries if num_retries else self . num_retries self . log . info ( 'Creating Table %s:%s.%s' , project_id , dataset_id , table_id ) try : self . service . tables ( ) . insert ( projectId = project_id , datasetId = dataset_id , body = table_resource ) . execute ( num_retries = num_retries ) self . log . info ( 'Table created successfully: %s:%s.%s' , project_id , dataset_id , table_id ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err  . content ) )"}
{"code_tokens": "def patch_table ( self , dataset_id , table_id , project_id = None , description = None , expiration_time = None , external_data_configuration = None , friendly_name = None , labels = None , schema = None , time_partitioning = None , view = None , require_partition_filter = None ) : project_id = project_id if project_id is not None else self . project_id table_resource = { } if description is not None : table_resource [ 'description' ] = description if expiration_time is not None : table_resource [ 'expirationTime' ] = expiration_time if external_data_configuration : table_resource [ 'externalDataConfiguration' ] = external_data_configuration if friendly_name is not None : table_resource [ 'friendlyName' ] = friendly_name if labels : table_resource [ 'labels' ] = labels if schema : table_resource [ 'schema' ] = { 'fields' : schema } if time_partitioning : table_resource [ 'timePartitioning' ] = time_partitioning if view : table_resource [ 'view' ] = view if require_partition_filter is not None : table_resource [ 'requirePartitionFilter' ] = require_partition_filter self . log . info ( 'Patching Table %s:%s.%s' , project_id , dataset_id , table_id ) try : self . service . tables ( ) . patch ( projectId = project_id , datasetId = dataset_id , tableId = table_id , body = table_resource ) . execute ( num_retries = self  . num_retries ) self . log . info ( 'Table patched successfully: %s:%s.%s' , project_id , dataset_id , table_id ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) )"}
{"code_tokens": "def cancel_query ( self ) : jobs = self . service . jobs ( ) if ( self . running_job_id and not self . poll_job_complete ( self . running_job_id ) ) : self . log . info ( 'Attempting to cancel job : %s, %s' , self . project_id , self . running_job_id ) if self . location : jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id , location = self . location ) . execute ( num_retries = self . num_retries ) else : jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id ) . execute ( num_retries = self . num_retries ) else : self . log . info ( 'No running BigQuery jobs to cancel.' ) return max_polling_attempts = 12 polling_attempts = 0 job_complete = False while polling_attempts < max_polling_attempts and not job_complete : polling_attempts = polling_attempts + 1 job_complete = self . poll_job_complete ( self . running_job_id ) if  job_complete : self . log . info ( 'Job successfully canceled: %s, %s' , self . project_id , self . running_job_id ) elif polling_attempts == max_polling_attempts : self . log . info ( \"Stopping polling due to timeout. Job with id %s \" \"has not completed cancel and may or may not finish.\" , self . running_job_id ) else : self . log . info ( 'Waiting for canceled job with id %s to finish.' , self . running_job_id ) time . sleep ( 5 )"}
{"code_tokens": "def run_table_delete ( self , deletion_dataset_table , ignore_if_missing = False ) : deletion_project , deletion_dataset , deletion_table = _split_tablename ( table_input = deletion_dataset_table , default_project_id = self . project_id ) try : self . service . tables ( ) . delete ( projectId = deletion_project , datasetId = deletion_dataset , tableId = deletion_table ) . execute ( num_retries = self . num_retries ) self . log  . info ( 'Deleted table %s:%s.%s.' , deletion_project , deletion_dataset , deletion_table ) except HttpError : if not ignore_if_missing : raise Exception ( 'Table deletion failed. Table does not exist.' ) else : self . log . info ( 'Table does not exist. Skipping.' )"}
{"code_tokens": "def run_table_upsert ( self , dataset_id , table_resource , project_id = None ) : table_id = table_resource [ 'tableReference' ] [ 'tableId' ] project_id = project_id if project_id is not None else self . project_id tables_list_resp = self . service . tables ( ) . list ( projectId = project_id , datasetId = dataset_id ) . execute ( num_retries = self . num_retries ) while True : for table in tables_list_resp . get ( 'tables' , [ ] ) : if table [ 'tableReference' ] [ 'tableId' ] == table_id : self . log . info ( 'Table %s:%s.%s exists, updating.' , project_id , dataset_id , table_id ) return self . service . tables ( ) . update ( projectId = project_id , datasetId = dataset_id , tableId = table_id , body = table_resource ) . execute ( num_retries = self . num_retries ) if 'nextPageToken' in tables_list_resp : tables_list_resp = self . service . tables ( ) . list ( projectId = project_id , datasetId = dataset_id , pageToken = tables_list_resp [ 'nextPageToken' ] ) . execute ( num_retries = self . num_retries ) else : self . log . info ( 'Table %s:%s.%s does not exist. creating.' , project_id , dataset_id , table_id ) return self . service . tables ( ) . insert (  projectId = project_id , datasetId = dataset_id , body = table_resource ) . execute ( num_retries = self . num_retries )"}
{"code_tokens": "def run_grant_dataset_view_access ( self , source_dataset , view_dataset , view_table , source_project = None , view_project = None ) : source_project = source_project if source_project else self . project_id view_project = view_project if view_project else self . project_id source_dataset_resource = self . service . datasets ( ) . get ( projectId = source_project , datasetId = source_dataset ) . execute ( num_retries = self . num_retries ) access = source_dataset_resource [ 'access' ] if 'access' in source_dataset_resource else [ ] view_access = { 'view' : { 'projectId' : view_project , 'datasetId' : view_dataset , 'tableId' : view_table } } if view_access not in access : self . log . info ( 'Granting table %s:%s.%s authorized view access to %s:%s dataset.' , view_project , view_dataset , view_table , source_project , source_dataset ) access . append ( view_access ) return self . service . datasets ( ) . patch ( projectId = source_project , datasetId = source_dataset , body =  { 'access' : access } ) . execute ( num_retries = self . num_retries ) else : self . log . info ( 'Table %s:%s.%s already has authorized view access to %s:%s dataset.' , view_project , view_dataset , view_table , source_project , source_dataset ) return source_dataset_resource"}
{"code_tokens": "def get_dataset ( self , dataset_id , project_id = None ) : if not dataset_id or not isinstance ( dataset_id , str ) : raise ValueError ( \"dataset_id argument must be provided and has \" \"a type 'str'. You provided: {}\" . format ( dataset_id ) ) dataset_project_id = project_id if project_id else self . project_id try : dataset_resource = self . service . datasets ( ) . get ( datasetId  = dataset_id , projectId = dataset_project_id ) . execute ( num_retries = self . num_retries ) self . log . info ( \"Dataset Resource: %s\" , dataset_resource ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) ) return dataset_resource"}
{"code_tokens": "def get_datasets_list ( self , project_id = None ) : dataset_project_id = project_id if project_id else self . project_id try : datasets_list = self . service . datasets ( ) . list ( projectId = dataset_project_id ) . execute ( num_retries = self . num_retries ) [ 'datasets' ] self . log . info (  \"Datasets List: %s\" , datasets_list ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) ) return datasets_list"}
{"code_tokens": "def insert_all ( self , project_id , dataset_id , table_id , rows , ignore_unknown_values = False , skip_invalid_rows = False , fail_on_error = False ) : dataset_project_id = project_id if project_id else self . project_id body = { \"rows\" : rows , \"ignoreUnknownValues\" : ignore_unknown_values , \"kind\" : \"bigquery#tableDataInsertAllRequest\" , \"skipInvalidRows\" : skip_invalid_rows , } try : self . log . info ( 'Inserting %s row(s) into Table %s:%s.%s' , len ( rows ) , dataset_project_id , dataset_id , table_id ) resp = self . service . tabledata ( ) . insertAll ( projectId = dataset_project_id , datasetId = dataset_id , tableId = table_id , body = body ) . execute ( num_retries = self . num_retries ) if 'insertErrors' not in resp : self  . log . info ( 'All row(s) inserted successfully: %s:%s.%s' , dataset_project_id , dataset_id , table_id ) else : error_msg = '{} insert error(s) occurred: {}:{}.{}. Details: {}' . format ( len ( resp [ 'insertErrors' ] ) , dataset_project_id , dataset_id , table_id , resp [ 'insertErrors' ] ) if fail_on_error : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( error_msg ) ) self . log . info ( error_msg ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) )"}
{"code_tokens": "def execute ( self , operation , parameters = None ) : sql = _bind_parameters ( operation , parameters ) if parameters else operation self . job_id =  self . run_query ( sql )"}
{"code_tokens": "def executemany ( self , operation , seq_of_parameters ) : for parameters  in seq_of_parameters : self . execute ( operation , parameters )"}
{"code_tokens": "def next ( self ) : if not self . job_id : return None if len ( self . buffer ) == 0 : if self . all_pages_loaded : return None query_results = ( self . service . jobs ( ) . getQueryResults ( projectId = self . project_id , jobId = self . job_id , pageToken = self . page_token ) . execute ( num_retries = self . num_retries ) ) if 'rows' in query_results and query_results [ 'rows' ] : self . page_token = query_results . get ( 'pageToken' ) fields = query_results [ 'schema' ] [ 'fields' ] col_types = [ field [ 'type' ] for field in fields ] rows = query_results [ 'rows' ] for dict_row in rows : typed_row = ( [ _bq_cast ( vs [ 'v' ] , col_types [ idx ] ) for idx , vs in enumerate ( dict_row [ 'f' ] ) ] ) self . buffer . append ( typed_row ) if not self . page_token : self . all_pages_loaded = True else : self . page_token = None self . job_id = None self .  page_token = None return None return self . buffer . pop ( 0 )"}
{"code_tokens": "def _query_postgres ( self ) : postgres = PostgresHook ( postgres_conn_id = self . postgres_conn_id ) conn = postgres . get_conn ( ) cursor =  conn . cursor ( ) cursor . execute ( self . sql , self . parameters ) return cursor"}
{"code_tokens": "def _make_intermediate_dirs ( sftp_client , remote_directory ) : if remote_directory == '/' : sftp_client . chdir ( '/' ) return if remote_directory == '' : return try : sftp_client . chdir ( remote_directory ) except IOError  : dirname , basename = os . path . split ( remote_directory . rstrip ( '/' ) ) _make_intermediate_dirs ( sftp_client , dirname ) sftp_client . mkdir ( basename ) sftp_client . chdir ( basename ) return"}
{"code_tokens": "def create_queue ( self , queue_name , attributes = None ) : return self . get_conn ( ) . create_queue (  QueueName = queue_name , Attributes = attributes or { } )"}
{"code_tokens": "def send_message ( self , queue_url , message_body , delay_seconds = 0 , message_attributes = None ) : return self . get_conn ( ) . send_message ( QueueUrl = queue_url , MessageBody = message_body ,  DelaySeconds = delay_seconds , MessageAttributes = message_attributes or { } )"}
{"code_tokens": "def run_command ( self , run_with = None , join_args = False ) : run_with = run_with or [ ] cmd = [ \" \" . join ( self . _command ) ] if join_args else self . _command full_cmd = run_with + cmd self . log . info ( 'Running: %s' , full_cmd ) proc = subprocess . Popen ( full_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True  , close_fds = True , env = os . environ . copy ( ) , preexec_fn = os . setsid ) log_reader = threading . Thread ( target = self . _read_task_logs , args = ( proc . stdout , ) , ) log_reader . daemon = True log_reader . start ( ) return proc"}
{"code_tokens": "def on_finish ( self ) : if self . _cfg_path and os . path . isfile ( self . _cfg_path ) : if self . run_as_user : subprocess . call ( [ 'sudo' , 'rm' , self . _cfg_path ] , close_fds = True  ) else : os . remove ( self . _cfg_path )"}
{"code_tokens": "def _main ( ) : usage = \"usage: nvd3.py [options]\" parser = OptionParser ( usage = usage , version = ( \"python-nvd3 - Charts generator with \" \"nvd3.js and d3.js\" ) ) parser . add_option ( \"-q\" , \"--quiet\" , action = \"store_false\" , dest = \"verbose\" , default = True , help = \"don't print messages to  stdout\" ) ( options , args ) = parser . parse_args ( )"}
{"code_tokens": "def buildhtmlheader ( self ) : self . htmlheader = '' global _js_initialized if '_js_initialized' not in globals ( ) or not _js_initialized : for css in self . header_css : self . htmlheader += css for js in self  . header_js : self . htmlheader += js"}
{"code_tokens": "def buildcontainer ( self ) : if self . container : return if self . width : if self . width [ - 1 ] != '%' : self . style += 'width:%spx;' % self . width else : self . style += 'width:%s;' % self . width if self . height : if self . height [ - 1  ] != '%' : self . style += 'height:%spx;' % self . height else : self . style += 'height:%s;' % self . height if self . style : self . style = 'style=\"%s\"' % self . style self . container = self . containerheader + '<div id=\"%s\"><svg %s></svg></div>\\n' % ( self . name , self . style )"}
{"code_tokens": "def buildjschart ( self ) : self . jschart = '' if self . tooltip_condition_string == '' : self . tooltip_condition_string = 'var y = String(graph.point.y);\\n' self . series_js =  json . dumps ( self . series )"}
{"code_tokens": "def create_x_axis ( self , name , label = None , format = None , date = False , custom_format = False ) : axis = { } if custom_format and format : axis [ 'tickFormat' ] = format elif format : if format == 'AM_PM' : axis [ 'tickFormat' ] = \"function(d) { return get_am_pm(parseInt(d)); }\" else : axis [ 'tickFormat' ] = \"d3.format(',%s')\" % format if label : axis [ 'axisLabel' ] = \"'\" + label + \"'\" if date : self . dateformat = format axis [ 'tickFormat' ] = ( \"function(d) { return d3.time.format('%s')\" \"(new Date(parseInt(d))) }\\n\" \"\" % self . dateformat ) if name [ 0 ] == 'x' : self . x_axis_date = True self . axislist [ name ] = axis if name == \"xAxis\" and self . focus_enable :  self . axislist [ 'x2Axis' ] = axis"}
{"code_tokens": "def create_y_axis ( self , name , label = None , format = None , custom_format = False ) : axis = { } if custom_format and format : axis [ 'tickFormat' ] = format elif  format : axis [ 'tickFormat' ] = \"d3.format(',%s')\" % format if label : axis [ 'axisLabel' ] = \"'\" + label + \"'\" self . axislist [ name ] = axis"}
{"code_tokens": "def get_conn ( self ) : conn = self . get_connection ( self . sqlite_conn_id ) conn = sqlite3 . connect ( conn . host  ) return conn"}
{"code_tokens": "def action_logging ( f ) : @ functools . wraps ( f ) def wrapper ( * args , ** kwargs ) : with create_session ( ) as session : if g . user . is_anonymous : user = 'anonymous' else : user = g . user . username log = Log ( event = f . __name__ , task_instance = None , owner = user , extra = str ( list ( request . args . items ( ) ) ) , task_id = request . args . get ( 'task_id' ) , dag_id = request . args . get ( 'dag_id' ) ) if 'execution_date' in request . args : log . execution_date = pendulum . parse  ( request . args . get ( 'execution_date' ) ) session . add ( log ) return f ( * args , ** kwargs ) return wrapper"}
{"code_tokens": "def gzipped ( f ) : @ functools . wraps ( f ) def view_func ( * args , ** kwargs ) : @ after_this_request def zipper ( response ) : accept_encoding = request . headers . get ( 'Accept-Encoding' , '' ) if 'gzip' not in accept_encoding . lower ( ) : return response response . direct_passthrough = False if ( response . status_code < 200 or response . status_code >= 300 or 'Content-Encoding' in response . headers ) : return response gzip_buffer = IO ( ) gzip_file = gzip . GzipFile ( mode = 'wb' , fileobj = gzip_buffer ) gzip_file . write ( response . data ) gzip_file . close ( ) response . data = gzip_buffer . getvalue ( ) response  . headers [ 'Content-Encoding' ] = 'gzip' response . headers [ 'Vary' ] = 'Accept-Encoding' response . headers [ 'Content-Length' ] = len ( response . data ) return response return f ( * args , ** kwargs ) return view_func"}
{"code_tokens": "def get_last_dagrun ( dag_id , session , include_externally_triggered = False ) : DR = DagRun query = session . query ( DR ) . filter ( DR . dag_id == dag_id ) if not include_externally_triggered : query = query . filter ( DR . external_trigger == False ) query = query . order_by ( DR . execution_date . desc ( ) ) return query . first  ( )"}
{"code_tokens": "def create_dagrun ( self , run_id , state , execution_date , start_date = None , external_trigger = False , conf = None , session = None ) : return self . get_dag (  ) . create_dagrun ( run_id = run_id , state = state , execution_date = execution_date , start_date = start_date , external_trigger = external_trigger , conf = conf , session = session )"}
{"code_tokens": "def execute ( self , context ) : hook = SQSHook ( aws_conn_id = self . aws_conn_id ) result = hook . send_message ( queue_url = self . sqs_queue , message_body = self . message_content , delay_seconds = self . delay_seconds , message_attributes = self . message_attributes ) self . log . info ( 'result is send_message is %s' ,  result ) return result"}
{"code_tokens": "def json_response ( obj ) : return Response ( response = json . dumps ( obj , indent = 4 , cls = AirflowJsonEncoder ) , status  = 200 , mimetype = \"application/json\" )"}
{"code_tokens": "def open_maybe_zipped ( f , mode = 'r' ) : _ , archive , filename = ZIP_REGEX . search ( f ) . groups ( ) if archive and zipfile . is_zipfile ( archive )  : return zipfile . ZipFile ( archive , mode = mode ) . open ( filename ) else : return io . open ( f , mode = mode )"}
{"code_tokens": "def make_cache_key ( * args , ** kwargs ) : path = request . path args = str ( hash ( frozenset ( request . args . items ( ) ) ) ) return ( path + args ) .  encode ( 'ascii' , 'ignore' )"}
{"code_tokens": "def get_conn ( self ) : if not self . _conn : self . _conn = VideoIntelligenceServiceClient ( credentials = self . _get_credentials ( )  ) return self . _conn"}
{"code_tokens": "def annotate_video ( self , input_uri = None , input_content = None , features = None , video_context = None , output_uri = None , location = None , retry = None , timeout = None , metadata = None , ) : client = self . get_conn ( ) return client . annotate_video ( input_uri = input_uri , input_content = input_content , features = features , video_context  = video_context , output_uri = output_uri , location_id = location , retry = retry , timeout = timeout , metadata = metadata , )"}
{"code_tokens": "def _get_api_key ( self ) : conn = self . get_connection ( self . http_conn_id ) api_key = conn . password if not api_key : raise AirflowException ( 'Opsgenie API Key is required for this hook, ' 'please check your conn_id configuration.' ) return api_key "}
{"code_tokens": "def get_conn ( self , headers = None ) : conn = self . get_connection ( self . http_conn_id ) self . base_url = conn . host if conn . host else 'https://api.opsgenie.com' session = requests . Session  ( ) if headers : session . headers . update ( headers ) return session"}
{"code_tokens": "def execute ( self , payload = { } ) : api_key = self . _get_api_key ( ) return self . run ( endpoint = 'v2/alerts' , data = json . dumps ( payload  ) , headers = { 'Content-Type' : 'application/json' , 'Authorization' : 'GenieKey %s' % api_key } )"}
{"code_tokens": "def _build_opsgenie_payload ( self ) : payload = { } for key in [ \"message\" , \"alias\" , \"description\" , \"responders\" , \"visibleTo\" , \"actions\" , \"tags\" , \"details\" , \"entity\" , \"source\" , \"priority\" , \"user\" , \"note\" ] : val = getattr ( self , key ) if val : payload [ key ] = val  return payload"}
{"code_tokens": "def execute ( self , context ) : self . hook = OpsgenieAlertHook ( self .  opsgenie_conn_id ) self . hook . execute ( self . _build_opsgenie_payload ( ) )"}
{"code_tokens": "def get_conn ( self ) : if not self . conn : self . conn = self . get_client_type ( 'athena' ) return  self . conn"}
{"code_tokens": "def run_query ( self , query , query_context , result_configuration , client_request_token = None ) : response = self . conn . start_query_execution ( QueryString = query , ClientRequestToken = client_request_token , QueryExecutionContext = query_context , ResultConfiguration = result_configuration ) query_execution_id  = response [ 'QueryExecutionId' ] return query_execution_id"}
{"code_tokens": "def check_query_status ( self , query_execution_id ) : response = self . conn . get_query_execution ( QueryExecutionId = query_execution_id ) state = None try : state = response [ 'QueryExecution' ] [ 'Status' ] [ 'State' ] except Exception as ex : self . log . error  ( 'Exception while getting query state' , ex ) finally : return state"}
{"code_tokens": "def poll_query_status ( self , query_execution_id , max_tries = None ) : try_number = 1 final_query_state = None while True : query_state = self . check_query_status ( query_execution_id ) if query_state is None : self . log . info ( 'Trial {try_number}: Invalid query state. Retrying again' . format ( try_number = try_number ) ) elif query_state in self . INTERMEDIATE_STATES : self . log . info ( 'Trial {try_number}: Query is still in an intermediate state - {state}' . format ( try_number = try_number , state = query_state ) ) else : self . log . info (  'Trial {try_number}: Query execution completed. Final state is {state}' . format ( try_number = try_number , state = query_state ) ) final_query_state = query_state break if max_tries and try_number >= max_tries : final_query_state = query_state break try_number += 1 sleep ( self . sleep_time ) return final_query_state"}
{"code_tokens": "def get_conn ( self ) : if self . conn is None : cnopts = pysftp . CnOpts ( ) if self . no_host_key_check : cnopts . hostkeys = None cnopts . compression = self . compress conn_params = { 'host' : self . remote_host , 'port' : self . port , 'username' : self . username , 'cnopts' : cnopts } if self . password and self . password . strip ( ) : conn_params [ 'password' ] = self . password if self . key_file : conn_params [ 'private_key' ] = self . key_file if self . private_key_pass : conn_params [ 'private_key_pass' ] = self . private_key_pass self . conn = pysftp . Connection ( ** conn_params  ) return self . conn"}
{"code_tokens": "def __handle_rate_limit_exception ( self , rate_limit_exception ) : retry_after = int ( rate_limit_exception . response . headers . get ( 'Retry-After' , 60 ) ) self . log . info  ( \"Hit Zendesk API rate limit. Pausing for %s seconds\" , retry_after ) time . sleep ( retry_after )"}
{"code_tokens": "def call ( self , path , query = None , get_all_pages = True , side_loading = False ) : zendesk = self . get_conn ( ) first_request_successful = False while not first_request_successful : try : results = zendesk . call ( path , query ) first_request_successful = True except RateLimitError as rle : self . __handle_rate_limit_exception ( rle ) keys = [ path . split ( \"/\" ) [ - 1 ] . split ( \".json\" ) [ 0 ] ] next_page = results [ 'next_page' ] if side_loading : keys += query [ 'include' ] . split ( ',' ) results = { key : results [ key ] for key in keys } if get_all_pages : while next_page is not None : try :  next_url = next_page . split ( self . __url ) [ 1 ] self . log . info ( \"Calling %s\" , next_url ) more_res = zendesk . call ( next_url ) for key in results : results [ key ] . extend ( more_res [ key ] ) if next_page == more_res [ 'next_page' ] : break else : next_page = more_res [ 'next_page' ] except RateLimitError as rle : self . __handle_rate_limit_exception ( rle ) except ZendeskError as ze : if b\"Use a start_time older than 5 minutes\" in ze . msg : break else : raise ze return results"}
{"code_tokens": "def get_partitions ( self , database_name , table_name , expression = '' , page_size = None , max_items = None ) : config = { 'PageSize' : page_size , 'MaxItems' : max_items , } paginator = self . get_conn ( ) . get_paginator ( 'get_partitions' ) response = paginator . paginate (  DatabaseName = database_name , TableName = table_name , Expression = expression , PaginationConfig = config ) partitions = set ( ) for page in response : for p in page [ 'Partitions' ] : partitions . add ( tuple ( p [ 'Values' ] ) ) return partitions"}
{"code_tokens": "def get_table ( self , database_name , table_name ) : result = self . get_conn ( ) . get_table ( DatabaseName  = database_name , Name = table_name ) return result [ 'Table' ]"}
{"code_tokens": "def get_table_location ( self , database_name , table_name ) : table = self . get_table  ( database_name , table_name ) return table [ 'StorageDescriptor' ] [ 'Location' ]"}
{"code_tokens": "def cluster_status ( self , cluster_identifier ) : conn = self . get_conn ( ) try : response = conn . describe_clusters ( ClusterIdentifier = cluster_identifier ) [ 'Clusters' ] return response [ 0 ] [ 'ClusterStatus' ] if  response else None except conn . exceptions . ClusterNotFoundFault : return 'cluster_not_found'"}
{"code_tokens": "def delete_cluster ( self , cluster_identifier , skip_final_cluster_snapshot = True , final_cluster_snapshot_identifier = '' ) : response = self . get_conn ( ) . delete_cluster (  ClusterIdentifier = cluster_identifier , SkipFinalClusterSnapshot = skip_final_cluster_snapshot , FinalClusterSnapshotIdentifier = final_cluster_snapshot_identifier ) return response [ 'Cluster' ] if response [ 'Cluster' ] else None"}
{"code_tokens": "def describe_cluster_snapshots ( self , cluster_identifier ) : response = self . get_conn ( ) . describe_cluster_snapshots ( ClusterIdentifier = cluster_identifier ) if 'Snapshots' not in response : return None snapshots = response [ 'Snapshots' ]  snapshots = filter ( lambda x : x [ 'Status' ] , snapshots ) snapshots . sort ( key = lambda x : x [ 'SnapshotCreateTime' ] , reverse = True ) return snapshots"}
{"code_tokens": "def restore_from_cluster_snapshot ( self , cluster_identifier , snapshot_identifier ) : response = self . get_conn ( ) . restore_from_cluster_snapshot ( ClusterIdentifier = cluster_identifier , SnapshotIdentifier = snapshot_identifier ) return response [ 'Cluster' ] if response [ 'Cluster'  ] else None"}
{"code_tokens": "def create_cluster_snapshot ( self , snapshot_identifier , cluster_identifier ) : response = self . get_conn ( ) . create_cluster_snapshot ( SnapshotIdentifier = snapshot_identifier , ClusterIdentifier = cluster_identifier , ) return response  [ 'Snapshot' ] if response [ 'Snapshot' ] else None"}
{"code_tokens": "def execute ( self , ** kwargs ) : if not self . api_params : self . construct_api_call_params ( ) slack = SlackHook ( token = self . token ,  slack_conn_id = self . slack_conn_id ) slack . call ( self . method , self . api_params )"}
{"code_tokens": "def create_job_flow ( self , job_flow_overrides ) : if not self . emr_conn_id : raise AirflowException ( 'emr_conn_id must be present to use create_job_flow' ) emr_conn = self . get_connection ( self . emr_conn_id ) config = emr_conn . extra_dejson . copy ( ) config . update ( job_flow_overrides ) response = self . get_conn ( ) .  run_job_flow ( ** config ) return response"}
{"code_tokens": "def filter_for_filesize ( result , size = None ) : if size : log = LoggingMixin ( ) . log log . debug ( 'Filtering for file size >= %s in files: %s' , size , map ( lambda x : x [ 'path' ] , result ) ) size *= settings . MEGABYTE result = [ x for x in  result if x [ 'length' ] >= size ] log . debug ( 'HdfsSensor.poke: after size filter result is %s' , result ) return result"}
{"code_tokens": "def filter_for_ignored_ext ( result , ignored_ext , ignore_copying ) : if ignore_copying : log = LoggingMixin ( ) . log regex_builder = r\"^.*\\.(%s$)$\" % '$|' . join ( ignored_ext ) ignored_extensions_regex = re . compile ( regex_builder ) log . debug ( 'Filtering result for ignored extensions: %s in files %s' , ignored_extensions_regex . pattern , map  ( lambda x : x [ 'path' ] , result ) ) result = [ x for x in result if not ignored_extensions_regex . match ( x [ 'path' ] ) ] log . debug ( 'HdfsSensor.poke: after ext filter result is %s' , result ) return result"}
{"code_tokens": "def execute ( self , context ) : s3_conn = S3Hook ( self . s3_conn_id ) if self . is_pipeline : results = MongoHook ( self . mongo_conn_id ) . aggregate ( mongo_collection = self . mongo_collection , aggregate_query = self . mongo_query , mongo_db = self . mongo_db ) else : results = MongoHook ( self . mongo_conn_id ) . find ( mongo_collection = self . mongo_collection , query = self . mongo_query , mongo_db = self . mongo_db ) docs_str = self . _stringify ( self . transform ( results ) ) s3_conn . load_string ( string_data =  docs_str , key = self . s3_key , bucket_name = self . s3_bucket , replace = self . replace ) return True"}
{"code_tokens": "def get_pool ( name , session = None ) : if not ( name and name . strip ( ) ) : raise AirflowBadRequest ( \"Pool name shouldn't be empty\" ) pool = session . query ( Pool ) . filter_by ( pool = name ) . first ( ) if pool is None :  raise PoolNotFound ( \"Pool '%s' doesn't exist\" % name ) return pool"}
{"code_tokens": "def create_pool ( name , slots , description , session = None ) : if not ( name and name . strip ( ) ) : raise AirflowBadRequest ( \"Pool name shouldn't be empty\" ) try : slots = int ( slots ) except ValueError : raise AirflowBadRequest ( \"Bad value for `slots`: %s\" % slots ) session . expire_on_commit = False pool = session . query ( Pool ) . filter_by ( pool = name ) . first ( ) if pool is  None : pool = Pool ( pool = name , slots = slots , description = description ) session . add ( pool ) else : pool . slots = slots pool . description = description session . commit ( ) return pool"}
{"code_tokens": "def delete_pool ( name , session = None ) : if not ( name and name . strip ( ) ) : raise AirflowBadRequest ( \"Pool name shouldn't be empty\" ) pool = session . query ( Pool ) . filter_by ( pool = name ) . first ( ) if pool is None : raise PoolNotFound ( \"Pool  '%s' doesn't exist\" % name ) session . delete ( pool ) session . commit ( ) return pool"}
{"code_tokens": "def _dict_to_proto ( py_dict , proto ) : dict_json_str = json . dumps ( py_dict ) return json_format  . Parse ( dict_json_str , proto )"}
{"code_tokens": "def wait_for_operation ( self , operation , project_id = None ) : self . log . info ( \"Waiting for OPERATION_NAME %s\" , operation . name ) time . sleep ( OPERATIONAL_POLL_INTERVAL ) while operation . status != Operation . Status . DONE : if operation . status == Operation . Status . RUNNING or operation . status == Operation . Status . PENDING : time . sleep ( OPERATIONAL_POLL_INTERVAL ) else : raise exceptions . GoogleCloudError ( \"Operation  has failed with status: %s\" % operation . status ) operation = self . get_operation ( operation . name , project_id = project_id or self . project_id ) return operation"}
{"code_tokens": "def get_operation ( self , operation_name , project_id = None ) : return self . get_client ( ) . get_operation ( project_id = project_id or self . project_id , zone = self .  location , operation_id = operation_name )"}
{"code_tokens": "def _append_label ( cluster_proto , key , val ) : val = val . replace ( '.' , '-' ) . replace ( '+' , '-' ) cluster_proto . resource_labels . update ( { key : val } ) return  cluster_proto"}
{"code_tokens": "def create_cluster ( self , cluster , project_id = None , retry = DEFAULT , timeout = DEFAULT ) : if isinstance ( cluster , dict ) : cluster_proto = Cluster ( ) cluster = self . _dict_to_proto ( py_dict = cluster , proto = cluster_proto ) elif not isinstance ( cluster , Cluster ) : raise AirflowException ( \"cluster is not instance of Cluster proto or python dict\" ) self . _append_label ( cluster , 'airflow-version' , 'v' + version . version ) self . log . info ( \"Creating (project_id=%s, zone=%s, cluster_name=%s)\" , self . project_id , self . location , cluster . name ) try : op = self . get_client ( ) . create_cluster ( project_id = project_id or self . project_id , zone = self . location , cluster = cluster  , retry = retry , timeout = timeout ) op = self . wait_for_operation ( op ) return op . target_link except AlreadyExists as error : self . log . info ( 'Assuming Success: %s' , error . message ) return self . get_cluster ( name = cluster . name ) . self_link"}
{"code_tokens": "def get_cluster ( self , name , project_id = None , retry = DEFAULT , timeout = DEFAULT ) : self . log . info ( \"Fetching cluster (project_id=%s, zone=%s, cluster_name=%s)\" , project_id or self . project_id , self . location , name ) return self . get_client ( ) . get_cluster ( project_id = project_id or self . project_id , zone = self . location , cluster_id = name , retry = retry , timeout  = timeout ) . self_link"}
{"code_tokens": "def _get_webhook_endpoint ( self , http_conn_id , webhook_endpoint ) : if webhook_endpoint : endpoint = webhook_endpoint elif http_conn_id : conn = self . get_connection ( http_conn_id ) extra = conn . extra_dejson endpoint = extra . get ( 'webhook_endpoint' , '' ) else : raise AirflowException ( 'Cannot get webhook endpoint: No valid Discord ' 'webhook endpoint or http_conn_id supplied.' ) if not re . match ( '^webhooks/[0-9]+/[a-zA-Z0-9_-]+$' , endpoint ) : raise AirflowException ( 'Expected Discord webhook endpoint in the form  ' 'of \"webhooks/{webhook.id}/{webhook.token}\".' ) return endpoint"}
{"code_tokens": "def _build_discord_payload ( self ) : payload = { } if self . username : payload [ 'username' ] = self . username if self . avatar_url : payload [ 'avatar_url' ] = self . avatar_url payload [ 'tts' ] = self . tts if  len ( self . message ) <= 2000 : payload [ 'content' ] = self . message else : raise AirflowException ( 'Discord message length must be 2000 or fewer ' 'characters.' ) return json . dumps ( payload )"}
{"code_tokens": "def execute ( self ) : proxies = { } if self . proxy : proxies = { 'https' : self . proxy } discord_payload = self . _build_discord_payload ( ) self . run ( endpoint = self . webhook_endpoint , data = discord_payload , headers = { 'Content-type' : 'application/json' } , extra_options = { 'proxies'  : proxies } )"}
{"code_tokens": "def encrypt ( self , key_name , plaintext , authenticated_data = None ) : keys = self . get_conn ( ) . projects ( ) . locations ( ) . keyRings ( ) . cryptoKeys ( ) body = { 'plaintext' : _b64encode ( plaintext ) } if authenticated_data : body [ 'additionalAuthenticatedData' ] = _b64encode ( authenticated_data ) request = keys  . encrypt ( name = key_name , body = body ) response = request . execute ( num_retries = self . num_retries ) ciphertext = response [ 'ciphertext' ] return ciphertext"}
{"code_tokens": "def import_table ( self , table , target_dir = None , append = False , file_type = \"text\" , columns = None , split_by = None , where = None , direct = False , driver = None , extra_import_options = None ) : cmd = self .  _import_cmd ( target_dir , append , file_type , split_by , direct , driver , extra_import_options ) cmd += [ \"--table\" , table ] if columns : cmd += [ \"--columns\" , columns ] if where : cmd += [ \"--where\" , where ] self . Popen ( cmd )"}
{"code_tokens": "def import_query ( self , query , target_dir , append = False , file_type = \"text\" , split_by = None , direct = None , driver = None , extra_import_options = None ) : cmd = self . _import_cmd ( target_dir , append , file_type , split_by , direct  , driver , extra_import_options ) cmd += [ \"--query\" , query ] self . Popen ( cmd )"}
{"code_tokens": "def export_table ( self , table , export_dir , input_null_string , input_null_non_string , staging_table , clear_staging_table , enclosed_by , escaped_by , input_fields_terminated_by , input_lines_terminated_by , input_optionally_enclosed_by , batch , relaxed_isolation , extra_export_options = None ) : cmd = self . _export_cmd ( table , export_dir , input_null_string , input_null_non_string , staging_table , clear_staging_table , enclosed_by , escaped_by , input_fields_terminated_by , input_lines_terminated_by , input_optionally_enclosed_by , batch ,  relaxed_isolation , extra_export_options ) self . Popen ( cmd )"}
{"code_tokens": "def get_conn ( self ) : if not self . _client : self . _client = TextToSpeechClient ( credentials = self . _get_credentials ( ) ) return self  . _client"}
{"code_tokens": "def synthesize_speech ( self , input_data , voice , audio_config , retry = None , timeout = None ) : client = self . get_conn ( ) self . log . info  ( \"Synthesizing input: %s\" % input_data ) return client . synthesize_speech ( input_ = input_data , voice = voice , audio_config = audio_config , retry = retry , timeout = timeout )"}
{"code_tokens": "def close ( self ) : if self . closed : return super ( ) . close ( ) if not self . upload_on_close : return local_loc = os . path . join ( self . local_base , self . log_relative_path ) remote_loc = os . path . join ( self . remote_base , self . log_relative_path ) if os .  path . exists ( local_loc ) : with open ( local_loc , 'r' ) as logfile : log = logfile . read ( ) self . s3_write ( log , remote_loc ) self . closed = True"}
{"code_tokens": "def _get_init_containers ( self ) : if self . kube_config . dags_volume_claim or self . kube_config . dags_volume_host or self . kube_config . dags_in_image : return [ ] init_environment = [ { 'name' : 'GIT_SYNC_REPO' , 'value' : self . kube_config . git_repo } , { 'name' : 'GIT_SYNC_BRANCH' , 'value' : self . kube_config . git_branch } , { 'name' : 'GIT_SYNC_ROOT' , 'value' : self . kube_config . git_sync_root } , { 'name' : 'GIT_SYNC_DEST' , 'value' : self . kube_config . git_sync_dest } , { 'name' : 'GIT_SYNC_DEPTH' , 'value' : '1' } , { 'name' : 'GIT_SYNC_ONE_TIME' , 'value' : 'true' } ] if self . kube_config . git_user : init_environment . append ( { 'name' : 'GIT_SYNC_USERNAME' , 'value' : self . kube_config . git_user } ) if self . kube_config . git_password : init_environment . append ( { 'name' : 'GIT_SYNC_PASSWORD' , 'value' : self . kube_config . git_password } ) volume_mounts = [ { 'mountPath' : self . kube_config . git_sync_root , 'name' : self . dags_volume_name , 'readOnly' : False } ] if self . kube_config . git_ssh_key_secret_name : volume_mounts . append ( { 'name' : self . git_sync_ssh_secret_volume_name , 'mountPath' : '/etc/git-secret/ssh' , 'subPath' : 'ssh' } ) init_environment . extend ( [ { 'name' : 'GIT_SSH_KEY_FILE' , 'value' : '/etc/git-secret/ssh' } , { 'name' : 'GIT_SYNC_SSH' , 'value' : 'true' } ] ) if self . kube_config . git_ssh_known_hosts_configmap_name : volume_mounts . append ( { 'name' : self . git_sync_ssh_known_hosts_volume_name , 'mountPath' : '/etc/git-secret/known_hosts' , 'subPath' : 'known_hosts' } ) init_environment . extend ( [ { 'name' : 'GIT_KNOWN_HOSTS' , 'value' : 'true' } , { 'name' : 'GIT_SSH_KNOWN_HOSTS_FILE' , 'value' : '/etc/git-secret/known_hosts' } ] ) else : init_environment . append ( { 'name' : 'GIT_KNOWN_HOSTS' , 'value'  : 'false' } ) return [ { 'name' : self . kube_config . git_sync_init_container_name , 'image' : self . kube_config . git_sync_container , 'securityContext' : { 'runAsUser' : 65533 } , 'env' : init_environment , 'volumeMounts' : volume_mounts } ]"}
{"code_tokens": "def _get_environment ( self ) : env = { } for env_var_name , env_var_val in six . iteritems ( self . kube_config . kube_env_vars ) : env [ env_var_name ] = env_var_val env [ \"AIRFLOW__CORE__EXECUTOR\" ] = \"LocalExecutor\" if self . kube_config . airflow_configmap : env [ 'AIRFLOW_HOME' ] = self . worker_airflow_home env [ 'AIRFLOW__CORE__DAGS_FOLDER' ] = self . worker_airflow_dags if ( not self . kube_config . airflow_configmap and 'AIRFLOW__CORE__SQL_ALCHEMY_CONN' not in self . kube_config . kube_secrets ) : env [ 'AIRFLOW__CORE__SQL_ALCHEMY_CONN' ] = conf . get ( \"core\" , \"SQL_ALCHEMY_CONN\" ) if self . kube_config . git_dags_folder_mount_point : dag_volume_mount_path = os . path . join ( self . kube_config  . git_dags_folder_mount_point , self . kube_config . git_sync_dest , self . kube_config . git_subpath ) env [ 'AIRFLOW__CORE__DAGS_FOLDER' ] = dag_volume_mount_path return env"}
{"code_tokens": "def _get_secrets ( self ) : worker_secrets = [ ] for env_var_name , obj_key_pair in six . iteritems ( self . kube_config . kube_secrets ) : k8s_secret_obj , k8s_secret_key = obj_key_pair . split ( '=' ) worker_secrets . append ( Secret ( 'env' ,  env_var_name , k8s_secret_obj , k8s_secret_key ) ) if self . kube_config . env_from_secret_ref : for secret_ref in self . kube_config . env_from_secret_ref . split ( ',' ) : worker_secrets . append ( Secret ( 'env' , None , secret_ref ) ) return worker_secrets"}
{"code_tokens": "def _get_security_context ( self ) : security_context = { } if self . kube_config . worker_run_as_user : security_context [ 'runAsUser' ] = self . kube_config . worker_run_as_user if self . kube_config . worker_fs_group : security_context  [ 'fsGroup' ] = self . kube_config . worker_fs_group if self . kube_config . git_ssh_key_secret_name and security_context . get ( 'fsGroup' ) is None : security_context [ 'fsGroup' ] = 65533 return security_context"}
{"code_tokens": "def get_extra_links ( self , operator , dttm ) : conn = BaseHook . get_connection ( operator . kwargs [ 'qubole_conn_id' ] ) if conn and conn . host : host = re . sub ( r'api$' , 'v2/analyze?command_id=' , conn . host ) else : host = 'https://api.qubole.com/v2/analyze?command_id=' ti = TaskInstance ( task = operator , execution_date = dttm ) qds_command_id  = ti . xcom_pull ( task_ids = operator . task_id , key = 'qbol_cmd_id' ) url = host + str ( qds_command_id ) if qds_command_id else '' return url"}
{"code_tokens": "def heartbeat ( self ) : try : with create_session ( ) as session : job = session . query ( BaseJob ) . filter_by ( id = self . id ) . one ( ) make_transient ( job ) session . commit ( ) if job . state == State . SHUTDOWN : self . kill ( ) is_unit_test = conf . getboolean ( 'core' , 'unit_test_mode' ) if not is_unit_test : sleep_for = 0 if job . latest_heartbeat : seconds_remaining = self . heartrate - ( timezone . utcnow ( ) - job . latest_heartbeat ) . total_seconds ( ) sleep_for = max ( 0 , seconds_remaining ) sleep ( sleep_for ) with create_session (  ) as session : job = session . query ( BaseJob ) . filter ( BaseJob . id == self . id ) . first ( ) job . latest_heartbeat = timezone . utcnow ( ) session . merge ( job ) session . commit ( ) self . heartbeat_callback ( session = session ) self . log . debug ( '[heartbeat]' ) except OperationalError as e : self . log . error ( \"Scheduler heartbeat got an exception: %s\" , str ( e ) )"}
{"code_tokens": "def _launch_process ( result_queue , file_path , pickle_dags , dag_id_white_list , thread_name , zombies ) : def helper ( ) : log = logging . getLogger ( \"airflow.processor\" ) stdout = StreamLogWriter ( log , logging . INFO ) stderr = StreamLogWriter ( log , logging . WARN ) set_context ( log , file_path ) try : sys . stdout = stdout sys . stderr = stderr settings . configure_orm ( ) threading . current_thread ( ) . name = thread_name start_time = time . time ( ) log . info ( \"Started process (PID=%s) to work on %s\" , os . getpid ( ) , file_path ) scheduler_job = SchedulerJob ( dag_ids = dag_id_white_list , log = log ) result = scheduler_job . process_file (  file_path , zombies , pickle_dags ) result_queue . put ( result ) end_time = time . time ( ) log . info ( \"Processing %s took %.3f seconds\" , file_path , end_time - start_time ) except Exception : log . exception ( \"Got an exception! Propagating...\" ) raise finally : sys . stdout = sys . __stdout__ sys . stderr = sys . __stderr__ settings . dispose_orm ( ) p = multiprocessing . Process ( target = helper , args = ( ) , name = \"{}-Process\" . format ( thread_name ) ) p . start ( ) return p"}
{"code_tokens": "def start ( self ) : self . _process = DagFileProcessor . _launch_process ( self . _result_queue , self . file_path , self . _pickle_dags , self . _dag_id_white_list  , \"DagFileProcessor{}\" . format ( self . _instance_id ) , self . _zombies ) self . _start_time = timezone . utcnow ( )"}
{"code_tokens": "def done ( self ) : if self . _process is None : raise AirflowException ( \"Tried to see if it's done before starting!\" ) if self . _done : return True if self . _result_queue and not self . _result_queue . empty ( ) : self . _result = self . _result_queue . get_nowait ( ) self . _done = True self . log . debug ( \"Waiting for %s\" , self . _process ) self . _process . join ( ) return True if self . _result_queue and not self .  _process . is_alive ( ) : self . _done = True if not self . _result_queue . empty ( ) : self . _result = self . _result_queue . get_nowait ( ) self . log . debug ( \"Waiting for %s\" , self . _process ) self . _process . join ( ) return True return False"}
{"code_tokens": "def _exit_gracefully ( self , signum , frame ) : self . log . info ( \"Exiting gracefully upon receiving signal %s\" , signum ) if self  . processor_agent : self . processor_agent . end ( ) sys . exit ( os . EX_OK )"}
{"code_tokens": "def update_import_errors ( session , dagbag ) : for dagbag_file in dagbag . file_last_changed : session . query ( errors . ImportError ) . filter ( errors . ImportError . filename == dagbag_file ) . delete ( ) for filename , stacktrace in six . iteritems ( dagbag . import_errors ) : session . add ( errors . ImportError ( filename = filename , stacktrace =  stacktrace ) ) session . commit ( )"}
{"code_tokens": "def _process_task_instances ( self , dag , queue , session = None ) : dag_runs = DagRun . find ( dag_id = dag . dag_id , state = State . RUNNING , session = session ) active_dag_runs = [ ] for run in dag_runs : self . log . info ( \"Examining DAG run %s\" , run ) if run . execution_date > timezone . utcnow ( ) : self . log . error ( \"Execution date is in future: %s\" , run . execution_date ) continue if len ( active_dag_runs ) >= dag . max_active_runs : self . log . info ( \"Number of active dag runs reached max_active_run.\" ) break if run . is_backfill : continue run . dag = dag run . verify_integrity ( session = session ) run . update_state ( session = session ) if run . state == State . RUNNING : make_transient ( run ) active_dag_runs . append ( run ) for run in active_dag_runs : self . log . debug ( \"Examining active DAG run: %s\" , run ) tis = run . get_task_instances ( state = ( State . NONE , State . UP_FOR_RETRY , State . UP_FOR_RESCHEDULE ) ) for ti in tis : task = dag . get_task ( ti . task_id ) ti . task = task if ti . are_dependencies_met ( dep_context = DepContext ( flag_upstream_failed = True ) , session = session ) : self . log . debug ( 'Queuing task: %s' , ti )  queue . append ( ti . key )"}
{"code_tokens": "def _change_state_for_tis_without_dagrun ( self , simple_dag_bag , old_states , new_state , session = None ) : tis_changed = 0 query = session . query ( models . TaskInstance ) . outerjoin ( models . DagRun , and_ ( models . TaskInstance . dag_id == models . DagRun . dag_id , models . TaskInstance . execution_date == models . DagRun . execution_date ) ) . filter ( models . TaskInstance . dag_id . in_ ( simple_dag_bag . dag_ids ) ) . filter ( models . TaskInstance . state . in_ ( old_states ) ) . filter ( or_ ( models . DagRun . state != State . RUNNING , models . DagRun . state . is_ ( None ) ) ) if self . using_sqlite : tis_to_change = query . with_for_update ( ) . all ( ) for ti in tis_to_change : ti . set_state ( new_state , session = session ) tis_changed += 1 else : subq = query . subquery ( ) tis_changed = session . query ( models . TaskInstance ) . filter ( and_ ( models . TaskInstance . dag_id == subq . c . dag_id , models . TaskInstance . task_id == subq . c . task_id , models . TaskInstance . execution_date == subq . c . execution_date ) ) . update ( { models . TaskInstance . state : new_state } , synchronize_session = False ) session . commit ( ) if tis_changed > 0 : self . log . warning (  \"Set %s task instances to state=%s as their associated DagRun was not in RUNNING state\" , tis_changed , new_state )"}
{"code_tokens": "def __get_concurrency_maps ( self , states , session = None ) : TI = models . TaskInstance ti_concurrency_query = ( session . query ( TI . task_id , TI . dag_id , func . count ( '*' ) ) . filter ( TI . state . in_ ( states ) ) . group_by ( TI . task_id , TI . dag_id ) ) . all ( ) dag_map = defaultdict ( int ) task_map = defaultdict ( int ) for result in ti_concurrency_query : task_id , dag_id , count =  result dag_map [ dag_id ] += count task_map [ ( dag_id , task_id ) ] = count return dag_map , task_map"}
{"code_tokens": "def _change_state_for_executable_task_instances ( self , task_instances , acceptable_states , session = None ) : if len ( task_instances ) == 0 : session . commit ( ) return [ ] TI = models . TaskInstance filter_for_ti_state_change = ( [ and_ ( TI . dag_id == ti . dag_id , TI . task_id == ti . task_id , TI . execution_date == ti . execution_date ) for ti in task_instances ] ) ti_query = ( session . query ( TI ) . filter ( or_ ( * filter_for_ti_state_change ) ) ) if None in acceptable_states : ti_query = ti_query . filter ( or_ ( TI . state == None , TI . state . in_ ( acceptable_states ) ) ) else : ti_query = ti_query . filter ( TI . state . in_ ( acceptable_states ) ) tis_to_set_to_queued = ( ti_query . with_for_update ( ) . all ( ) ) if len  ( tis_to_set_to_queued ) == 0 : self . log . info ( \"No tasks were able to have their state changed to queued.\" ) session . commit ( ) return [ ] for task_instance in tis_to_set_to_queued : task_instance . state = State . QUEUED task_instance . queued_dttm = ( timezone . utcnow ( ) if not task_instance . queued_dttm else task_instance . queued_dttm ) session . merge ( task_instance ) simple_task_instances = [ SimpleTaskInstance ( ti ) for ti in tis_to_set_to_queued ] task_instance_str = \"\\n\\t\" . join ( [ repr ( x ) for x in tis_to_set_to_queued ] ) session . commit ( ) self . log . info ( \"Setting the following %s tasks to queued state:\\n\\t%s\" , len ( tis_to_set_to_queued ) , task_instance_str ) return simple_task_instances"}
{"code_tokens": "def _enqueue_task_instances_with_queued_state ( self , simple_dag_bag , simple_task_instances ) : TI = models . TaskInstance for simple_task_instance in simple_task_instances : simple_dag = simple_dag_bag . get_dag ( simple_task_instance . dag_id ) command = TI . generate_command ( simple_task_instance . dag_id , simple_task_instance . task_id , simple_task_instance . execution_date , local = True , mark_success = False , ignore_all_deps = False , ignore_depends_on_past = False , ignore_task_deps = False , ignore_ti_state = False , pool = simple_task_instance . pool , file_path = simple_dag . full_filepath , pickle_id = simple_dag . pickle_id ) priority = simple_task_instance . priority_weight queue = simple_task_instance . queue self . log . info  ( \"Sending %s to executor with priority %s and queue %s\" , simple_task_instance . key , priority , queue ) self . executor . queue_command ( simple_task_instance , command , priority = priority , queue = queue )"}
{"code_tokens": "def _execute_task_instances ( self , simple_dag_bag , states , session = None ) : executable_tis = self . _find_executable_task_instances ( simple_dag_bag , states , session = session ) def query ( result , items ) : simple_tis_with_state_changed = self . _change_state_for_executable_task_instances ( items , states , session = session ) self . _enqueue_task_instances_with_queued_state ( simple_dag_bag , simple_tis_with_state_changed ) session . commit ( ) return result + len ( simple_tis_with_state_changed ) return helpers . reduce_in_chunks ( query ,  executable_tis , 0 , self . max_tis_per_query )"}
{"code_tokens": "def _change_state_for_tasks_failed_to_execute ( self , session ) : if self . executor . queued_tasks : TI = models . TaskInstance filter_for_ti_state_change = ( [ and_ ( TI . dag_id == dag_id , TI . task_id == task_id , TI . execution_date == execution_date , TI . _try_number == try_number - 1 , TI . state == State . QUEUED ) for dag_id , task_id , execution_date , try_number in self . executor . queued_tasks . keys ( ) ] ) ti_query = ( session . query ( TI ) . filter ( or_ ( * filter_for_ti_state_change ) ) ) tis_to_set_to_scheduled = ( ti_query .  with_for_update ( ) . all ( ) ) if len ( tis_to_set_to_scheduled ) == 0 : session . commit ( ) return for task_instance in tis_to_set_to_scheduled : task_instance . state = State . SCHEDULED task_instance_str = \"\\n\\t\" . join ( [ repr ( x ) for x in tis_to_set_to_scheduled ] ) session . commit ( ) self . log . info ( \"Set the following tasks to scheduled state:\\n\\t%s\" , task_instance_str )"}
{"code_tokens": "def _process_executor_events ( self , simple_dag_bag , session = None ) : TI = models . TaskInstance for key , state in list ( self . executor . get_event_buffer ( simple_dag_bag . dag_ids ) . items ( ) ) : dag_id , task_id , execution_date , try_number = key self . log . info ( \"Executor reports execution of %s.%s execution_date=%s \" \"exited with status %s for try_number %s\" , dag_id , task_id , execution_date , state , try_number ) if state == State . FAILED or state == State . SUCCESS : qry = session . query ( TI ) . filter ( TI . dag_id == dag_id , TI . task_id == task_id , TI . execution_date == execution_date ) ti = qry . first ( ) if not ti : self . log . warning ( \"TaskInstance %s went missing from the database\" , ti ) continue if ti . try_number == try_number and ti . state == State . QUEUED : msg = ( \"Executor  reports task instance {} finished ({}) \" \"although the task says its {}. Was the task \" \"killed externally?\" . format ( ti , state , ti . state ) ) self . log . error ( msg ) try : simple_dag = simple_dag_bag . get_dag ( dag_id ) dagbag = models . DagBag ( simple_dag . full_filepath ) dag = dagbag . get_dag ( dag_id ) ti . task = dag . get_task ( task_id ) ti . handle_failure ( msg ) except Exception : self . log . error ( \"Cannot load the dag bag to handle failure for %s\" \". Setting task to FAILED without callbacks or \" \"retries. Do you have enough resources?\" , ti ) ti . state = State . FAILED session . merge ( ti ) session . commit ( )"}
{"code_tokens": "def process_file ( self , file_path , zombies , pickle_dags = False , session = None ) : self . log . info ( \"Processing file %s for tasks to queue\" , file_path ) simple_dags = [ ] try : dagbag = models . DagBag ( file_path , include_examples = False ) except Exception : self . log . exception ( \"Failed at reloading the DAG file %s\" , file_path ) Stats . incr ( 'dag_file_refresh_error' , 1 , 1 ) return [ ] if len ( dagbag . dags ) > 0 : self . log . info ( \"DAG(s) %s retrieved from %s\" , dagbag . dags . keys ( ) , file_path ) else : self . log . warning ( \"No viable dags retrieved from %s\" , file_path ) self . update_import_errors ( session , dagbag ) return [ ] for dag in dagbag . dags . values ( ) : dag . sync_to_db ( ) paused_dag_ids = [ dag . dag_id for dag in dagbag . dags . values ( ) if dag . is_paused ] for dag_id in dagbag . dags : if dag_id not in paused_dag_ids : dag = dagbag . get_dag ( dag_id ) pickle_id = None if pickle_dags : pickle_id = dag . pickle ( session ) . id simple_dags . append ( SimpleDag ( dag , pickle_id = pickle_id ) ) if len ( self . dag_ids ) > 0 : dags = [ dag for dag in dagbag . dags . values ( ) if dag . dag_id in self . dag_ids and dag . dag_id not in paused_dag_ids ] else : dags = [ dag for dag in dagbag . dags . values ( ) if not dag . parent_dag and dag . dag_id not in paused_dag_ids ] ti_keys_to_schedule = [ ] self . _process_dags ( dagbag , dags , ti_keys_to_schedule ) for ti_key in ti_keys_to_schedule : dag = dagbag . dags [ ti_key [ 0 ] ] task = dag . get_task ( ti_key [ 1 ] ) ti = models . TaskInstance ( task , ti_key [ 2 ] ) ti . refresh_from_db ( session = session , lock_for_update = True ) dep_context = DepContext ( deps = QUEUE_DEPS , ignore_task_deps = True ) if ti . are_dependencies_met ( dep_context = dep_context , session = session , verbose = True ) : ti . state = State . SCHEDULED self . log . info ( \"Creating / updating %s in ORM\" , ti ) session . merge ( ti ) session . commit ( ) try : self . update_import_errors ( session , dagbag ) except Exception : self . log . exception ( \"Error logging import errors!\" ) try : dagbag . kill_zombies ( zombies  ) except Exception : self . log . exception ( \"Error killing zombies!\" ) return simple_dags"}
{"code_tokens": "def _update_counters ( self , ti_status ) : for key , ti in list ( ti_status . running . items ( ) ) : ti . refresh_from_db ( ) if ti . state == State . SUCCESS : ti_status . succeeded . add ( key ) self . log . debug ( \"Task instance %s succeeded. Don't rerun.\" , ti ) ti_status . running . pop ( key ) continue elif ti . state == State . SKIPPED : ti_status . skipped . add ( key ) self . log . debug ( \"Task instance %s skipped. Don't rerun.\" , ti ) ti_status . running . pop ( key ) continue elif ti . state == State . FAILED : self . log . error ( \"Task instance %s failed\" , ti ) ti_status . failed . add ( key ) ti_status . running . pop ( key ) continue elif ti . state == State . UP_FOR_RETRY : self . log . warning ( \"Task instance %s is up for retry\" , ti ) ti_status . running . pop ( key ) ti_status . to_run [ key ] = ti elif ti . state == State . UP_FOR_RESCHEDULE : self . log . warning ( \"Task instance %s is up for reschedule\" , ti ) ti_status . running . pop ( key ) ti_status . to_run [ key ] = ti elif ti . state == State . NONE : self . log . warning ( \"FIXME: task instance %s state was set to none externally  or \" \"reaching concurrency limits. Re-adding task to queue.\" , ti ) ti . set_state ( State . SCHEDULED ) ti_status . running . pop ( key ) ti_status . to_run [ key ] = ti"}
{"code_tokens": "def _manage_executor_state ( self , running ) : executor = self . executor for key , state in list ( executor . get_event_buffer ( ) . items ( ) ) : if key not in running : self . log . warning ( \"%s state %s not in running=%s\" , key , state , running . values ( ) ) continue ti = running [ key ] ti . refresh_from_db ( ) self . log . debug ( \"Executor state: %s task %s\" , state , ti ) if state == State . FAILED or state == State . SUCCESS :  if ti . state == State . RUNNING or ti . state == State . QUEUED : msg = ( \"Executor reports task instance {} finished ({}) \" \"although the task says its {}. Was the task \" \"killed externally?\" . format ( ti , state , ti . state ) ) self . log . error ( msg ) ti . handle_failure ( msg )"}
{"code_tokens": "def _get_dag_run ( self , run_date , session = None ) : run_id = BackfillJob . ID_FORMAT_PREFIX . format ( run_date . isoformat ( ) ) respect_dag_max_active_limit = ( True if ( self . dag . schedule_interval and not self . dag . is_subdag ) else False ) current_active_dag_count = self . dag . get_num_active_runs ( external_trigger = False ) run = DagRun . find ( dag_id = self . dag . dag_id , execution_date = run_date , session = session ) if run is not None and len ( run ) > 0 : run = run [ 0 ] if run . state == State . RUNNING : respect_dag_max_active_limit = False else : run = None if ( respect_dag_max_active_limit and current_active_dag_count >= self . dag . max_active_runs ) : return None run = run or self . dag . create_dagrun ( run_id = run_id , execution_date = run_date , start_date = timezone . utcnow ( ) , state = State . RUNNING , external_trigger = False , session = session , conf = self .  conf , ) run . dag = self . dag run . state = State . RUNNING run . run_id = run_id run . verify_integrity ( session = session ) return run"}
{"code_tokens": "def _task_instances_for_dag_run ( self , dag_run , session = None ) : tasks_to_run = { } if dag_run is None : return tasks_to_run self . reset_state_for_orphaned_tasks ( filter_by_dag_run = dag_run , session = session ) dag_run . refresh_from_db ( ) make_transient ( dag_run ) for ti in dag_run . get_task_instances ( ) : if ti . state == State . NONE : ti . set_state ( State . SCHEDULED , session = session ) if ti . state != State . REMOVED : tasks_to_run [ ti . key  ] = ti return tasks_to_run"}
{"code_tokens": "def _execute_for_run_dates ( self , run_dates , ti_status , executor , pickle_id , start_date , session = None ) : for next_run_date in run_dates : dag_run = self . _get_dag_run ( next_run_date , session = session ) tis_map = self . _task_instances_for_dag_run ( dag_run , session = session ) if dag_run is None : continue ti_status . active_runs . append ( dag_run ) ti_status . to_run . update ( tis_map or { } ) processed_dag_run_dates = self . _process_backfill_task_instances ( ti_status = ti_status , executor = executor , pickle_id = pickle_id , start_date = start_date , session = session ) ti_status  . executed_dag_run_dates . update ( processed_dag_run_dates )"}
{"code_tokens": "def _set_unfinished_dag_runs_to_failed ( self , dag_runs , session = None ) : for dag_run in dag_runs : dag_run . update_state ( ) if dag_run . state not in State . finished ( ) : dag_run . set_state ( State . FAILED ) session .  merge ( dag_run )"}
{"code_tokens": "def _execute ( self , session = None ) : ti_status = BackfillJob . _DagRunTaskStatus ( ) start_date = self . bf_start_date run_dates = self . dag . get_run_dates ( start_date = start_date , end_date = self . bf_end_date ) if self . run_backwards : tasks_that_depend_on_past = [ t . task_id for t in self . dag . task_dict . values ( ) if t . depends_on_past ] if tasks_that_depend_on_past : raise AirflowException ( 'You cannot backfill backwards because one or more tasks depend_on_past: {}' . format ( \",\" . join ( tasks_that_depend_on_past ) ) ) run_dates = run_dates [ : : - 1 ] if len ( run_dates ) == 0 : self . log . info ( \"No run dates were found for the given dates and dag interval.\" ) return pickle_id = None if not self . donot_pickle and self . executor . __class__ not in ( executors . LocalExecutor , executors . SequentialExecutor ) : pickle = DagPickle ( self . dag ) session . add ( pickle ) session . commit ( ) pickle_id = pickle . id executor = self . executor executor . start ( ) ti_status . total_runs = len ( run_dates ) try : remaining_dates = ti_status . total_runs while remaining_dates > 0 : dates_to_process = [ run_date for run_date in run_dates if run_date not in ti_status . executed_dag_run_dates ] self . _execute_for_run_dates ( run_dates = dates_to_process , ti_status = ti_status , executor = executor , pickle_id = pickle_id , start_date = start_date , session = session ) remaining_dates = ( ti_status . total_runs - len ( ti_status . executed_dag_run_dates ) ) err = self . _collect_errors ( ti_status = ti_status , session = session ) if err : raise AirflowException ( err ) if remaining_dates > 0 : self . log . info ( \"max_active_runs limit for dag %s has been reached \" \" - waiting for other dag runs to finish\" , self . dag_id ) time . sleep ( self . delay_on_limit_secs ) except ( KeyboardInterrupt , SystemExit ) : self  . log . warning ( \"Backfill terminated by user.\" ) self . _set_unfinished_dag_runs_to_failed ( ti_status . active_runs ) finally : session . commit ( ) executor . end ( ) self . log . info ( \"Backfill done. Exiting.\" )"}
{"code_tokens": "def heartbeat_callback ( self , session = None ) : if self . terminating : self . task_runner . terminate ( ) return self . task_instance . refresh_from_db ( ) ti = self . task_instance fqdn = get_hostname ( ) same_hostname = fqdn == ti . hostname same_process = ti . pid == os . getpid ( ) if ti . state == State . RUNNING : if not same_hostname : self . log . warning ( \"The recorded hostname %s \" \"does not match this instance's hostname \" \"%s\" , ti . hostname , fqdn ) raise AirflowException ( \"Hostname of job runner does not match\" ) elif not same_process : current_pid = os . getpid ( ) self . log . warning ( \"Recorded pid %s does not match \" \"the current pid %s\" , ti . pid , current_pid ) raise AirflowException ( \"PID of  job runner does not match\" ) elif ( self . task_runner . return_code ( ) is None and hasattr ( self . task_runner , 'process' ) ) : self . log . warning ( \"State of this instance has been externally set to %s. \" \"Taking the poison pill.\" , ti . state ) self . task_runner . terminate ( ) self . terminating = True"}
{"code_tokens": "def _get_client ( self , project_id ) : if not self . _client : self . _client = Client  ( project = project_id , credentials = self . _get_credentials ( ) ) return self . _client"}
{"code_tokens": "def get_instance ( self , instance_id , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance  ( instance_id = instance_id ) if not instance . exists ( ) : return None return instance"}
{"code_tokens": "def _apply_to_instance ( self , project_id , instance_id , configuration_name , node_count , display_name , func ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id , configuration_name = configuration_name , node_count = node_count , display_name = display_name ) try : operation = func ( instance ) except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e if operation : result = operation . result ( ) self . log .  info ( result )"}
{"code_tokens": "def create_instance ( self , instance_id , configuration_name , node_count , display_name , project_id = None ) : self . _apply_to_instance ( project_id , instance_id , configuration_name ,  node_count , display_name , lambda x : x . create ( ) )"}
{"code_tokens": "def update_instance ( self , instance_id , configuration_name , node_count , display_name , project_id = None ) : return self . _apply_to_instance ( project_id , instance_id , configuration_name , node_count , display_name , lambda x : x . update (  ) )"}
{"code_tokens": "def delete_instance ( self , instance_id , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id ) try : instance . delete ( ) return except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message )  raise e"}
{"code_tokens": "def get_database ( self , instance_id , database_id , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) if not instance . exists ( ) : raise AirflowException ( \"The instance {} does not exist in project {} !\" . format ( instance_id , project_id ) ) database = instance . database ( database_id = database_id ) if not database . exists ( ) : return None else :  return database"}
{"code_tokens": "def create_database ( self , instance_id , database_id , ddl_statements , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) if not instance . exists ( ) : raise AirflowException ( \"The instance {} does not exist in project {} !\" . format ( instance_id , project_id ) ) database = instance . database  ( database_id = database_id , ddl_statements = ddl_statements ) try : operation = database . create ( ) except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e if operation : result = operation . result ( ) self . log . info ( result ) return"}
{"code_tokens": "def update_database ( self , instance_id , database_id , ddl_statements , project_id = None , operation_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) if not instance . exists ( ) : raise AirflowException ( \"The instance {} does not exist in project {} !\" . format ( instance_id , project_id ) ) database = instance . database ( database_id = database_id ) try : operation = database . update_ddl ( ddl_statements = ddl_statements ,  operation_id = operation_id ) if operation : result = operation . result ( ) self . log . info ( result ) return except AlreadyExists as e : if e . code == 409 and operation_id in e . message : self . log . info ( \"Replayed update_ddl message - the operation id %s \" \"was already done before.\" , operation_id ) return except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e"}
{"code_tokens": "def delete_database ( self , instance_id , database_id , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) if not instance . exists ( ) : raise AirflowException ( \"The instance {} does not exist in project {} !\" . format ( instance_id , project_id ) ) database = instance . database ( database_id = database_id ) if not database . exists ( ) : self . log . info ( \"The database {} is already deleted from instance {}. \" \"Exiting.\" . format ( database_id , instance_id ) ) return try : operation = database . drop ( ) except GoogleAPICallError as e  : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e if operation : result = operation . result ( ) self . log . info ( result ) return"}
{"code_tokens": "def poke ( self , context ) : self . log . info ( 'Poking for %s' , self . attachment_name ) with ImapHook ( imap_conn_id = self . conn_id ) as imap_hook : return imap_hook . has_mail_attachment ( name = self . attachment_name , mail_folder = self  . mail_folder , check_regex = self . check_regex )"}
{"code_tokens": "def prepare_additional_parameters ( additional_properties , language_hints , web_detection_params ) : if language_hints is None and web_detection_params is None : return additional_properties if additional_properties is None : return { } merged_additional_parameters = deepcopy ( additional_properties ) if 'image_context' not in merged_additional_parameters : merged_additional_parameters [ 'image_context' ] = { }  merged_additional_parameters [ 'image_context' ] [ 'language_hints' ] = merged_additional_parameters [ 'image_context' ] . get ( 'language_hints' , language_hints ) merged_additional_parameters [ 'image_context' ] [ 'web_detection_params' ] = merged_additional_parameters [ 'image_context' ] . get ( 'web_detection_params' , web_detection_params ) return merged_additional_parameters"}
{"code_tokens": "def get_conn ( self ) : if self . session and not self . session . is_shutdown : return self . session self .  session = self . cluster . connect ( self . keyspace ) return self . session"}
{"code_tokens": "def table_exists ( self , table ) : keyspace = self . keyspace if '.' in table : keyspace , table = table . split ( '.' , 1 ) cluster_metadata = self . get_conn ( ) . cluster . metadata return ( keyspace in cluster_metadata . keyspaces and table in cluster_metadata . keyspaces [  keyspace ] . tables )"}
{"code_tokens": "def record_exists ( self , table , keys ) : keyspace = self . keyspace if '.' in table : keyspace , table = table . split ( '.' , 1 ) ks = \" AND \" . join ( \"{}=%({})s\" . format ( key , key ) for key in keys . keys ( ) ) cql = \"SELECT * FROM {keyspace}.{table} WHERE {keys}\" . format ( keyspace = keyspace , table = table , keys = ks ) try : rs = self . get_conn ( ) . execute ( cql , keys ) return rs . one ( ) is not None except Exception  : return False"}
{"code_tokens": "def _build_track_driver_status_command ( self ) : connection_cmd = self . _get_spark_binary_path ( ) connection_cmd += [ \"--master\" , self . _connection [ 'master' ] ] if self . _driver_id : connection_cmd += [ \"--status\" , self . _driver_id ] else : raise AirflowException  ( \"Invalid status: attempted to poll driver \" + \"status but no driver id is known. Giving up.\" ) self . log . debug ( \"Poll driver status cmd: %s\" , connection_cmd ) return connection_cmd"}
{"code_tokens": "def submit ( self , application = \"\" , ** kwargs ) : spark_submit_cmd = self . _build_spark_submit_command ( application ) if hasattr ( self , '_env' ) : env = os . environ . copy ( ) env . update ( self . _env ) kwargs [ \"env\" ] = env self . _submit_sp = subprocess . Popen ( spark_submit_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , bufsize = - 1 , universal_newlines = True , ** kwargs ) self . _process_spark_submit_log ( iter ( self . _submit_sp . stdout . readline , '' ) ) returncode = self . _submit_sp . wait ( ) if returncode or ( self . _is_kubernetes and self . _spark_exit_code != 0 ) : raise AirflowException ( \"Cannot execute: {}. Error code is: {}.\" . format ( spark_submit_cmd , returncode ) ) self . log . debug ( \"Should track driver: {}\" . format ( self . _should_track_driver_status ) ) if self . _should_track_driver_status : if self . _driver_id is None : raise AirflowException ( \"No driver id is known: something went wrong when executing  \" + \"the spark submit command\" ) self . _driver_status = \"SUBMITTED\" self . _start_driver_status_tracking ( ) if self . _driver_status != \"FINISHED\" : raise AirflowException ( \"ERROR : Driver {} badly exited with status {}\" . format ( self . _driver_id , self . _driver_status ) )"}
{"code_tokens": "def _process_spark_submit_log ( self , itr ) : for line in itr : line = line . strip ( ) if self . _is_yarn and self . _connection [ 'deploy_mode' ] == 'cluster' : match = re . search ( '(application[0-9_]+)' , line ) if match : self . _yarn_application_id = match . groups ( ) [ 0 ] self . log . info ( \"Identified spark driver id: %s\" , self . _yarn_application_id ) elif self . _is_kubernetes : match = re . search ( r'\\s*pod name: ((.+?)-([a-z0-9]+)-driver)' , line ) if match : self . _kubernetes_driver_pod = match . groups ( ) [ 0 ] self . log . info ( \"Identified spark driver pod: %s\" , self . _kubernetes_driver_pod ) match_exit_code = re . search ( r'\\s*Exit code: (\\d+)' , line ) if match_exit_code : self . _spark_exit_code = int ( match_exit_code . groups ( ) [ 0 ] ) elif self . _should_track_driver_status and not self . _driver_id : match_driver_id = re . search ( r'(driver-[0-9\\-]+)' , line ) if match_driver_id : self . _driver_id = match_driver_id . groups ( ) [ 0 ] self . log  . info ( \"identified spark driver id: {}\" . format ( self . _driver_id ) ) else : self . log . info ( line ) self . log . debug ( \"spark submit log: {}\" . format ( line ) )"}
{"code_tokens": "def _process_spark_status_log ( self , itr ) : for line in itr : line = line . strip ( ) if \"driverState\" in line : self . _driver_status = line . split ( ' : ' ) [  1 ] . replace ( ',' , '' ) . replace ( '\\\"' , '' ) . strip ( ) self . log . debug ( \"spark driver status log: {}\" . format ( line ) )"}
{"code_tokens": "def get_task_runner ( local_task_job ) : if _TASK_RUNNER == \"StandardTaskRunner\" : return StandardTaskRunner ( local_task_job ) elif _TASK_RUNNER == \"CgroupTaskRunner\" : from airflow . contrib . task_runner . cgroup_task_runner import CgroupTaskRunner return CgroupTaskRunner ( local_task_job ) else : raise AirflowException ( \"Unknown  task runner type {}\" . format ( _TASK_RUNNER ) )"}
{"code_tokens": "def _wait_for_task_ended ( self ) : try : waiter = self . client . get_waiter ( 'job_execution_complete' ) waiter . config . max_attempts = sys . maxsize waiter . wait ( jobs = [ self . jobId ] ) except ValueError : retry = True retries = 0 while retries < self . max_retries and retry : self . log . info ( 'AWS Batch retry in the next %s seconds' , retries ) response = self . client . describe_jobs ( jobs = [ self . jobId ] ) if response [ 'jobs' ] [ - 1 ] [ 'status' ] in [ 'SUCCEEDED' ,  'FAILED' ] : retry = False sleep ( 1 + pow ( retries * 0.1 , 2 ) ) retries += 1"}
{"code_tokens": "def _query_mysql ( self ) : mysql = MySqlHook ( mysql_conn_id = self . mysql_conn_id ) conn = mysql . get_conn ( )  cursor = conn . cursor ( ) cursor . execute ( self . sql ) return cursor"}
{"code_tokens": "def _configure_csv_file ( self , file_handle , schema ) : csv_writer = csv . writer ( file_handle , encoding = 'utf-8' , delimiter  = self . field_delimiter ) csv_writer . writerow ( schema ) return csv_writer"}
{"code_tokens": "def _write_local_schema_file ( self , cursor ) : schema_str = None schema_file_mime_type = 'application/json' tmp_schema_file_handle = NamedTemporaryFile ( delete = True ) if self . schema is not None and isinstance ( self . schema , string_types ) : schema_str = self . schema . encode ( 'utf-8' ) elif self . schema is not None and isinstance ( self . schema , list ) : schema_str = json . dumps ( self . schema ) . encode ( 'utf-8' ) else : schema = [ ] for field in cursor . description : field_name = field [ 0 ] field_type = self . type_map ( field [ 1 ] ) if field [ 6 ] or field_type == 'TIMESTAMP' : field_mode = 'NULLABLE' else : field_mode = 'REQUIRED' schema . append ( { 'name' : field_name , 'type' : field_type , 'mode' : field_mode , } ) schema_str = json . dumps ( schema , sort_keys =  True ) . encode ( 'utf-8' ) tmp_schema_file_handle . write ( schema_str ) self . log . info ( 'Using schema for %s: %s' , self . schema_filename , schema_str ) schema_file_to_upload = { 'file_name' : self . schema_filename , 'file_handle' : tmp_schema_file_handle , 'file_mime_type' : schema_file_mime_type } return schema_file_to_upload"}
{"code_tokens": "def _get_col_type_dict ( self ) : schema = [ ] if isinstance ( self . schema , string_types ) : schema = json . loads ( self . schema ) elif isinstance ( self . schema , list ) : schema = self . schema elif self . schema is not None : self . log . warn ( 'Using default schema due to unexpected type.' 'Should be a string or list.' ) col_type_dict = { } try : col_type_dict = { col [ 'name' ] : col [ 'type' ] for col in schema } except KeyError : self . log . warn ( 'Using default schema due to missing name or type. Please ' 'refer to: https://cloud.google.com/bigquery/docs/schemas'  '#specifying_a_json_schema_file' ) return col_type_dict"}
{"code_tokens": "def type_map ( cls , mysql_type ) : d = { FIELD_TYPE . INT24 : 'INTEGER' , FIELD_TYPE . TINY : 'INTEGER' , FIELD_TYPE . BIT : 'INTEGER' , FIELD_TYPE . DATETIME : 'TIMESTAMP' , FIELD_TYPE . DATE : 'TIMESTAMP' , FIELD_TYPE . DECIMAL : 'FLOAT' , FIELD_TYPE . NEWDECIMAL : 'FLOAT' , FIELD_TYPE . DOUBLE : 'FLOAT' , FIELD_TYPE . FLOAT : 'FLOAT' , FIELD_TYPE . LONG : 'INTEGER' , FIELD_TYPE . LONGLONG : 'INTEGER' , FIELD_TYPE .  SHORT : 'INTEGER' , FIELD_TYPE . TIMESTAMP : 'TIMESTAMP' , FIELD_TYPE . YEAR : 'INTEGER' , } return d [ mysql_type ] if mysql_type in d else 'STRING'"}
{"code_tokens": "def execute ( self , context ) : self . hook = SqoopHook ( conn_id = self . conn_id , verbose = self . verbose , num_mappers = self . num_mappers , hcatalog_database = self . hcatalog_database , hcatalog_table = self . hcatalog_table , properties = self . properties ) if self . cmd_type == 'export' : self . hook . export_table ( table = self . table , export_dir = self . export_dir , input_null_string = self . input_null_string , input_null_non_string = self . input_null_non_string , staging_table = self . staging_table , clear_staging_table = self . clear_staging_table , enclosed_by = self . enclosed_by , escaped_by = self . escaped_by , input_fields_terminated_by = self . input_fields_terminated_by , input_lines_terminated_by = self . input_lines_terminated_by , input_optionally_enclosed_by = self . input_optionally_enclosed_by , batch = self . batch , relaxed_isolation = self . relaxed_isolation , extra_export_options = self . extra_export_options ) elif self . cmd_type == 'import' : if self . create_hcatalog_table : self . extra_import_options [ 'create-hcatalog-table' ] = '' if self . table and self . query : raise AirflowException ( 'Cannot specify query and table together. Need to specify either or.' ) if self . table : self . hook . import_table ( table = self . table , target_dir = self . target_dir , append = self . append , file_type = self . file_type , columns = self . columns , split_by = self . split_by , where = self . where , direct = self . direct , driver = self  . driver , extra_import_options = self . extra_import_options ) elif self . query : self . hook . import_query ( query = self . query , target_dir = self . target_dir , append = self . append , file_type = self . file_type , split_by = self . split_by , direct = self . direct , driver = self . driver , extra_import_options = self . extra_import_options ) else : raise AirflowException ( \"Provide query or table parameter to import using Sqoop\" ) else : raise AirflowException ( \"cmd_type should be 'import' or 'export'\" )"}
{"code_tokens": "def apply_lineage ( func ) : backend = _get_backend ( ) @ wraps ( func ) def wrapper ( self , context , * args , ** kwargs ) : self . log . debug ( \"Backend: %s, Lineage called with inlets: %s, outlets: %s\" , backend , self . inlets , self . outlets ) ret_val = func ( self , context , * args , ** kwargs ) outlets = [ x . as_dict ( ) for x in self . outlets ] inlets = [ x . as_dict ( ) for x in self . inlets ] if len ( self . outlets ) > 0 : self . xcom_push ( context , key = PIPELINE_OUTLETS , value = outlets , execution_date = context [ 'ti' ] . execution_date ) if len ( self . inlets ) > 0 : self . xcom_push ( context , key = PIPELINE_INLETS , value = inlets , execution_date = context [ 'ti' ] . execution_date ) if backend : backend . send_lineage ( operator = self , inlets  = self . inlets , outlets = self . outlets , context = context ) return ret_val return wrapper"}
{"code_tokens": "def extra_dejson ( self ) : obj = { } if self . extra : try : obj = json . loads ( self . extra ) except Exception as e : self . log . exception ( e ) self . log . error  ( \"Failed parsing the json for conn_id %s\" , self . conn_id ) return obj"}
{"code_tokens": "def date_range ( start_date , end_date = None , num = None , delta = None ) : if not delta : return [ ] if end_date and start_date > end_date : raise Exception ( \"Wait. start_date needs to be before end_date\" ) if end_date and num : raise Exception ( \"Wait. Either specify end_date OR num\" ) if not end_date and not num : end_date = timezone . utcnow ( ) delta_iscron = False tz = start_date . tzinfo if isinstance ( delta , six . string_types ) : delta_iscron = True start_date = timezone . make_naive ( start_date , tz ) cron = croniter ( delta , start_date ) elif isinstance ( delta , timedelta ) : delta = abs ( delta ) dates = [ ] if end_date : if timezone . is_naive ( start_date ) : end_date = timezone . make_naive ( end_date , tz ) while start_date <= end_date : if timezone . is_naive ( start_date ) : dates . append ( timezone . make_aware ( start_date , tz ) ) else : dates . append ( start_date ) if delta_iscron : start_date = cron . get_next ( datetime ) else : start_date += delta else : for _ in range ( abs ( num ) ) : if timezone . is_naive ( start_date ) : dates . append ( timezone . make_aware ( start_date , tz ) ) else : dates . append ( start_date  ) if delta_iscron : if num > 0 : start_date = cron . get_next ( datetime ) else : start_date = cron . get_prev ( datetime ) else : if num > 0 : start_date += delta else : start_date -= delta return sorted ( dates )"}
{"code_tokens": "def scale_time_units ( time_seconds_arr , unit ) : if unit == 'minutes' : return list ( map ( lambda x : x * 1.0 / 60 , time_seconds_arr ) ) elif unit == 'hours' : return list ( map ( lambda x : x * 1.0 / ( 60 * 60 ) , time_seconds_arr ) ) elif unit == 'days' : return list ( map ( lambda x : x * 1.0 / ( 24 *  60 * 60 ) , time_seconds_arr ) ) return time_seconds_arr"}
{"code_tokens": "def days_ago ( n , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) : today = timezone . utcnow ( ) . replace ( hour = hour , minute = minute , second =  second , microsecond = microsecond ) return today - timedelta ( days = n )"}
{"code_tokens": "def init_role ( self , role_name , role_vms , role_perms ) : pvms = self . get_session . query ( sqla_models . PermissionView ) . all ( ) pvms = [ p for p in pvms if p . permission and p . view_menu ] role = self . find_role ( role_name ) if not role : role = self . add_role ( role_name ) if len ( role . permissions ) == 0 : self . log . info ( 'Initializing permissions for role:%s in the database.' , role_name ) role_pvms = set ( ) for pvm in pvms : if pvm . view_menu . name in role_vms and pvm . permission . name in role_perms : role_pvms . add ( pvm ) role . permissions = list ( role_pvms ) self . get_session . merge ( role ) self . get_session . commit (  ) else : self . log . debug ( 'Existing permissions for the role:%s ' 'within the database will persist.' , role_name )"}
{"code_tokens": "def delete_role ( self , role_name ) : session = self . get_session role = session . query ( sqla_models . Role ) . filter ( sqla_models . Role . name == role_name ) . first ( ) if role : self . log . info ( \"Deleting role '%s'\" , role_name  ) session . delete ( role ) session . commit ( ) else : raise AirflowException ( \"Role named '{}' does not exist\" . format ( role_name ) )"}
{"code_tokens": "def get_user_roles ( self , user = None ) : if user is None : user = g . user if user . is_anonymous : public_role = appbuilder . config . get ( 'AUTH_ROLE_PUBLIC' ) return [ appbuilder . security_manager . find_role ( public_role ) ]  if public_role else [ ] return user . roles"}
{"code_tokens": "def get_all_permissions_views ( self ) : perms_views = set ( ) for role in self . get_user_roles ( ) : perms_views . update ( { ( perm_view . permission . name , perm_view  . view_menu . name ) for perm_view in role . permissions } ) return perms_views"}
{"code_tokens": "def _has_role ( self , role_name_or_list ) : if not isinstance ( role_name_or_list , list ) : role_name_or_list = [ role_name_or_list ] return any ( [ r . name in role_name_or_list  for r in self . get_user_roles ( ) ] )"}
{"code_tokens": "def _has_perm ( self , permission_name , view_menu_name ) : if hasattr ( self , 'perms' ) : if ( permission_name , view_menu_name ) in self . perms : return True self . _get_and_cache_perms ( ) return ( permission_name , view_menu_name ) in  self . perms"}
{"code_tokens": "def clean_perms ( self ) : self . log . debug ( 'Cleaning faulty perms' ) sesh = self . get_session pvms = ( sesh . query ( sqla_models . PermissionView ) . filter ( or_ ( sqla_models . PermissionView . permission == None , sqla_models . PermissionView . view_menu == None  , ) ) ) deleted_count = pvms . delete ( ) sesh . commit ( ) if deleted_count : self . log . info ( 'Deleted %s faulty permissions' , deleted_count )"}
{"code_tokens": "def _merge_perm ( self , permission_name , view_menu_name ) : permission = self . find_permission ( permission_name ) view_menu = self . find_view_menu ( view_menu_name ) pv = None if permission and view_menu : pv = self . get_session . query ( self . permissionview_model ) . filter_by ( permission = permission , view_menu = view_menu ) . first ( ) if not pv and permission_name and view_menu_name : self . add_permission_view_menu ( permission_name , view_menu_name  )"}
{"code_tokens": "def update_admin_perm_view ( self ) : pvms = self . get_session . query ( sqla_models . PermissionView ) . all ( ) pvms = [ p for p in pvms if p . permission and p . view_menu ] admin = self . find_role ( 'Admin' ) admin . permissions = list ( set ( admin . permissions ) | set ( pvms ) ) self . get_session . commit (  )"}
{"code_tokens": "def _sync_dag_view_permissions ( self , dag_id , access_control ) : def _get_or_create_dag_permission ( perm_name ) : dag_perm = self . find_permission_view_menu ( perm_name , dag_id ) if not dag_perm : self . log . info ( \"Creating new permission '%s' on view '%s'\" , perm_name , dag_id ) dag_perm = self . add_permission_view_menu ( perm_name , dag_id ) return dag_perm def _revoke_stale_permissions ( dag_view ) : existing_dag_perms = self . find_permissions_view_menu ( dag_view ) for perm in existing_dag_perms : non_admin_roles = [ role for role in perm . role if role . name != 'Admin' ] for role in non_admin_roles : target_perms_for_role = access_control . get ( role . name , { } ) if perm . permission . name not in target_perms_for_role : self . log . info ( \"Revoking '%s' on DAG '%s' for role '%s'\" , perm . permission , dag_id , role . name ) self . del_permission_role ( role , perm ) dag_view = self . find_view_menu ( dag_id ) if dag_view : _revoke_stale_permissions ( dag_view ) for rolename , perms in access_control . items ( ) : role = self . find_role ( rolename ) if not role : raise AirflowException ( \"The access_control mapping for DAG '{}' includes a role \" \"named '{}', but that role does not exist\" . format ( dag_id , rolename ) ) perms = set ( perms ) invalid_perms = perms - self . DAG_PERMS if invalid_perms : raise AirflowException ( \"The access_control map for DAG '{}' includes the following \" \"invalid permissions: {};  The set of valid permissions \" \"is: {}\" . format ( dag_id , ( perms - self . DAG_PERMS ) , self . DAG_PERMS ) ) for perm_name in perms : dag_perm = _get_or_create_dag_permission ( perm_name ) self . add_permission_role ( role , dag_perm )"}
{"code_tokens": "def create_perm_vm_for_all_dag ( self ) : for dag_vm in self . DAG_VMS : for perm in self . DAG_PERMS : self . _merge_perm ( permission_name = perm ,  view_menu_name = dag_vm )"}
{"code_tokens": "def get_fernet ( ) : global _fernet log = LoggingMixin ( ) . log if _fernet : return _fernet try : from cryptography . fernet import Fernet , MultiFernet , InvalidToken global InvalidFernetToken InvalidFernetToken = InvalidToken except BuiltinImportError : log . warning ( \"cryptography not found - values will not be stored encrypted.\" ) _fernet = NullFernet ( ) return _fernet try : fernet_key = configuration . conf . get ( 'core' , 'FERNET_KEY' ) if not fernet_key : log . warning ( \"empty cryptography key - values will not be stored encrypted.\" ) _fernet = NullFernet ( ) else : _fernet = MultiFernet ( [ Fernet ( fernet_part . encode ( 'utf-8' ) ) for fernet_part in fernet_key . split ( ',' ) ] ) _fernet . is_encrypted = True except (  ValueError , TypeError ) as ve : raise AirflowException ( \"Could not create Fernet object: {}\" . format ( ve ) ) return _fernet"}
{"code_tokens": "def poke ( self , context ) : if '.' in self . table_name : self . database_name , self . table_name = self . table_name . split ( '.' ) self . log . info ( 'Poking for table %s. %s,  expression %s' , self . database_name , self . table_name , self . expression ) return self . get_hook ( ) . check_for_partition ( self . database_name , self . table_name , self . expression )"}
{"code_tokens": "def get_hook ( self ) : if not hasattr ( self , 'hook' ) : from airflow . contrib . hooks . aws_glue_catalog_hook import AwsGlueCatalogHook self . hook = AwsGlueCatalogHook ( aws_conn_id  = self . aws_conn_id , region_name = self . region_name ) return self . hook"}
{"code_tokens": "def poke ( self , context ) : sqs_hook = SQSHook ( aws_conn_id = self . aws_conn_id ) sqs_conn = sqs_hook . get_conn ( ) self . log . info ( 'SQSSensor checking for message on queue: %s' , self . sqs_queue ) messages = sqs_conn . receive_message ( QueueUrl = self . sqs_queue , MaxNumberOfMessages = self . max_messages , WaitTimeSeconds = self . wait_time_seconds ) self . log . info ( \"reveived message %s\" , str ( messages ) ) if 'Messages' in messages and len ( messages [ 'Messages' ] ) > 0 : entries = [ { 'Id' : message [ 'MessageId' ] , 'ReceiptHandle' : message [ 'ReceiptHandle' ] } for message in messages [ 'Messages' ] ] result = sqs_conn . delete_message_batch ( QueueUrl  = self . sqs_queue , Entries = entries ) if 'Successful' in result : context [ 'ti' ] . xcom_push ( key = 'messages' , value = messages ) return True else : raise AirflowException ( 'Delete SQS Messages failed ' + str ( result ) + ' for messages ' + str ( messages ) ) return False"}
{"code_tokens": "def get_conn ( self ) : effective_user = self . proxy_user autoconfig = self . autoconfig use_sasl = configuration . conf . get ( 'core' , 'security' ) == 'kerberos' try : connections = self . get_connections ( self . hdfs_conn_id ) if not effective_user : effective_user = connections [ 0 ] . login if not autoconfig : autoconfig = connections [ 0 ] . extra_dejson . get ( 'autoconfig' , False ) hdfs_namenode_principal = connections [ 0 ] . extra_dejson . get ( 'hdfs_namenode_principal' ) except AirflowException : if not autoconfig : raise if autoconfig : client = AutoConfigClient ( effective_user = effective_user , use_sasl = use_sasl ) elif len ( connections ) == 1 : client = Client ( connections [ 0 ] . host , connections [ 0 ] . port , effective_user = effective_user , use_sasl = use_sasl  , hdfs_namenode_principal = hdfs_namenode_principal ) elif len ( connections ) > 1 : nn = [ Namenode ( conn . host , conn . port ) for conn in connections ] client = HAClient ( nn , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) else : raise HDFSHookException ( \"conn_id doesn't exist in the repository \" \"and autoconfig is not specified\" ) return client"}
{"code_tokens": "def get_conn ( self ) : connections = self . get_connections ( self . webhdfs_conn_id ) for connection in connections : try : self . log . debug ( 'Trying namenode %s' , connection . host ) client = self . _get_client ( connection ) client . status ( '/' ) self . log . debug ( 'Using namenode %s for hook' , connection . host ) return client except HdfsError as hdfs_error : self . log . debug ( 'Read operation on namenode %s failed with error: %s' , connection . host , hdfs_error  ) hosts = [ connection . host for connection in connections ] error_message = 'Read operations failed on the namenodes below:\\n{hosts}' . format ( hosts = '\\n' . join ( hosts ) ) raise AirflowWebHDFSHookException ( error_message )"}
{"code_tokens": "def check_for_path ( self , hdfs_path ) : conn = self . get_conn ( ) status = conn . status ( hdfs_path ,  strict = False ) return bool ( status )"}
{"code_tokens": "def load_file ( self , source , destination , overwrite = True , parallelism = 1 , ** kwargs ) : r conn = self . get_conn ( ) conn . upload ( hdfs_path = destination , local_path = source , overwrite = overwrite , n_threads = parallelism , ** kwargs ) self . log . debug  ( \"Uploaded file %s to %s\" , source , destination )"}
{"code_tokens": "def get_conn ( self ) : conn = self . get_connection ( self . pinot_broker_conn_id ) pinot_broker_conn = connect ( host = conn . host , port = conn . port , path = conn . extra_dejson . get ( 'endpoint' , '/pql' ) , scheme = conn . extra_dejson . get ( 'schema' , 'http' ) ) self . log . info  ( 'Get the connection to pinot ' 'broker on {host}' . format ( host = conn . host ) ) return pinot_broker_conn"}
{"code_tokens": "def get_uri ( self ) : conn = self . get_connection ( getattr ( self , self . conn_name_attr ) ) host = conn . host if conn . port is not None : host += ':{port}' . format ( port = conn . port ) conn_type = 'http'  if not conn . conn_type else conn . conn_type endpoint = conn . extra_dejson . get ( 'endpoint' , 'pql' ) return '{conn_type}://{host}/{endpoint}' . format ( conn_type = conn_type , host = host , endpoint = endpoint )"}
{"code_tokens": "def _convert_date_to_dict ( field_date ) : return { DAY : field_date . day , MONTH : field_date . month , YEAR : field_date . year } "}
{"code_tokens": "def split_phylogeny ( p , level = \"s\" ) : level = level + \"__\" result = p . split ( level ) return result [ 0  ] + level + result [ 1 ] . split ( \";\" ) [ 0 ]"}
{"code_tokens": "def ensure_dir ( d ) : if not os . path . exists ( d ) : try : os . makedirs ( d ) except OSError as oe : if os . errno == errno . ENOENT : msg = twdd ( ) return msg  . format ( d ) else : msg = twdd ( ) return msg . format ( d , oe . strerror )"}
{"code_tokens": "def file_handle ( fnh , mode = \"rU\" ) : handle = None if isinstance ( fnh , file ) : if fnh . closed : raise ValueError (  \"Input file is closed.\" ) handle = fnh elif isinstance ( fnh , str ) : handle = open ( fnh , mode ) return handle"}
{"code_tokens": "def gather_categories ( imap , header , categories = None ) : if categories is None : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } cat_ids = [ header . index ( cat ) for cat in categories if cat in header and \"=\" not in cat ] table = OrderedDict ( ) conditions = defaultdict ( set ) for i , cat in enumerate ( categories ) : if \"=\" in cat and cat . split ( \"=\" ) [ 0 ] in header : cat_name = header [ header . index ( cat . split ( \"=\" ) [ 0 ] ) ] conditions [ cat_name ] . add ( cat . split ( \"=\" ) [ 1 ] ) if not cat_ids and not conditions : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } if cat_ids and not conditions : for sid , row in imap . items ( ) : cat_name = \"_\" . join ( [ row [ cid ] for cid in cat_ids ] ) if cat_name not in table : table [ cat_name ] = DataCategory ( set ( ) , { } ) table [ cat_name ] . sids . add ( sid ) return table cond_ids = set ( ) for k in conditions : try : cond_ids . add ( header . index ( k ) ) except ValueError : continue idx_to_test = set ( cat_ids ) .  union ( cond_ids ) for sid , row in imap . items ( ) : if all ( [ row [ header . index ( c ) ] in conditions [ c ] for c in conditions ] ) : key = \"_\" . join ( [ row [ idx ] for idx in idx_to_test ] ) try : assert key in table . keys ( ) except AssertionError : table [ key ] = DataCategory ( set ( ) , { } ) table [ key ] . sids . add ( sid ) try : assert len ( table ) > 0 except AssertionError : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } else : return table"}
{"code_tokens": "def parse_unifrac ( unifracFN ) : with open ( unifracFN , \"rU\" ) as uF : first = uF . next ( ) . split ( \"\\t\" ) lines = [ line . strip ( ) for line in uF ] unifrac = { \"pcd\" : OrderedDict ( ) , \"eigvals\" : [ ] , \"varexp\" : [ ] } if first [ 0 ] == \"pc vector number\" : return parse_unifrac_v1_8 ( unifrac , lines ) elif first [ 0 ] == \"Eigvals\" : return parse_unifrac_v1_9 ( unifrac , lines ) else : raise ValueError ( \"File format  not supported/recognized. Please check input \" \"unifrac file.\" )"}
{"code_tokens": "def parse_unifrac_v1_8 ( unifrac , file_data ) : for line in file_data : if line == \"\" : break line = line . split ( \"\\t\" ) unifrac [ \"pcd\" ] [ line [ 0 ] ] = [ float ( e ) for e in line [ 1 : ] ] unifrac [ \"eigvals\" ] = [ float ( entry ) for entry in file_data [  - 2 ] . split ( \"\\t\" ) [ 1 : ] ] unifrac [ \"varexp\" ] = [ float ( entry ) for entry in file_data [ - 1 ] . split ( \"\\t\" ) [ 1 : ] ] return unifrac"}
{"code_tokens": "def parse_unifrac_v1_9 ( unifrac , file_data ) : unifrac [ \"eigvals\" ] = [ float ( entry ) for entry in file_data [ 0 ] . split ( \"\\t\" ) ] unifrac [ \"varexp\" ] = [ float ( entry ) * 100 for entry in file_data [ 3 ] . split ( \"\\t\" ) ] for line in file_data [ 8 : ] : if line == \"\" : break line = line . split ( \"\\t\" ) unifrac [ \"pcd\" ] [ line [ 0 ] ] = [ float ( e ) for e in line [ 1 : ] ] return unifrac "}
{"code_tokens": "def color_mapping ( sample_map , header , group_column , color_column = None ) : group_colors = OrderedDict ( ) group_gather = gather_categories ( sample_map , header , [ group_column ] ) if color_column is not None : color_gather = gather_categories ( sample_map , header , [ color_column ] ) for group in group_gather : for color in color_gather : if group_gather [ group ] . sids . intersection ( color_gather [ color ] . sids ) : group_colors [ group ] = color else : bcolors = itertools . cycle ( Set3_12 . hex_colors ) for group in group_gather : group_colors [ group ] = bcolors . next (  ) return group_colors"}
{"code_tokens": "def rev_c ( read ) : rc = [ ] rc_nucs = { 'A' : 'T' , 'T' : 'A' , 'G' : 'C' , 'C' : 'G' , 'N' : 'N' } for base in read : rc  . extend ( rc_nucs [ base . upper ( ) ] ) return rc [ : : - 1 ]"}
{"code_tokens": "def shuffle_genome ( genome , cat , fraction = float ( 100 ) , plot = True , alpha = 0.1 , beta = 100000 , min_length = 1000 , max_length = 200000 ) : header = '>randomized_%s' % ( genome . name ) sequence = list ( '' . join ( [ i [ 1 ] for i in parse_fasta ( genome ) ] ) ) length = len ( sequence ) shuffled = [ ] while sequence is not False : s = int ( random . gammavariate ( alpha , beta ) ) if s <= min_length or s >= max_length : continue if len ( sequence ) < s : seq = sequence [ 0 : ] else : seq = sequence [ 0 : s ] sequence = sequence [ s : ] shuffled . append ( '' . join ( seq ) ) if sequence == [ ] : break random . shuffle ( shuffled ) if fraction == float ( 100 ) : subset = shuffled else : max_pieces = int ( length * fraction / 100 ) subset , total = [ ] , 0 for fragment in shuffled : length = len ( fragment ) if total + length <= max_pieces : subset . append ( fragment ) total += length else : diff = max_pieces - total subset . append ( fragment [ 0 : diff ] ) break if cat is True : yield [ header , '' . join ( subset  ) ] else : for i , seq in enumerate ( subset ) : yield [ '%s fragment:%s' % ( header , i ) , seq ]"}
{"code_tokens": "def _prune ( self , fit , p_max ) : def remove_from_model_desc ( x , model_desc ) : rhs_termlist = [ ] for t in model_desc . rhs_termlist : if not t . factors : rhs_termlist . append ( t ) elif not x == t . factors [ 0 ] . _varname : rhs_termlist . append ( t ) md = ModelDesc ( model_desc . lhs_termlist , rhs_termlist ) return md corrected_model_desc = ModelDesc ( fit . model . formula . lhs_termlist [ : ] , fit . model . formula . rhs_termlist [ : ] ) pars_to_prune = fit . pvalues . where ( fit . pvalues > p_max ) . dropna ( ) . index . tolist ( ) try : pars_to_prune . remove ( 'Intercept' ) except : pass while pars_to_prune : corrected_model_desc = remove_from_model_desc ( pars_to_prune [ 0 ] , corrected_model_desc ) fit = fm . ols ( corrected_model_desc  , data = self . df ) . fit ( ) pars_to_prune = fit . pvalues . where ( fit . pvalues > p_max ) . dropna ( ) . index . tolist ( ) try : pars_to_prune . remove ( 'Intercept' ) except : pass return fit"}
{"code_tokens": "def find_best_rsquared ( list_of_fits ) : res = sorted ( list_of_fits , key = lambda x : x . rsquared ) return res [ -  1 ]"}
{"code_tokens": "def _predict ( self , fit , df ) : df_res = df . copy ( ) if 'Intercept' in fit . model . exog_names : df_res [ 'Intercept' ] = 1.0 df_res [ 'predicted' ] = fit . predict ( df_res ) if not self . allow_negative_predictions : df_res . loc [ df_res [ 'predicted' ] < 0 , 'predicted' ] = 0 prstd , interval_l , interval_u = wls_prediction_std ( fit , df_res [ fit . model . exog_names ] , alpha = 1 - self . confint ) df_res [ 'interval_l' ] = interval_l df_res [ 'interval_u' ] = interval_u if 'Intercept' in df_res : df_res . drop ( labels = [ 'Intercept' ] ,  axis = 1 , inplace = True ) return df_res"}
{"code_tokens": "def relative_abundance ( biomf , sampleIDs = None ) : if sampleIDs is None : sampleIDs = biomf . ids ( ) else : try : for sid in sampleIDs : assert sid in biomf . ids ( ) except AssertionError : raise ValueError ( \"\\nError while calculating relative abundances: The sampleIDs provided do\" \" not match the sampleIDs in biom file. Please double check the sampleIDs\" \" provided.\\n\" ) otuIDs = biomf . ids ( axis = \"observation\" ) norm_biomf = biomf . norm ( inplace = False ) return { sample : { otuID : norm_biomf . get_value_by_ids (  otuID , sample ) for otuID in otuIDs } for sample in sampleIDs }"}
{"code_tokens": "def mean_otu_pct_abundance ( ra , otuIDs ) : sids = ra . keys ( ) otumeans = defaultdict ( int ) for oid in otuIDs : otumeans [ oid ] = sum ( [ ra [ sid ] [ oid ] for sid in sids  if oid in ra [ sid ] ] ) / len ( sids ) * 100 return otumeans"}
{"code_tokens": "def MRA ( biomf , sampleIDs = None , transform = None ) : ra = relative_abundance ( biomf , sampleIDs ) if transform is not None : ra = { sample : { otuID : transform ( abd ) for otuID , abd in ra [ sample ] . items ( ) } for sample in ra . keys ( ) } otuIDs = biomf . ids (  axis = \"observation\" ) return mean_otu_pct_abundance ( ra , otuIDs )"}
{"code_tokens": "def raw_abundance ( biomf , sampleIDs = None , sample_abd = True ) : results = defaultdict ( int ) if sampleIDs is None : sampleIDs = biomf . ids ( ) else : try : for sid in sampleIDs : assert sid in biomf . ids ( ) except AssertionError : raise ValueError ( \"\\nError while calculating raw total abundances: The sampleIDs provided \" \"do not match the sampleIDs in biom file. Please double check the \" \"sampleIDs provided.\\n\" ) otuIDs = biomf . ids ( axis =  \"observation\" ) for sampleID in sampleIDs : for otuID in otuIDs : abd = biomf . get_value_by_ids ( otuID , sampleID ) if sample_abd : results [ sampleID ] += abd else : results [ otuID ] += abd return results"}
{"code_tokens": "def transform_raw_abundance ( biomf , fn = math . log10 , sampleIDs = None , sample_abd = True ) : totals = raw_abundance ( biomf , sampleIDs , sample_abd ) return { sid : fn ( abd ) for sid , abd in totals . items (  ) }"}
{"code_tokens": "def print_MannWhitneyU ( div_calc ) : try : x = div_calc . values ( ) [ 0 ] . values ( ) y = div_calc . values ( ) [ 1 ] . values ( ) except : return \"Error setting up input arrays for Mann-Whitney U Test. Skipping \" \"significance testing.\" T , p = stats . mannwhitneyu ( x , y )  print \"\\nMann-Whitney U test statistic:\" , T print \"Two-tailed p-value: {}\" . format ( 2 * p )"}
{"code_tokens": "def print_KruskalWallisH ( div_calc ) : calc = defaultdict ( list ) try : for k1 , v1 in div_calc . iteritems ( ) : for k2 , v2 in v1 . iteritems ( ) : calc [ k1 ] . append ( v2 ) except : return \"Error setting up input arrays for Kruskal-Wallis H-Test. Skipping \" \"significance testing.\" h , p = stats . kruskal ( * calc . values ( ) ) print \"\\nKruskal-Wallis H-test statistic for {} groups: {}\" . format ( str ( len ( div_calc ) ) , h )  print \"p-value: {}\" . format ( p )"}
{"code_tokens": "def handle_program_options ( ) : parser = argparse . ArgumentParser ( description = \"Calculate the alpha diversity\\ of a set of samples using one or more \\ metrics and output a kernal density \\ estimator-smoothed histogram of the \\ results.\" ) parser . add_argument ( \"-m\" , \"--map_file\" , help = \"QIIME mapping file.\" ) parser . add_argument ( \"-i\" , \"--biom_fp\" , help = \"Path to the BIOM table\" ) parser . add_argument ( \"-c\" , \"--category\" , help = \"Specific category from the mapping file.\" ) parser . add_argument ( \"-d\" , \"--diversity\" , default = [ \"shannon\" ] , nargs = \"+\" , help = \"The alpha diversity metric. Default \\ value is 'shannon', which will calculate the Shannon\\ entropy. Multiple metrics can be specified (space separated).\\ The full list of metrics is available at:\\ http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\\ Beta diversity metrics will be supported in the future.\" ) parser . add_argument ( \"--x_label\" , default = [ None ] , nargs = \"+\" , help = \"The name of the diversity metric to be displayed on the\\ plot as the X-axis label. If multiple metrics are specified,\\ then multiple entries for the X-axis label should be given.\" ) parser . add_argument ( \"--color_by\" , help = \"A column name in the mapping file containing\\ hexadecimal (#FF0000) color values that will\\ be used to color the groups. Each sample ID must\\ have a color entry.\" ) parser . add_argument ( \"--plot_title\" , default = \"\" , help = \"A descriptive title that will appear at the top \\ of the output plot. Surround with quotes if there are\\ spaces in the title.\" ) parser . add_argument ( \"-o\" , \"--output_dir\" , default = \".\" , help = \"The directory plots will be saved to.\" ) parser . add_argument ( \"--image_type\" , default = \"png\" , help = \"The type of image to save: png, svg, pdf, eps, etc...\" ) parser . add_argument ( \"--save_calculations\" , help = \"Path and name of text file to store the calculated \" \"diversity metrics.\" ) parser . add_argument ( \"--suppress_stats\" , action = \"store_true\" , help = \"Do not display \" \"significance testing results which are shown by default.\" ) parser . add_argument ( \"--show_available_metrics\" , action  = \"store_true\" , help = \"Supply this parameter to see which alpha diversity metrics \" \" are available for usage. No calculations will be performed\" \" if this parameter is provided.\" ) return parser . parse_args ( )"}
{"code_tokens": "def blastdb ( fasta , maxfile = 10000000 ) : db = fasta . rsplit ( '.' , 1 ) [ 0 ] type = check_type ( fasta ) if type == 'nucl' : type = [ 'nhr' , type ] else : type = [ 'phr' , type ] if os . path . exists ( '%s.%s' % ( db , type [ 0 ] ) ) is False and os . path . exists ( '%s.00.%s' % ( db , type [ 0 ] ) ) is False : print ( '# ... making blastdb for: %s' % ( fasta ) , file = sys . stderr ) os . system ( 'makeblastdb \\  -in %s -out %s -dbtype %s -max_file_sz %s >> log.txt' % ( fasta , db , type [ 1 ] , maxfile ) ) else : print ( '# ... database found for: %s' % ( fasta ) , file = sys . stderr ) return db"}
{"code_tokens": "def usearchdb ( fasta , alignment = 'local' , usearch_loc = 'usearch' ) : if '.udb' in fasta : print ( '# ... database found: %s' % ( fasta ) , file = sys . stderr ) return fasta type = check_type ( fasta ) db = '%s.%s.udb' % ( fasta . rsplit ( '.' , 1 ) [ 0 ] , type ) if os . path . exists ( db ) is False : print ( '# ... making usearch db for: %s' % ( fasta ) , file = sys . stderr ) if alignment == 'local' : os . system ( '%s -makeudb_ublast %s -output %s >> log.txt' % ( usearch_loc , fasta , db ) ) elif alignment == 'global' : os . system ( '%s -makeudb_usearch %s -output  %s >> log.txt' % ( usearch_loc , fasta , db ) ) else : print ( '# ... database found for: %s' % ( fasta ) , file = sys . stderr ) return db"}
{"code_tokens": "def _pp ( dict_data ) : for key , val in dict_data . items ( )  : print ( '{0:<11}: {1}' . format ( key , val ) )"}
{"code_tokens": "def print_licences ( params , metadata ) : if hasattr ( params , 'licenses' ) : if params . licenses : _pp ( metadata . licenses_desc ( )  ) sys . exit ( 0 )"}
{"code_tokens": "def check_repository_existence ( params ) : repodir = os . path . join ( params . outdir , params . name ) if os . path . isdir (  repodir ) : raise Conflict ( 'Package repository \"{0}\" has already exists.' . format ( repodir ) )"}
{"code_tokens": "def generate_package ( params ) : pkg_data = package . PackageData ( params ) pkg_tree = package . PackageTree ( pkg_data ) pkg_tree . generate ( ) pkg_tree . move ( ) VCS ( os . path . join ( pkg_tree . outdir , pkg_tree . name ) , pkg_tree . pkg_data  )"}
{"code_tokens": "def print_single ( line , rev ) : if rev is True : seq = rc ( [ '' , line [ 9 ] ] ) [ 1 ] qual = line [ 10 ] [ : : - 1 ] else : seq = line [ 9 ] qual = line [ 10 ] fq = [ '@%s' % line [ 0 ] , seq , '+%s' % line  [ 0 ] , qual ] print ( '\\n' . join ( fq ) , file = sys . stderr )"}
{"code_tokens": "def sam2fastq ( sam , singles = False , force = False ) : L , R = None , None for line in sam : if line . startswith ( '@' ) is True : continue line = line . strip ( ) . split ( ) bit = [ True if i == '1' else False for i in bin ( int ( line [ 1 ] ) ) . split ( 'b' ) [ 1 ] [ : : - 1 ] ] while len ( bit ) < 8 : bit . append ( False ) pair , proper , na , nap , rev , mrev , left , right = bit if pair is False : if singles is True : print_single ( line , rev ) continue if rev is True : seq = rc ( [ '' , line [ 9 ] ] ) [ 1 ] qual = line [ 10 ] [ : : - 1 ] else : seq = line [ 9 ] qual = line [ 10 ] if left is True : if L is not None and force is False : print ( 'sam file is not sorted' , file = sys . stderr ) print ( '\\te.g.: %s' % ( line [ 0 ] ) , file = sys . stderr ) exit ( ) if L is not None : L = None continue L = [ '@%s' % line [ 0 ] , seq , '+%s' % line [ 0 ] , qual ] if R is not None : yield L yield R L , R = None , None if right is True : if R is not None and force is False : print ( 'sam file is not sorted' , file = sys . stderr ) print ( '\\te.g.: %s' % ( line [ 0 ]  ) , file = sys . stderr ) exit ( ) if R is not None : R = None continue R = [ '@%s' % line [ 0 ] , seq , '+%s' % line [ 0 ] , qual ] if L is not None : yield L yield R L , R = None , None"}
{"code_tokens": "def sort_sam ( sam , sort ) : tempdir = '%s/' % ( os . path . abspath ( sam ) . rsplit ( '/' , 1 ) [ 0 ] ) if sort is True : mapping = '%s.sorted.sam' % ( sam . rsplit ( '.' , 1 ) [ 0 ] ) if sam != '-' : if os . path . exists ( mapping ) is False : os . system ( \"\\ sort -k1 --buffer-size=%sG -T %s -o %s %s\\ \" % ( sbuffer , tempdir , mapping , sam ) ) else : mapping = 'stdin-sam.sorted.sam' p = Popen ( \"sort -k1 --buffer-size=%sG -T %s -o %s\" % ( sbuffer , tempdir , mapping ) ,  stdin = sys . stdin , shell = True ) p . communicate ( ) mapping = open ( mapping ) else : if sam == '-' : mapping = sys . stdin else : mapping = open ( sam ) return mapping"}
{"code_tokens": "def sub_sam ( sam , percent , sort = True , sbuffer = False ) : mapping = sort_sam ( sam , sort ) pool = [ 1 for i in range ( 0 , percent ) ] + [ 0 for i in range ( 0 , 100 - percent ) ] c = cycle ( [ 1 , 2 ] ) for line in mapping : line = line . strip ( ) . split ( ) if line [ 0 ] . startswith ( '@' ) : yield line continue if int ( line [ 1 ] ) <= 20  : if random . choice ( pool ) == 1 : yield line else : n = next ( c ) if n == 1 : prev = line if n == 2 and random . choice ( pool ) == 1 : yield prev yield line"}
{"code_tokens": "def fq2fa ( fq ) : c = cycle ( [ 1 , 2 , 3 , 4 ] ) for line in fq : n = next ( c ) if n == 1 : seq = [ '>%s' % ( line . strip ( ) . split ( '@' , 1 ) [ 1 ] ) ] if n == 2 : seq . append ( line . strip ( )  ) yield seq"}
{"code_tokens": "def change_return_type ( f ) : @ wraps ( f ) def wrapper ( * args , ** kwargs ) : if kwargs . has_key ( 'return_type' ) : return_type = kwargs [ 'return_type' ] kwargs . pop ( 'return_type' ) return return_type ( f ( * args , ** kwargs ) ) elif len ( args  ) > 0 : return_type = type ( args [ 0 ] ) return return_type ( f ( * args , ** kwargs ) ) else : return f ( * args , ** kwargs ) return wrapper"}
{"code_tokens": "def convert_args_to_sets ( f ) : @ wraps ( f ) def wrapper ( * args , ** kwargs ) : args = ( setify ( x )  for x in args ) return f ( * args , ** kwargs ) return wrapper"}
{"code_tokens": "def _init_entri ( self , laman ) : sup = BeautifulSoup ( laman . text , 'html.parser' ) estr = '' for label in sup . find ( 'hr' ) . next_siblings : if label . name == 'hr' : self . entri . append ( Entri ( estr ) ) break if label . name == 'h2' : if estr : self . entri . append ( Entri ( estr ) ) estr = '' estr += str (  label ) . strip ( )"}
{"code_tokens": "def _init_kata_dasar ( self , dasar ) : for tiap in dasar : kata = tiap . find ( 'a' ) dasar_no = kata . find ( 'sup' ) kata = ambil_teks_dalam_label ( kata  ) self . kata_dasar . append ( kata + ' [{}]' . format ( dasar_no . text . strip ( ) ) if dasar_no else kata )"}
{"code_tokens": "def serialisasi ( self ) : return { \"nama\" : self . nama , \"nomor\" : self . nomor , \"kata_dasar\" : self . kata_dasar , \"pelafalan\" : self . pelafalan , \"bentuk_tidak_baku\" : self . bentuk_tidak_baku , \"varian\"  : self . varian , \"makna\" : [ makna . serialisasi ( ) for makna in self . makna ] }"}
{"code_tokens": "def _makna ( self ) : if len ( self . makna ) > 1 : return '\\n' . join ( str ( i ) + \". \" + str ( makna ) for i , makna in enumerate ( self  . makna , 1 ) ) return str ( self . makna [ 0 ] )"}
{"code_tokens": "def _nama ( self ) : hasil = self . nama if self . nomor : hasil += \" [{}]\" . format ( self . nomor ) if self .  kata_dasar : hasil = \" » \". j oin( s elf. k ata_dasar) » \" + h sil return hasil"}
{"code_tokens": "def _varian ( self , varian ) : if varian == self . bentuk_tidak_baku : nama = \"Bentuk tidak baku\" elif varian == self . varian : nama  = \"Varian\" else : return '' return nama + ': ' + ', ' . join ( varian )"}
{"code_tokens": "def _init_kelas ( self , makna_label ) : kelas = makna_label . find ( color = 'red' ) lain = makna_label . find ( color = 'darkgreen' ) info = makna_label . find ( color = 'green' ) if kelas : kelas = kelas . find_all ( 'span' ) if lain : self . kelas = { lain . text . strip ( ) : lain [ 'title' ] . strip ( ) } self . submakna = lain . next_sibling . strip ( ) self . submakna += ' ' + makna_label . find ( color = 'grey' ) . text . strip ( ) else : self . kelas = { k . text . strip ( )  : k [ 'title' ] . strip ( ) for k in kelas } if kelas else { } self . info = info . text . strip ( ) if info else ''"}
{"code_tokens": "def _init_contoh ( self , makna_label ) : indeks = makna_label . text . find ( ': ' ) if indeks != - 1 : contoh = makna_label . text [ indeks + 2 : ] . strip ( ) self . contoh = contoh . split ( '; ' )  else : self . contoh = [ ]"}
{"code_tokens": "def serialisasi ( self ) : return { \"kelas\" : self . kelas , \"submakna\" : self . submakna , \"info\" : self .  info , \"contoh\" : self . contoh }"}
{"code_tokens": "def build_sphinx ( pkg_data , projectdir ) : try : version , _minor_version = pkg_data . version . rsplit ( '.' , 1 ) except ValueError : version = pkg_data . version args = ' ' . join ( ( 'sphinx-quickstart' , '--sep' , '-q' , '-p \"{name}\"' , '-a \"{author}\"' , '-v \"{version}\"' , '-r \"{release}\"' , '-l en' , '--suffix=.rst' , '--master=index' , '--ext-autodoc' , '--ext-viewcode' , '--makefile' , '{projectdir}' ) ) . format ( name = pkg_data . name , author = pkg_data . author , version = version , release =  pkg_data . version , projectdir = projectdir ) if subprocess . call ( shlex . split ( args ) ) == 0 : _touch_gitkeep ( projectdir )"}
{"code_tokens": "def bowtiedb ( fa , keepDB ) : btdir = '%s/bt2' % ( os . getcwd ( ) ) if not os . path . exists ( btdir ) : os . mkdir ( btdir ) btdb = '%s/%s' % ( btdir , fa . rsplit ( '/' , 1 ) [ - 1 ] ) if keepDB is True : if os . path . exists ( '%s.1.bt2' % ( btdb ) ) : return btdb p = subprocess . Popen ( 'bowtie2-build -q %s  %s' % ( fa , btdb ) , shell = True ) p . communicate ( ) return btdb"}
{"code_tokens": "def bowtie ( sam , btd , f , r , u , opt , no_shrink , threads ) : bt2 = 'bowtie2 -x %s -p %s ' % ( btd , threads ) if f is not False : bt2 += '-1 %s -2 %s ' % ( f , r ) if u is not False : bt2 += '-U %s ' % ( u ) bt2 += opt if no_shrink is False : if f is False : bt2 += ' | shrinksam -u -k %s-shrunk.sam ' % ( sam )  else : bt2 += ' | shrinksam -k %s-shrunk.sam ' % ( sam ) else : bt2 += ' > %s.sam' % ( sam ) return bt2"}
{"code_tokens": "def crossmap ( fas , reads , options , no_shrink , keepDB , threads , cluster , nodes ) : if cluster is True : threads = '48' btc = [ ] for fa in fas : btd = bowtiedb ( fa , keepDB ) F , R , U = reads if F is not False : if U is False : u = False for i , f in enumerate ( F ) : r = R [ i ] if U is not False : u = U [ i ] sam = '%s/%s-vs-%s' % ( os . getcwd ( ) , fa . rsplit ( '/' , 1 ) [ - 1 ] , f . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.' , 3 ) [ 0 ] ) btc . append ( bowtie ( sam , btd , f , r , u , options , no_shrink , threads ) ) else : f = False r = False for u in U : sam = '%s/%s-vs-%s' % ( os . getcwd ( ) , fa . rsplit ( '/' , 1 ) [ - 1 ] , u . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.' , 3 ) [ 0 ] ) btc . append ( bowtie ( sam , btd , f , r , u , options , no_shrink , threads ) ) if cluster is False : for i in btc : p = subprocess . Popen ( i , shell = True ) p . communicate ( ) else : ID = '' . join ( random . choice ( [ str ( i ) for i in range ( 0 , 9 ) ] ) for _ in range ( 5 ) ) for node , commands in enumerate ( chunks ( btc , nodes ) , 1 ) : bs = open ( '%s/crossmap-qsub.%s.%s.sh' % ( os . getcwd ( ) , ID , node ) , 'w' ) print ( '\\n' . join ( commands ) , file =  bs ) bs . close ( ) p = subprocess . Popen ( 'qsub -V -N crossmap %s' % ( bs . name ) , shell = True ) p . communicate ( )"}
{"code_tokens": "def get_conn ( self , * args , ** kwargs ) : connections = self . __connections_for ( 'get_conn' , args = args , kwargs = kwargs )  if len ( connections ) is 1 : return connections [ 0 ] else : return connections"}
{"code_tokens": "def __get_nondirect_init ( self , init ) : crc = init for i in range ( self . Width ) : bit = crc & 0x01 if bit : crc ^= self . Poly  crc >>= 1 if bit : crc |= self . MSB_Mask return crc & self . Mask"}
{"code_tokens": "def reflect ( self , data , width ) : x = data & 0x01 for i in range ( width - 1 ) : data >>=  1 x = ( x << 1 ) | ( data & 0x01 ) return x"}
{"code_tokens": "def bit_by_bit ( self , in_data ) : if isinstance ( in_data , str ) : in_data = [ ord ( c ) for c in in_data ] register = self . NonDirectInit for octet in in_data : if self . ReflectIn : octet = self . reflect ( octet , 8 ) for i in range ( 8 ) : topbit = register & self . MSB_Mask register = ( ( register << 1 ) & self . Mask ) | ( ( octet >> ( 7 - i ) ) & 0x01 ) if topbit : register ^= self . Poly for i in range ( self . Width ) : topbit = register & self . MSB_Mask register = ( ( register << 1 ) & self . Mask ) if topbit : register ^= self . Poly if self . ReflectOut : register = self . reflect  ( register , self . Width ) return register ^ self . XorOut"}
{"code_tokens": "def gen_table ( self ) : table_length = 1 << self . TableIdxWidth tbl = [ 0 ] * table_length for i in range ( table_length ) : register = i if self . ReflectIn : register = self . reflect ( register , self . TableIdxWidth ) register = register << ( self . Width - self . TableIdxWidth + self . CrcShift ) for j in range ( self . TableIdxWidth ) : if register & ( self . MSB_Mask << self . CrcShift ) != 0 : register = ( register << 1 ) ^ ( self . Poly  << self . CrcShift ) else : register = ( register << 1 ) if self . ReflectIn : register = self . reflect ( register >> self . CrcShift , self . Width ) << self . CrcShift tbl [ i ] = register & ( self . Mask << self . CrcShift ) return tbl"}
{"code_tokens": "def table_driven ( self , in_data ) : if isinstance ( in_data , str ) : in_data = [ ord ( c ) for c in in_data ] tbl = self . gen_table ( ) register = self . DirectInit << self . CrcShift if not self . ReflectIn : for octet in in_data : tblidx = ( ( register >> ( self . Width - self . TableIdxWidth + self . CrcShift ) ) ^ octet ) & 0xff register = ( ( register << ( self . TableIdxWidth - self . CrcShift ) ) ^ tbl [ tblidx ] ) & ( self . Mask << self . CrcShift ) register = register >> self . CrcShift else : register = self . reflect ( register , self . Width + self . CrcShift ) << self . CrcShift for octet in in_data : tblidx = ( ( register >> self . CrcShift ) ^ octet ) & 0xff register = ( ( register >> self . TableIdxWidth ) ^ tbl [ tblidx ] ) & ( self . Mask  << self . CrcShift ) register = self . reflect ( register , self . Width + self . CrcShift ) & self . Mask if self . ReflectOut : register = self . reflect ( register , self . Width ) return register ^ self . XorOut"}
{"code_tokens": "def parse_masked ( seq , min_len ) : nm , masked = [ ] , [ [ ] ] prev = None for base in seq [ 1 ] : if base . isupper ( ) : nm . append ( base ) if masked != [ [ ] ] and len ( masked [ - 1 ]  ) < min_len : nm . extend ( masked [ - 1 ] ) del masked [ - 1 ] prev = False elif base . islower ( ) : if prev is False : masked . append ( [ ] ) masked [ - 1 ] . append ( base ) prev = True return nm , masked"}
{"code_tokens": "def strip_masked ( fasta , min_len , print_masked ) : for seq in parse_fasta ( fasta ) : nm , masked = parse_masked ( seq , min_len ) nm = [ '%s removed_masked >=%s' % ( seq [ 0 ] , min_len ) , '' . join ( nm ) ] yield [ 0 , nm ] if print_masked is  True : for i , m in enumerate ( [ i for i in masked if i != [ ] ] , 1 ) : m = [ '%s insertion:%s' % ( seq [ 0 ] , i ) , '' . join ( m ) ] yield [ 1 , m ]"}
{"code_tokens": "def get_relative_abundance ( biomfile ) : biomf = biom . load_table ( biomfile ) norm_biomf = biomf . norm ( inplace = False ) rel_abd = { } for sid in norm_biomf . ids ( ) : rel_abd [ sid ] = { } for otuid in norm_biomf . ids ( \"observation\" ) : otuname = oc . otu_name ( norm_biomf . metadata ( otuid , axis = \"observation\" ) [ \"taxonomy\" ] ) otuname = \" \" . join ( otuname  . split ( \"_\" ) ) abd = norm_biomf . get_value_by_ids ( otuid , sid ) rel_abd [ sid ] [ otuname ] = abd ast_rel_abd = bc . arcsine_sqrt_transform ( rel_abd ) return ast_rel_abd"}
{"code_tokens": "def find_otu ( otuid , tree ) : for m in re . finditer ( otuid , tree ) : before , after = tree [ m . start ( ) - 1 ] , tree [ m . start ( ) + len ( otuid ) ] if before in [ \"(\" , \",\" , \")\" ] and after in [ \":\" , \";\" ] :  return m . start ( ) return None"}
{"code_tokens": "def newick_replace_otuids ( tree , biomf ) : for val , id_ , md in biomf . iter ( axis = \"observation\" ) : otu_loc = find_otu ( id_ , tree ) if otu_loc is not None : tree  = tree [ : otu_loc ] + oc . otu_name ( md [ \"taxonomy\" ] ) + tree [ otu_loc + len ( id_ ) : ] return tree"}
{"code_tokens": "def genome_info ( genome , info ) : try : scg = info [ '#SCGs' ] dups = info [ '#SCG duplicates' ] length = info [ 'genome size (bp)' ] return [ scg - dups , length , genome ] except : return  [ False , False , info [ 'genome size (bp)' ] , genome ]"}
{"code_tokens": "def print_clusters ( fastas , info , ANI ) : header = [ '#cluster' , 'num. genomes' , 'rep.' , 'genome' , '#SCGs' , '#SCG duplicates' , 'genome size (bp)' , 'fragments' , 'list' ] yield header in_cluster = [ ] for cluster_num , cluster in enumerate ( connected_components ( ANI ) ) : cluster = sorted ( [ genome_info ( genome , info [ genome ] ) for genome in cluster ] , key = lambda x : x [ 0 : ] , reverse = True ) rep = cluster [ 0 ] [ - 1 ] cluster = [ i [ - 1 ] for i in cluster ] size = len ( cluster ) for genome in cluster : in_cluster . append ( genome ) try : stats = [ size , rep , genome , info [ genome ] [ '#SCGs' ] , info [ genome ] [ '#SCG duplicates' ] , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , cluster ] except : stats = [ size , rep , genome , 'n/a' , 'n/a' , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , cluster ] if rep == genome : stats = [ '*%s' % ( cluster_num ) ] + stats else : stats = [ cluster_num ] + stats yield stats try : start = cluster_num + 1 except : start = 0 fastas = set ( [ i . rsplit ( '.' , 1 ) [ 0 ] . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.contigs' ) [ 0 ] for i in fastas ] ) for cluster_num , genome in enumerate ( fastas . difference ( set ( in_cluster ) ) , start ) : try : stats =  [ '*%s' % ( cluster_num ) , 1 , genome , genome , info [ genome ] [ '#SCGs' ] , info [ genome ] [ '#SCG duplicates' ] , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , [ genome ] ] except : stats = [ '*%s' % ( cluster_num ) , 1 , genome , genome , 'n/a' , 'n/a' , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , [ genome ] ] yield stats"}
{"code_tokens": "def parse_ggKbase_tables ( tables , id_type ) : g2info = { } for table in tables : for line in open ( table ) : line = line . strip ( ) . split ( '\\t' ) if line [ 0 ] . startswith ( 'name' ) : header = line header [ 4 ] = 'genome size (bp)' header [ 12 ] = '#SCGs' header [ 13 ] = '#SCG duplicates' continue name , code , info = line [ 0 ] , line [ 1 ] , line info = [ to_int ( i ) for i in info ] if id_type is False : if 'UNK' in code or 'unknown' in code : code = name if ( name != code ) and ( name and code in g2info ) : print ( '# duplicate name or code in table(s)' , file = sys . stderr ) print ( '# %s and/or %s' % ( name , code ) , file = sys . stderr ) exit ( ) if name not in g2info : g2info [ name ] = { item : stat for item , stat in  zip ( header , info ) } if code not in g2info : g2info [ code ] = { item : stat for item , stat in zip ( header , info ) } else : if id_type == 'name' : ID = name elif id_type == 'code' : ID = code else : print ( '# specify name or code column using -id' , file = sys . stderr ) exit ( ) ID = ID . replace ( ' ' , '' ) g2info [ ID ] = { item : stat for item , stat in zip ( header , info ) } if g2info [ ID ] [ 'genome size (bp)' ] == '' : g2info [ ID ] [ 'genome size (bp)' ] = 0 return g2info"}
{"code_tokens": "def parse_checkM_tables ( tables ) : g2info = { } for table in tables : for line in open ( table ) : line = line . strip ( ) . split ( '\\t' ) if line [ 0 ] . startswith ( 'Bin Id' ) : header = line header [ 8 ] = 'genome size (bp)' header [ 5 ] = '#SCGs' header [ 6 ] = '#SCG duplicates' continue ID , info = line [ 0 ] , line info = [ to_int ( i ) for i in info ] ID = ID . replace ( ' ' , '' ) g2info [ ID ] = { item : stat for item , stat in zip ( header , info ) }  if g2info [ ID ] [ 'genome size (bp)' ] == '' : g2info [ ID ] [ 'genome size (bp)' ] = 0 return g2info"}
{"code_tokens": "def genome_lengths ( fastas , info ) : if info is False : info = { } for genome in fastas : name = genome . rsplit ( '.' , 1 ) [ 0 ] . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.contigs' ) [ 0 ] if name in info : continue length = 0 fragments = 0 for seq in parse_fasta ( genome ) : length += len ( seq [ 1 ] ) fragments += 1 info [ name ] = { 'genome size (bp)' : length , '# contigs' : fragments  } return info"}
{"code_tokens": "def get_dbs ( self , attr , args , kwargs , ** fkwargs ) : if not self . _ready : if not self . setup_router ( args = args , kwargs = kwargs , ** fkwargs ) : raise self . UnableToSetupRouter ( ) retval = self . _pre_routing ( attr = attr , args = args , kwargs = kwargs , ** fkwargs ) if retval is not None : args , kwargs = retval if not ( args or kwargs ) : return self . cluster . hosts . keys ( ) try : db_nums = self . _route ( attr = attr , args = args , kwargs = kwargs , ** fkwargs ) except Exception  as e : self . _handle_exception ( e ) db_nums = [ ] return self . _post_routing ( attr = attr , db_nums = db_nums , args = args , kwargs = kwargs , ** fkwargs )"}
{"code_tokens": "def setup_router ( self , args , kwargs , ** fkwargs ) : self . _ready = self . _setup_router ( args = args , kwargs = kwargs , ** fkwargs )  return self . _ready"}
{"code_tokens": "def _route ( self , attr , args , kwargs , ** fkwargs ) : return self . cluster . hosts .  keys ( )"}
{"code_tokens": "def check_down_connections ( self ) : now = time . time ( ) for db_num , marked_down_at in self . _down_connections . items ( ) : if marked_down_at + self .  retry_timeout <= now : self . mark_connection_up ( db_num )"}
{"code_tokens": "def flush_down_connections ( self ) : self . _get_db_attempts = 0 for db_num in self . _down_connections . keys ( )  : self . mark_connection_up ( db_num )"}
{"code_tokens": "def standby ( df , resolution = '24h' , time_window = None ) : if df . empty : raise EmptyDataFrame ( ) df = pd . DataFrame ( df ) def parse_time ( t ) : if isinstance ( t , numbers . Number ) : return pd . Timestamp . utcfromtimestamp ( t ) . time ( ) else : return pd . Timestamp ( t ) . time ( ) if time_window is not None : t_start = parse_time ( time_window [ 0 ] ) t_end = parse_time ( time_window [ 1 ] ) if t_start > t_end : df = df [ ( df . index . time >= t_start ) | ( df . index . time <  t_end ) ] else : df = df [ ( df . index . time >= t_start ) & ( df . index . time < t_end ) ] return df . resample ( resolution ) . min ( )"}
{"code_tokens": "def share_of_standby ( df , resolution = '24h' , time_window = None ) : p_sb = standby ( df , resolution , time_window ) df = df . resample ( resolution ) . mean ( ) p_tot = df . sum ( ) p_standby = p_sb . sum ( ) share_standby = p_standby / p_tot res  = share_standby . iloc [ 0 ] return res"}
{"code_tokens": "def count_peaks ( ts ) : on_toggles = ts . diff ( ) > 3000 shifted = np . logical_not ( on_toggles . shift ( 1 ) ) result = on_toggles & shifted count  = result . sum ( ) return count"}
{"code_tokens": "def load_factor ( ts , resolution = None , norm = None ) : if norm is None : norm = ts . max ( ) if resolution is not None : ts = ts . resample ( rule = resolution  ) . mean ( ) lf = ts / norm return lf"}
{"code_tokens": "def top_hits ( hits , num , column , reverse ) : hits . sort ( key = itemgetter ( column ) , reverse = reverse ) for hit in hits [ 0 : num  ] : yield hit"}
{"code_tokens": "def numBlast_sort ( blast , numHits , evalueT , bitT ) : header = [ '#query' , 'target' , 'pident' , 'alen' , 'mismatch' , 'gapopen' , 'qstart' , 'qend' , 'tstart' , 'tend' , 'evalue' , 'bitscore' ] yield header hmm = { h : [ ] for h in header } for line in blast : if line . startswith ( '#' ) : continue line = line . strip ( ) . split ( '\\t' ) line [ 10 ] , line [ 11 ] = float ( line [ 10 ] ) , float ( line [ 11 ] ) evalue , bit = line [ 10 ] , line  [ 11 ] if evalueT is not False and evalue > evalueT : continue if bitT is not False and bit < bitT : continue for i , h in zip ( line , header ) : hmm [ h ] . append ( i ) hmm = pd . DataFrame ( hmm ) for query , df in hmm . groupby ( by = [ '#query' ] ) : df = df . sort_values ( by = [ 'bitscore' ] , ascending = False ) for hit in df [ header ] . values [ 0 : numHits ] : yield hit"}
{"code_tokens": "def numBlast ( blast , numHits , evalueT = False , bitT = False , sort = False ) : if sort is True : for hit in numBlast_sort ( blast , numHits , evalueT , bitT ) : yield hit return header = [ '#query' , 'target' , 'pident' , 'alen' , 'mismatch' , 'gapopen' , 'qstart' , 'qend' , 'tstart' , 'tend' , 'evalue' , 'bitscore' ] yield header prev , hits = None , [ ] for line in blast : line = line . strip ( ) . split ( '\\t' ) ID = line [ 0 ] line [ 10 ] , line [ 11 ] = float ( line [ 10 ] ) , float ( line [ 11 ] ) evalue , bit = line [ 10 ] , line [ 11 ] if ID != prev : if len ( hits ) > 0 : for hit in top_hits ( hits , numHits , 11 , True ) : yield  hit hits = [ ] if evalueT == False and bitT == False : hits . append ( line ) elif evalue <= evalueT and bitT == False : hits . append ( line ) elif evalue <= evalueT and bit >= bitT : hits . append ( line ) elif evalueT == False and bit >= bitT : hits . append ( line ) prev = ID for hit in top_hits ( hits , numHits , 11 , True ) : yield hit"}
{"code_tokens": "def numDomtblout ( domtblout , numHits , evalueT , bitT , sort ) : if sort is True : for hit in numDomtblout_sort ( domtblout , numHits , evalueT , bitT ) : yield hit return header = [ '#target name' , 'target accession' , 'tlen' , 'query name' , 'query accession' , 'qlen' , 'full E-value' , 'full score' , 'full bias' , 'domain #' , '# domains' , 'domain c-Evalue' , 'domain i-Evalue' , 'domain score' , 'domain bias' , 'hmm from' , 'hmm to' , 'seq from' , 'seq to' , 'env from' , 'env to' , 'acc' , 'target description' ] yield header prev , hits = None , [ ] for line in domtblout : if line . startswith ( '#' ) : continue line = line . strip ( ) . split ( ) desc = ' ' . join ( line [ 18 : ] ) line = line [ 0 : 18 ] line . append ( desc ) ID = line [ 0 ] + line [ 9 ] line [ 11 ] , line [ 13 ] = float ( line [ 11 ] ) , float ( line [ 13 ] ) evalue , bitscore = line [ 11 ] , line [ 13 ] line [ 11 ] , line [ 13 ] = evalue , bitscore if ID != prev : if len ( hits ) > 0 : for hit in top_hits (  hits , numHits , 13 , True ) : yield hit hits = [ ] if evalueT == False and bitT == False : hits . append ( line ) elif evalue <= evalueT and bitT == False : hits . append ( line ) elif evalue <= evalueT and bit >= bitT : hits . append ( line ) elif evalueT == False and bit >= bitT : hits . append ( line ) prev = ID for hit in top_hits ( hits , numHits , 13 , True ) : yield hit"}
{"code_tokens": "def stock2fa ( stock ) : seqs = { } for line in stock : if line . startswith ( '#' ) is False and line . startswith ( ' ' ) is False and len ( line ) > 3 : id , seq = line . strip ( ) . split ( ) id = id . rsplit ( '/' ,  1 ) [ 0 ] id = re . split ( '[0-9]\\|' , id , 1 ) [ - 1 ] if id not in seqs : seqs [ id ] = [ ] seqs [ id ] . append ( seq ) if line . startswith ( '//' ) : break return seqs"}
{"code_tokens": "def week_schedule ( index , on_time = None , off_time = None , off_days = None ) : if on_time is None : on_time = '9:00' if off_time is None : off_time = '17:00' if off_days is None : off_days = [ 'Sunday' , 'Monday' ] if not isinstance ( on_time , datetime . time ) : on_time = pd . to_datetime ( on_time , format = '%H:%M' ) . time ( ) if not isinstance ( off_time , datetime . time ) : off_time = pd . to_datetime ( off_time , format = '%H:%M' ) . time ( ) times  = ( index . time >= on_time ) & ( index . time < off_time ) & ( ~ index . weekday_name . isin ( off_days ) ) return pd . Series ( times , index = index )"}
{"code_tokens": "def carpet ( timeseries , ** kwargs ) : cmap = kwargs . pop ( 'cmap' , cm . coolwarm ) norm = kwargs . pop ( 'norm' , LogNorm ( ) ) interpolation = kwargs . pop ( 'interpolation' , 'nearest' ) cblabel = kwargs . pop ( 'zlabel' , timeseries . name if timeseries . name else '' ) title = kwargs . pop ( 'title' , 'carpet plot: ' + timeseries . name if timeseries . name else '' ) if timeseries . dropna ( ) . empty : print ( 'skipped {} - no data' . format ( title ) ) return ts = timeseries . resample ( '15min' ) . interpolate ( ) vmin = max ( 0.1 , kwargs . pop ( 'vmin' , ts [ ts > 0 ] . min ( ) ) ) vmax = max ( vmin , kwargs . pop ( 'vmax' , ts . quantile ( .999 ) ) ) mpldatetimes = date2num ( ts . index . to_pydatetime ( ) ) ts . index = pd . MultiIndex . from_arrays ( [ np . floor ( mpldatetimes ) , 2 + mpldatetimes % 1 ] ) df = ts . unstack ( ) fig , ax = plt . subplots ( ) extent = [ df . columns [ 0 ] , df . columns [ - 1 ] , df . index [ - 1 ] + 0.5 , df . index [ 0 ] - 0.5 ] im = plt . imshow ( df , vmin = vmin ,  vmax = vmax , extent = extent , cmap = cmap , aspect = 'auto' , norm = norm , interpolation = interpolation , ** kwargs ) ax . xaxis_date ( ) ax . xaxis . set_major_locator ( HourLocator ( interval = 2 ) ) ax . xaxis . set_major_formatter ( DateFormatter ( '%H:%M' ) ) ax . xaxis . grid ( True ) plt . xlabel ( 'UTC Time' ) ax . yaxis_date ( ) dmin , dmax = ax . yaxis . get_data_interval ( ) number_of_days = ( num2date ( dmax ) - num2date ( dmin ) ) . days if abs ( number_of_days ) <= 35 : ax . yaxis . set_major_locator ( DayLocator ( ) ) else : ax . yaxis . set_major_locator ( AutoDateLocator ( ) ) ax . yaxis . set_major_formatter ( DateFormatter ( \"%a, %d %b %Y\" ) ) cbticks = np . logspace ( np . log10 ( vmin ) , np . log10 ( vmax ) , 11 , endpoint = True ) cb = plt . colorbar ( format = '%.0f' , ticks = cbticks ) cb . set_label ( cblabel ) plt . title ( title ) return im"}
{"code_tokens": "def calc_pident_ignore_gaps ( a , b ) : m = 0 mm = 0 for A , B in zip ( list ( a ) , list ( b ) ) : if A == '-' or A == '.' or B == '-' or B == '.' : continue if A == B : m += 1 else : mm += 1 try : return float ( float ( m ) / float ( ( m + mm  ) ) ) * 100 except : return 0"}
{"code_tokens": "def remove_gaps ( A , B ) : a_seq , b_seq = [ ] , [ ] for a , b in zip ( list ( A ) , list ( B ) ) : if a == '-' or a == '.' or b == '-' or b == '.' : continue a_seq . append ( a ) b_seq . append ( b ) return '' . join ( a_seq ) , ''  . join ( b_seq )"}
{"code_tokens": "def compare_seqs ( seqs ) : A , B , ignore_gaps = seqs a , b = A [ 1 ] , B [ 1 ] if len ( a ) != len ( b ) : print ( '# reads are not the same length' , file  = sys . stderr ) exit ( ) if ignore_gaps is True : pident = calc_pident_ignore_gaps ( a , b ) else : pident = calc_pident ( a , b ) return A [ 0 ] , B [ 0 ] , pident"}
{"code_tokens": "def compare_seqs_leven ( seqs ) : A , B , ignore_gaps = seqs a , b = remove_gaps ( A [ 1 ] , B [ 1 ] ) if len ( a ) != len ( b ) : print ( '# reads are not the same length' , file = sys . stderr ) exit ( ) pident = lr ( a , b ) * 100 return A [ 0 ] , B  [ 0 ] , pident"}
{"code_tokens": "def pairwise_compare ( afa , leven , threads , print_list , ignore_gaps ) : seqs = { seq [ 0 ] : seq for seq in nr_fasta ( [ afa ] , append_index = True ) } num_seqs = len ( seqs ) pairs = ( ( i [ 0 ] , i [ 1 ] , ignore_gaps ) for i in itertools . combinations ( list ( seqs . values ( ) ) , 2 ) ) pool = multithread ( threads ) if leven is True : pident = pool . map ( compare_seqs_leven , pairs )  else : compare = pool . imap_unordered ( compare_seqs , pairs ) pident = [ i for i in tqdm ( compare , total = ( num_seqs * num_seqs ) / 2 ) ] pool . close ( ) pool . terminate ( ) pool . join ( ) return to_dictionary ( pident , print_list )"}
{"code_tokens": "def print_pairwise ( pw , median = False ) : names = sorted ( set ( [ i for i in pw ] ) ) if len ( names ) != 0 : if '>' in names [ 0 ] : yield [ '#' ] + [ i . split ( '>' ) [ 1 ] for i in names if '>' in i ] else : yield [ '#' ] + names for a in names : if '>' in a : yield [ a . split ( '>' ) [ 1 ] ] + [ pw [ a ] [ b ] for b in names ] else :  out = [ ] for b in names : if b in pw [ a ] : if median is False : out . append ( max ( pw [ a ] [ b ] ) ) else : out . append ( np . median ( pw [ a ] [ b ] ) ) else : out . append ( '-' ) yield [ a ] + out"}
{"code_tokens": "def print_comps ( comps ) : if comps == [ ] : print ( 'n/a' ) else : print ( '# min: %s, max: %s, mean: %s' % ( min ( comps ) , max ( comps )  , np . mean ( comps ) ) )"}
{"code_tokens": "def compare_clades ( pw ) : names = sorted ( set ( [ i for i in pw ] ) ) for i in range ( 0 , 4 ) : wi , bt = { } , { } for a in names : for b in pw [ a ] : if ';' not in a or ';' not in b : continue pident = pw [ a ] [ b ] cA , cB = a . split ( ';' ) [ i ] , b . split ( ';' ) [ i ] if i == 0 and '_' in cA and '_' in cB : cA = cA . rsplit ( '_' , 1 ) [ 1 ] cB = cB . rsplit ( '_' , 1 ) [ 1 ] elif '>' in cA or '>' in cB : cA = cA . split ( '>' ) [ 1 ] cB = cB . split ( '>' ) [ 1 ] if cA == cB : if cA not in wi : wi [ cA ] = [ ] wi [ cA ] . append ( pident ) else : if cA not in bt : bt [ cA ] = { } if cB not in bt [ cA ] : bt [ cA ] [ cB ] = [ ] bt [ cA ] [ cB ] . append ( pident ) print ( '\\n# min. within' ) for clade , pidents in list ( wi . items ( ) ) : print ( '\\t' . join ( [ 'wi:%s' % str ( i ) , clade , str ( min ( pidents ) ) ] ) ) comps = [ ] print ( '\\n# max. between' ) for comp in print_pairwise ( bt ) : if comp is not None : print ( '\\t' . join ( [ 'bt:%s' % str ( i ) ] + [ str ( j ) for j in comp ] ) ) if comp [ 0 ] != '#' : comps . extend ( [ j for j in comp [ 1 : ] if j != '-' ] ) print_comps ( comps ) comps = [ ] print ( '\\n# median between' ) for comp in print_pairwise ( bt , median = True ) : if comp is not None :  print ( '\\t' . join ( [ 'bt:%s' % str ( i ) ] + [ str ( j ) for j in comp ] ) ) if comp [ 0 ] != '#' : comps . extend ( [ j for j in comp [ 1 : ] if j != '-' ] ) print_comps ( comps )"}
{"code_tokens": "def matrix2dictionary ( matrix ) : pw = { } for line in matrix : line = line . strip ( ) . split ( '\\t' ) if line [ 0 ] . startswith ( '#' ) : names = line [ 1 : ] continue a = line [ 0 ] for i , pident in enumerate ( line [ 1 : ] ) : b = names [ i ] if a not in pw : pw [ a ] =  { } if b not in pw : pw [ b ] = { } if pident != '-' : pident = float ( pident ) pw [ a ] [ b ] = pident pw [ b ] [ a ] = pident return pw"}
{"code_tokens": "def setoption ( parser , metadata = None ) : parser . add_argument ( '-v' , action = 'version' , version = __version__ ) subparsers = parser . add_subparsers ( help = 'sub commands help' ) create_cmd = subparsers . add_parser ( 'create' ) create_cmd . add_argument ( 'name' , help = 'Specify Python package name.' ) create_cmd . add_argument ( '-d' , dest = 'description' , action = 'store' , help = 'Short description about your package.' ) create_cmd . add_argument ( '-a' , dest = 'author' , action = 'store' , required = True , help = 'Python package author name.' ) create_cmd . add_argument ( '-e' , dest = 'email' , action = 'store' , required = True , help = 'Python package author email address.' ) create_cmd . add_argument ( '-l' , dest = 'license' , choices = metadata . licenses ( ) . keys ( ) , default = 'GPLv3+' , help = 'Specify license. (default: %(default)s)' ) create_cmd . add_argument ( '-s' , dest = 'status' , choices = metadata . status ( ) . keys ( ) , default = 'Alpha' , help = ( 'Specify development status. ' '(default: %(default)s)' ) ) create_cmd . add_argument ( '--no-check' , action = 'store_true' , help = 'No checking package name in PyPI.' ) create_cmd . add_argument ( '--with-samples' , action = 'store_true' , help = 'Generate package with sample code.' ) group = create_cmd . add_mutually_exclusive_group ( required = True ) group . add_argument ( '-U' , dest = 'username' , action = 'store' , help = 'Specify GitHub username.' ) group . add_argument ( '-u' , dest = 'url' , action = 'store' , type = valid_url , help = 'Python  package homepage url.' ) create_cmd . add_argument ( '-o' , dest = 'outdir' , action = 'store' , default = os . path . abspath ( os . path . curdir ) , help = 'Specify output directory. (default: $PWD)' ) list_cmd = subparsers . add_parser ( 'list' ) list_cmd . add_argument ( '-l' , dest = 'licenses' , action = 'store_true' , help = 'show license choices.' )"}
{"code_tokens": "def parse_options ( metadata ) : parser = argparse . ArgumentParser ( description = '%(prog)s usage:' , prog = __prog__ ) setoption ( parser , metadata = metadata ) return parser "}
{"code_tokens": "def main ( ) : try : pkg_version = Update ( ) if pkg_version . updatable ( ) : pkg_version . show_message ( ) metadata = control . retreive_metadata ( ) parser = parse_options ( metadata ) argvs = sys . argv if len ( argvs ) <= 1 : parser . print_help ( ) sys . exit ( 1 ) args = parser  . parse_args ( ) control . print_licences ( args , metadata ) control . check_repository_existence ( args ) control . check_package_existence ( args ) control . generate_package ( args ) except ( RuntimeError , BackendFailure , Conflict ) as exc : sys . stderr . write ( '{0}\\n' . format ( exc ) ) sys . exit ( 1 )"}
{"code_tokens": "def _check_or_set_default_params ( self ) : if not hasattr ( self , 'date' ) : self . _set_param ( 'date' , datetime . utcnow ( ) . strftime ( '%Y-%m-%d' ) ) if not hasattr ( self , 'version' ) : self . _set_param ( 'version' , self . default_version ) if not hasattr ( self , 'description' ) or  self . description is None : getattr ( self , '_set_param' ) ( 'description' , self . warning_message )"}
{"code_tokens": "def move ( self ) : if not os . path . isdir ( self . outdir ) : os . makedirs ( self . outdir ) shutil . move ( self . tmpdir , os . path . join ( self  . outdir , self . name ) )"}
{"code_tokens": "def vcs_init ( self ) : VCS ( os . path . join ( self . outdir ,  self . name ) , self . pkg_data )"}
{"code_tokens": "def find_steam_location ( ) : if registry is None : return None key = registry . CreateKey ( registry . HKEY_CURRENT_USER , \"Software\\Valve\\Steam\" ) return registry . QueryValueEx (  key , \"SteamPath\" ) [ 0 ]"}
{"code_tokens": "def plot_PCoA ( cat_data , otu_name , unifrac , names , colors , xr , yr , outDir , save_as , plot_style ) : fig = plt . figure ( figsize = ( 14 , 8 ) ) ax = fig . add_subplot ( 111 ) for i , cat in enumerate ( cat_data ) : plt . scatter ( cat_data [ cat ] [ \"pc1\" ] , cat_data [ cat ] [ \"pc2\" ] , cat_data [ cat ] [ \"size\" ] , color = colors [ cat ] , alpha = 0.85 , marker = \"o\" , edgecolor = \"black\" , label = cat ) lgnd = plt . legend ( loc = \"best\" , scatterpoints = 3 , fontsize = 13 ) for i in range ( len ( colors . keys ( ) ) ) : lgnd . legendHandles [ i ] . _sizes = [ 80 ] plt . title ( \" \" . join ( otu_name . split ( \"_\" ) ) , style = \"italic\" ) plt . ylabel ( \"PC2 (Percent Explained Variance {:.3f}%)\" . format ( float ( unifrac [ \"varexp\" ] [ 1 ] ) ) ) plt . xlabel ( \"PC1 (Percent Explained Variance {:.3f}%)\" . format ( float ( unifrac [ \"varexp\" ] [ 0 ] ) ) ) plt . xlim ( round ( xr [ 0 ] * 1.5 , 1 ) , round ( xr [ 1 ] * 1.5 , 1 ) ) plt . ylim ( round ( yr [ 0 ] * 1.5 , 1 ) , round ( yr [ 1 ] * 1.5 , 1 ) ) if plot_style : gu . ggplot2_style ( ax ) fc = \"0.8\" else : fc = \"none\" fig . savefig ( os . path . join ( outDir , \"_\" . join ( otu_name . split ( ) ) ) + \".\" + save_as , facecolor = fc , edgecolor = \"none\" , format = save_as , bbox_inches = \"tight\" , pad_inches = 0.2 ) plt . close (  fig )"}
{"code_tokens": "def split_by_category ( biom_cols , mapping , category_id ) : columns = defaultdict ( list ) for i , col in enumerate ( biom_cols ) : columns [ mapping  [ col [ 'id' ] ] [ category_id ] ] . append ( ( i , col ) ) return columns"}
{"code_tokens": "def print_line ( l ) : print_lines = [ '# STOCKHOLM' , '#=GF' , '#=GS' , ' ' ] if len ( l . split ( ) ) == 0 : return True for start in print_lines : if l  . startswith ( start ) : return True return False"}
{"code_tokens": "def stock2one ( stock ) : lines = { } for line in stock : line = line . strip ( ) if print_line ( line ) is True : yield line continue if line . startswith ( '//' ) : continue ID , seq = line . rsplit ( ' ' , 1 ) if ID not in lines : lines [ ID ] = '' else  : seq = seq . strip ( ) lines [ ID ] += seq for ID , line in lines . items ( ) : yield '\\t' . join ( [ ID , line ] ) yield '\\n//'"}
{"code_tokens": "def math_func ( f ) : @ wraps ( f ) def wrapper ( * args , ** kwargs ) : if len ( args ) > 0 : return_type = type ( args [ 0 ] ) if kwargs . has_key ( 'return_type' ) : return_type = kwargs [ 'return_type' ] kwargs . pop ( 'return_type' ) return return_type ( f ( * args , **  kwargs ) ) args = list ( ( setify ( x ) for x in args ) ) return return_type ( f ( * args , ** kwargs ) ) return wrapper"}
{"code_tokens": "def _code_search ( query , github_user = None ) : github_client = temple . utils . GithubClient ( ) headers = { 'Accept' : 'application/vnd.github.v3.text-match+json' } resp = github_client . get ( '/search/code' , params = { 'q' : query , 'per_page' : 100 } , headers = headers ) if resp . status_code == requests . codes . unprocessable_entity and github_user : raise temple . exceptions . InvalidGithubUserError ( 'Invalid Github user or org - \"{}\"' . format ( github_user ) ) resp . raise_for_status ( ) resp_data = resp . json ( ) repositories = collections . defaultdict ( dict ) while True : repositories . update ( { 'git@github.com:{}.git' . format ( repo [ 'repository' ] [ 'full_name' ] ) : repo [ 'repository' ] for repo in resp_data [ 'items' ] } ) next_url = _parse_link_header ( resp . headers ) . get ( 'next' ) if next_url : resp = requests . get ( next_url , headers = headers ) resp .  raise_for_status ( ) resp_data = resp . json ( ) else : break return repositories"}
{"code_tokens": "def ls ( github_user , template = None ) : temple . check . has_env_vars ( temple . constants . GITHUB_API_TOKEN_ENV_VAR ) if template : temple . check . is_git_ssh_path ( template ) search_q = 'user:{} filename:{} {}' . format ( github_user , temple . constants . TEMPLE_CONFIG_FILE , template ) else : search_q = 'user:{} cookiecutter.json in:path' . format ( github_user ) results = _code_search ( search_q , github_user ) return collections . OrderedDict  ( sorted ( results . items ( ) ) )"}
{"code_tokens": "def update ( check , enter_parameters , version ) : if check : if temple . update . up_to_date ( version = version ) : print ( 'Temple package is up to date' ) else : msg = ( 'This temple package is out of date with the latest template.' ' Update your package by running \"temple update\"  and commiting changes.' ) raise temple . exceptions . NotUpToDateWithTemplateError ( msg ) else : temple . update . update ( new_version = version , enter_parameters = enter_parameters )"}
{"code_tokens": "def ls ( github_user , template , long_format ) : github_urls = temple . ls . ls ( github_user , template = template ) for ssh_path , info in github_urls . items ( )  : if long_format : print ( ssh_path , '-' , info [ 'description' ] or '(no project description found)' ) else : print ( ssh_path )"}
{"code_tokens": "def switch ( template , version ) : temple . update .  update ( new_template = template , new_version = version )"}
{"code_tokens": "def _in_git_repo ( ) : ret = temple . utils . shell ( 'git rev-parse' , stderr = subprocess .  DEVNULL , check = False ) return ret . returncode == 0"}
{"code_tokens": "def _has_branch ( branch ) : ret = temple . utils . shell ( 'git rev-parse --verify {}' . format ( branch ) , stderr = subprocess . DEVNULL , stdout = subprocess . DEVNULL , check = False ) return ret . returncode  == 0"}
{"code_tokens": "def not_has_branch ( branch ) : if _has_branch ( branch ) : msg = 'Cannot proceed while {} branch exists; remove  and try again.' . format ( branch ) raise temple . exceptions . ExistingBranchError ( msg )"}
{"code_tokens": "def has_env_vars ( * env_vars ) : for env_var in env_vars : if not os . environ . get ( env_var ) : msg = ( 'Must set {} environment variable. View docs for  setting up environment at {}' ) . format ( env_var , temple . constants . TEMPLE_DOCS_URL ) raise temple . exceptions . InvalidEnvironmentError ( msg )"}
{"code_tokens": "def is_temple_project ( ) : if not os . path . exists ( temple . constants . TEMPLE_CONFIG_FILE ) : msg = 'No {} file found in repository.' . format ( temple . constants . TEMPLE_CONFIG_FILE ) raise  temple . exceptions . InvalidTempleProjectError ( msg )"}
{"code_tokens": "def _get_current_branch ( ) : result = temple . utils . shell ( 'git rev-parse --abbrev-ref HEAD' , stdout = subprocess . PIPE  ) return result . stdout . decode ( 'utf8' ) . strip ( )"}
{"code_tokens": "def clean ( ) : temple . check . in_git_repo ( ) current_branch = _get_current_branch ( ) update_branch = temple . constants . UPDATE_BRANCH_NAME temp_update_branch = temple . constants . TEMP_UPDATE_BRANCH_NAME if current_branch in ( update_branch , temp_update_branch ) : err_msg = ( 'You must change from the \"{}\" branch since it will be deleted during cleanup' ) . format ( current_branch ) raise temple . exceptions . InvalidCurrentBranchError ( err_msg ) if temple . check . _has_branch ( update_branch ) : temple . utils . shell ( 'git branch -D {}' . format ( update_branch ) ) if temple . check . _has_branch ( temp_update_branch ) : temple . utils . shell ( 'git branch -D {}'  . format ( temp_update_branch ) )"}
{"code_tokens": "def _cookiecutter_configs_have_changed ( template , old_version , new_version ) : temple . check . is_git_ssh_path ( template ) repo_path = temple . utils . get_repo_path ( template ) github_client = temple . utils . GithubClient ( ) api = '/repos/{}/contents/cookiecutter.json' . format ( repo_path ) old_config_resp = github_client . get ( api , params = { 'ref' : old_version } ) old_config_resp . raise_for_status ( ) new_config_resp = github_client . get ( api , params = { 'ref' : new_version }  ) new_config_resp . raise_for_status ( ) return old_config_resp . json ( ) [ 'content' ] != new_config_resp . json ( ) [ 'content' ]"}
{"code_tokens": "def _apply_template ( template , target , * , checkout , extra_context ) : with tempfile . TemporaryDirectory ( ) as tempdir : repo_dir = cc_main . cookiecutter ( template , checkout = checkout , no_input = True , output_dir = tempdir , extra_context = extra_context ) for item in os . listdir ( repo_dir ) : src = os . path . join ( repo_dir , item ) dst = os . path . join ( target , item ) if os . path . isdir ( src ) : if os . path . exists  ( dst ) : shutil . rmtree ( dst ) shutil . copytree ( src , dst ) else : if os . path . exists ( dst ) : os . remove ( dst ) shutil . copy2 ( src , dst )"}
{"code_tokens": "def up_to_date ( version = None ) : temple . check . in_git_repo ( ) temple . check . is_temple_project ( ) temple_config = temple . utils . read_temple_config ( ) old_template_version = temple_config [ '_version' ] new_template_version = version or _get_latest_template_version ( temple_config [ '_template'  ] ) return new_template_version == old_template_version"}
{"code_tokens": "def _needs_new_cc_config_for_update ( old_template , old_version , new_template , new_version ) : if old_template != new_template : return True else  : return _cookiecutter_configs_have_changed ( new_template , old_version , new_version )"}
{"code_tokens": "def shell ( cmd , check = True , stdin = None , stdout = None , stderr = None ) : return subprocess . run ( cmd , shell = True , check = check , stdin =  stdin , stdout = stdout , stderr = stderr )"}
{"code_tokens": "def read_temple_config ( ) : with open ( temple . constants . TEMPLE_CONFIG_FILE ) as temple_config_file : return yaml . load ( temple_config_file , Loader = yaml . SafeLoader ) "}
{"code_tokens": "def write_temple_config ( temple_config , template , version ) : with open ( temple . constants . TEMPLE_CONFIG_FILE , 'w' ) as temple_config_file : versioned_config = { **  temple_config , ** { '_version' : version , '_template' : template } , } yaml . dump ( versioned_config , temple_config_file , Dumper = yaml . SafeDumper )"}
{"code_tokens": "def get_cookiecutter_config ( template , default_config = None , version = None ) : default_config = default_config or { } config_dict = cc_config . get_user_config ( ) repo_dir , _ = cc_repository . determine_repo_dir ( template = template , abbreviations = config_dict [ 'abbreviations' ] , clone_to_dir = config_dict [ 'cookiecutters_dir' ] , checkout = version , no_input = True ) context_file = os . path . join ( repo_dir , 'cookiecutter.json' ) context = cc_generate . generate_context ( context_file = context_file , default_context =  { ** config_dict [ 'default_context' ] , ** default_config } ) return repo_dir , cc_prompt . prompt_for_config ( context )"}
{"code_tokens": "def set_cmd_env_var ( value ) : def func_decorator ( function ) : @ functools . wraps ( function ) def wrapper ( * args , ** kwargs ) : previous_cmd_env_var = os . getenv ( temple . constants . TEMPLE_ENV_VAR ) os . environ [ temple . constants . TEMPLE_ENV_VAR ] =  value try : ret_val = function ( * args , ** kwargs ) finally : if previous_cmd_env_var is None : del os . environ [ temple . constants . TEMPLE_ENV_VAR ] else : os . environ [ temple . constants . TEMPLE_ENV_VAR ] = previous_cmd_env_var return ret_val return wrapper return func_decorator"}
{"code_tokens": "def _call_api ( self , verb , url , ** request_kwargs ) : api = 'https://api.github.com{}' . format ( url ) auth_headers = { 'Authorization' : 'token {}' . format ( self . api_token ) } headers = { ** auth_headers , ** request_kwargs . pop ( 'headers' , { } ) } return getattr ( requests , verb ) ( api , headers = headers , ** request_kwargs  )"}
{"code_tokens": "def deploy ( target ) : if not os . getenv ( CIRCLECI_ENV_VAR ) : raise EnvironmentError ( 'Must be on CircleCI to run this script' ) current_branch = os . getenv ( 'CIRCLE_BRANCH' ) if ( target == 'PROD' ) and ( current_branch != 'master' ) : raise EnvironmentError ( ( 'Refusing to deploy to production from branch {current_branch!r}. ' 'Production deploys can only be made from master.' ) . format ( current_branch = current_branch ) ) if target in ( 'PROD' , 'TEST' ) : pypi_username = os . getenv ( '{target}_PYPI_USERNAME' . format ( target = target ) ) pypi_password = os . getenv ( '{target}_PYPI_PASSWORD' . format ( target = target ) ) else : raise ValueError ( \"Deploy target must be 'PROD' or 'TEST', got {target!r}.\" . format ( target = target ) ) if not ( pypi_username and pypi_password ) : raise EnvironmentError ( ( \"Missing '{target}_PYPI_USERNAME' and/or '{target}_PYPI_PASSWORD' \" \"environment variables. These are required to push to PyPI.\" ) . format ( target = target ) ) os . environ [ 'TWINE_USERNAME' ] = pypi_username os . environ [ 'TWINE_PASSWORD' ] = pypi_password _shell ( 'git config --global user.email \"oss@cloverhealth.com\"' ) _shell ( 'git config --global user.name \"Circle CI\"' ) _shell ( 'git config push.default current' ) ret = _shell ( 'make version' , stdout = subprocess . PIPE ) version = ret . stdout . decode ( 'utf-8' ) . strip ( ) print ( 'Deploying version {version!r}...' . format ( version = version ) ) _shell ( 'git tag -f -a {version} -m \"Version {version}\"' . format ( version = version ) ) _shell ( 'sed -i.bak \"s/^__version__ = .*/__version__ = {version!r}/\" */version.py' . format ( version = version ) ) _shell ( 'python setup.py sdist bdist_wheel' ) _shell ( 'git add ChangeLog  AUTHORS */version.py' ) _shell ( 'git commit --no-verify -m \"Merge autogenerated files [skip ci]\"' ) _pypi_push ( 'dist' ) _shell ( 'git push --follow-tags' ) print ( 'Deployment complete. Latest version is {version}.' . format ( version = version ) )"}
{"code_tokens": "def run ( self ) : filename = \".DS_Store\" command = \"find {path} -type f -name \\\"{filename}\\\" \" . format ( path = self . path , filename = filename ) cmd = CommandHelper ( command ) cmd . execute ( ) files = cmd . output . split ( \"\\n\" ) for f in files : if not f . endswith ( filename ) : continue rel_path = f . replace ( self . path , \"\" ) if rel_path . startswith ( tuple ( self . CONFIG [ 'exclude_paths' ] ) ) : continue issue = Issue ( )  issue . name = \"File .DS_Store detected\" issue . potential = False issue . severity = Issue . SEVERITY_LOW issue . file = rel_path self . saveIssue ( issue )"}
{"code_tokens": "def run ( self ) : options = { } if bool ( self . config [ 'use_proxy' ] ) : options [ 'proxies' ] = { \"http\" : self . config [ 'proxy' ] , \"https\" : self . config [ 'proxy' ] } options [ \"url\" ] = self . config [ 'url' ] options [ \"data\" ] = { \"issues\" : json . dumps (  map ( lambda x : x . __todict__ ( ) , self . issues ) ) } if 'get' == self . config [ 'method' ] . lower ( ) : requests . get ( ** options ) else : requests . post ( ** options )"}
{"code_tokens": "def path ( self , value ) : if not value . endswith ( '/' ) : self . _path = '{v}/' . format ( v = value ) else : self . _path = value "}
{"code_tokens": "def parseConfig ( cls , value ) : if 'enabled' in value : value [ 'enabled' ] = bool ( value [ 'enabled' ] ) if 'exclude_paths' in value : value [ 'exclude_paths' ] = [ n  . strip ( ) for n in ast . literal_eval ( value [ 'exclude_paths' ] ) ] return value"}
{"code_tokens": "def getOSName ( self ) : _system = platform . system ( ) if _system in [ self . __class__ . OS_WINDOWS , self . __class__ . OS_MAC , self . __class__ . OS_LINUX ] : if _system == self . __class__ . OS_LINUX : _dist = platform . linux_distribution ( ) [ 0 ] if _dist . lower ( ) == self . __class__ . OS_UBUNTU . lower ( ) : return self . __class__ . OS_UBUNTU elif _dist . lower ( ) == self . __class__ . OS_DEBIAN . lower ( ) : return self . __class__ . OS_DEBIAN elif _dist . lower  ( ) == self . __class__ . OS_CENTOS . lower ( ) : return self . __class__ . OS_CENTOS elif _dist . lower ( ) == self . __class__ . OS_REDHAT . lower ( ) : return self . __class__ . OS_REDHAT elif _dist . lower ( ) == self . __class__ . OS_KALI . lower ( ) : return self . __class__ . OS_KALI return _system else : return None"}
{"code_tokens": "def execute ( self , shell = True ) : process = Popen ( self . command , stdout = PIPE , stderr = PIPE , shell = shell ) self . output , self . errors = process . communicate ( ) "}
{"code_tokens": "def _debug ( message , color = None , attrs = None ) : if attrs is None : attrs = [ ] if color is not None : print colored ( message , color , attrs = attrs ) else : if len ( attrs ) > 0 : print colored ( message , \"white\" ,  attrs = attrs ) else : print message"}
{"code_tokens": "def setup ( ) : if not os . path . isdir ( AtomShieldsScanner . CHECKERS_DIR ) : os . makedirs ( AtomShieldsScanner . CHECKERS_DIR ) if not os . path . isdir ( AtomShieldsScanner . REPORTS_DIR ) : os . makedirs ( AtomShieldsScanner . REPORTS_DIR ) for f in AtomShieldsScanner . _getFiles ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , \"checkers\" ) , \"*.py\" ) : AtomShieldsScanner . installChecker ( f ) for f in AtomShieldsScanner . _getFiles ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , \"reports\" ) , \"*.py\" ) : AtomShieldsScanner . installReport ( f ) AtomShieldsScanner . _executeMassiveMethod ( path = AtomShieldsScanner . CHECKERS_DIR , method = \"install\" , args = { } ) config_dir = os . path . dirname ( AtomShieldsScanner . CONFIG_PATH ) if not os . path . isdir (  config_dir ) : os . makedirs ( config_dir )"}
{"code_tokens": "def _addConfig ( instance , config , parent_section ) : try : section_name = \"{p}/{n}\" . format ( p = parent_section , n = instance . NAME . lower ( ) ) config . add_section ( section_name ) for k in instance . CONFIG . keys  ( ) : config . set ( section_name , k , instance . CONFIG [ k ] ) except Exception as e : print \"[!] %s\" % e"}
{"code_tokens": "def getConfig ( self , section = None ) : data = { } if section is None : for s in self . config . sections ( ) : if '/' in s : parent , _s = s . split ( '/' ) data [ parent ] [ _s ] = dict ( self . config . items ( s ) ) else : data [ s ] = dict ( self . config . items ( s ) ) else : data = dict ( self . config . items  ( section ) ) return data"}
{"code_tokens": "def _getClassInstance ( path , args = None ) : if not path . endswith ( \".py\" ) : return None if args is None : args = { } classname = AtomShieldsScanner . _getClassName ( path ) basename = os . path . basename ( path ) . replace ( \".py\" , \"\" ) sys . path . append ( os . path . dirname ( path ) ) try : mod = __import__ ( basename , globals ( ) , locals ( ) , [ classname ] , - 1 ) class_ = getattr ( mod , classname ) instance = class_ ( ** args ) except Exception as  e : AtomShieldsScanner . _debug ( \"[!] %s\" % e ) return None finally : sys . path . remove ( os . path . dirname ( path ) ) return instance"}
{"code_tokens": "def _executeMassiveMethod ( path , method , args = None , classArgs = None ) : response = { } if args is None : args = { } if classArgs is None : classArgs = { } sys . path . append ( path ) exclude = [ \"__init__.py\" , \"base.py\" ] for f in AtomShieldsScanner . _getFiles ( path , \"*.py\" , exclude = exclude ) : try : instance = AtomShieldsScanner . _getClassInstance ( path = f , args = classArgs ) if instance is not None : if callable ( method ) : args [ \"instance\" ] = instance output = method ( ** args ) response [ instance . __class__ . NAME ] = output else : if hasattr ( instance , method ) : output = getattr ( instance , method ) ( ** args ) response [ instance . __class__ . NAME ] = output else : continue except Exception as e : AtomShieldsScanner . _debug ( \"[!] %s\" % e )  sys . path . remove ( path ) return response"}
{"code_tokens": "def run ( self ) : self . checkProperties ( ) self . debug ( \"[*] Iniciando escaneo de AtomShields con las siguientes propiedades. . . \" ) self . showScanProperties ( ) self . loadConfig ( ) init_ts = datetime . now ( ) cwd = os . getcwd ( ) os . chdir ( self . path ) issues = self . executeCheckers ( ) os . chdir ( cwd ) end_ts = datetime . now ( ) duration = '{}' . format ( end_ts - init_ts ) for plugin in issues . keys ( ) : value = issues [ plugin ] if isinstance ( value , list ) : map ( self . saveIssue , value ) else : self .  saveIssue ( value ) print \"\" self . executeReports ( ) self . debug ( \"\" ) self . debug ( \"Duration: {t}\" . format ( t = duration ) ) self . showSummary ( ) return self . issues"}
{"code_tokens": "def install ( ) : cmd = CommandHelper ( ) cmd . install ( \"npm\" ) cmd = CommandHelper ( ) cmd . install ( \"nodejs-legacy\" ) cmd = CommandHelper ( ) cmd . command = \"npm install -g retire\" cmd . execute  ( ) if cmd . errors : from termcolor import colored print colored ( cmd . errors , \"red\" ) else : print cmd . output"}
{"code_tokens": "def potential ( self , value ) : if value : self . _potential = True else : self . _potential  = False"}
{"code_tokens": "def get ( name , default = None , allow_default = True ) : return Config ( ) . get ( name  , default , allow_default = allow_default )"}
{"code_tokens": "def env ( key , default ) : value = os . environ . get ( key , None ) if value is not None : log . info ( ' %s = %r' , key . lower ( ) . replace  ( '_' , '.' ) , value ) return value key = key . lower ( ) . replace ( '_' , '.' ) value = get ( key ) if value is not None : return value return default"}
{"code_tokens": "def env_key ( key , default ) : env = key . upper ( ) . replace ( '.' , '_' ) return os . environ . get ( env ,  default )"}
{"code_tokens": "def set ( self , name , value ) : if not self . settings . get ( 'pyconfig.case_sensitive' , False ) : name = name . lower ( ) log . info (  \" %s = %s\" , name , repr ( value ) ) with self . mut_lock : self . settings [ name ] = value"}
{"code_tokens": "def _update ( self , conf_dict , base_name = None ) : for name in conf_dict : if name . startswith ( '_' ) : continue value = conf_dict [ name ] if value is Namespace : continue if base_name : name = base_name + '.' + name if isinstance ( value , Namespace ) : for name , value in value . iteritems ( name )  : self . set ( name , value ) elif callable ( value ) : value = value ( ) if value is not None : self . set ( name , value ) else : self . set ( name , value )"}
{"code_tokens": "def load ( self , clear = False ) : if clear : self . settings = { } defer = [ ] for conf in pkg_resources . iter_entry_points ( 'pyconfig' ) : if conf . attrs : raise RuntimeError ( \"config must be a module\" ) mod_name = conf . module_name base_name = conf . name if conf . name != 'any' else None log . info ( \"Loading module '%s'\" , mod_name ) mod_dict = runpy . run_module ( mod_name ) if mod_dict . get ( 'deferred' , None ) is deferred : log . info ( \"Deferring module '%s'\" , mod_name ) mod_dict . pop ( 'deferred' ) defer . append ( ( mod_name , base_name , mod_dict ) ) continue self . _update ( mod_dict , base_name ) for mod_name , base_name , mod_dict in defer : log . info ( \"Loading deferred module '%s'\" , mod_name ) self . _update ( mod_dict , base_name ) if etcd ( ) . configured : mod_dict = etcd ( ) . load ( ) if mod_dict : self . _update ( mod_dict ) mod_dict = None try : mod_dict = runpy . run_module ( 'localconfig' ) except ImportError : pass except ValueError as err : if getattr ( err , 'message' ) != '__package__ set to non-string' : raise mod_name = 'localconfig' if sys . version_info < ( 2 , 7 ) : loader , code , fname = runpy . _get_module_details ( mod_name ) else : _ , loader , code , fname = runpy . _get_module_details ( mod_name ) mod_dict = runpy . _run_code ( code , { } , { } , mod_name , fname , loader , pkg_name = None ) if mod_dict : log . info (  \"Loading module 'localconfig'\" ) self . _update ( mod_dict ) self . call_reload_hooks ( )"}
{"code_tokens": "def get ( self , name , default , allow_default = True ) : if not self . settings . get ( 'pyconfig.case_sensitive' , False ) : name = name . lower ( ) if name not in self . settings : if not allow_default : raise LookupError (  'No setting \"{name}\"' . format ( name = name ) ) self . settings [ name ] = default return self . settings [ name ]"}
{"code_tokens": "def init ( self , hosts = None , cacert = None , client_cert = None , client_key = None ) : try : import etcd self . module = etcd except ImportError : pass if not self . module : return self . _parse_jetconfig ( ) hosts = env ( 'PYCONFIG_ETCD_HOSTS' , hosts ) protocol = env ( 'PYCONFIG_ETCD_PROTOCOL' , None ) cacert = env ( 'PYCONFIG_ETCD_CACERT' , cacert ) client_cert = env ( 'PYCONFIG_ETCD_CERT' , client_cert ) client_key = env ( 'PYCONFIG_ETCD_KEY' , client_key ) username = None password = None auth = env ( 'PYCONFIG_ETCD_AUTH' , None ) if auth : auth = auth . split ( ':' ) auth . append ( '' ) username = auth [ 0 ] password = auth [ 1 ] hosts = self . _parse_hosts ( hosts ) if hosts is None : return kw = { } kw [ 'allow_reconnect' ] = True if protocol : kw [ 'protocol' ] = protocol if username : kw [ 'username' ] = username if password : kw [ 'password' ] = password if cacert : kw [ 'ca_cert' ] = os . path . abspath ( cacert ) if client_cert and client_key : kw [ 'cert' ] = ( ( os . path . abspath ( client_cert ) , os . path . abspath ( client_key ) ) ) elif client_cert : kw [ 'cert' ] = os . path . abspath ( client_cert ) if  cacert or client_cert or client_key : kw [ 'protocol' ] = 'https' self . client = self . module . Client ( hosts , ** kw )"}
{"code_tokens": "def load ( self , prefix = None , depth = None ) : prefix = prefix or self . prefix prefix = '/' + prefix . strip ( '/' ) + '/' if depth is None : depth = self . inherit_depth if not self . configured : log . debug ( \"etcd not available\" ) return if self . watching : log . info ( \"Starting watcher for %r\" , prefix ) self . start_watching ( ) log . info ( \"Loading from etcd %r\" , prefix ) try : result = self . client . get ( prefix ) except self . module . EtcdKeyNotFound : result = None if not result : log . info ( \"No configuration found\" ) return { } update = { } for item in result . children : key = item . key value = item . value try : value = pytool . json . from_json ( value ) except : pass if not self . case_sensitive : key = key . lower ( ) if key . startswith ( prefix ) : key = key [ len ( prefix ) : ] update [ key ] = value inherited = Config ( ) . settings . get ( self . inherit_key , update . get ( self . inherit_key , None ) ) if depth > 0 and inherited : log . info ( \" ... inheriting ...\" ) inherited = self . load ( inherited , depth  - 1 ) or { } inherited . update ( update ) update = inherited return update"}
{"code_tokens": "def get_watcher ( self ) : if not self . watching : raise StopIteration ( ) return self .  client . eternal_watch ( self . prefix , recursive = True )"}
{"code_tokens": "def start_watching ( self ) : if self . watcher and self . watcher . is_alive ( ) : return self . watcher = Watcher (  ) self . watcher . start ( )"}
{"code_tokens": "def _parse_hosts ( self , hosts ) : if hosts is None : return if isinstance ( hosts , six . string_types ) : hosts = [ host . strip ( ) for host in hosts . split ( ',' ) ] hosts  = [ host . split ( ':' ) for host in hosts ] hosts = [ ( host [ 0 ] , int ( host [ 1 ] ) ) for host in hosts ] return tuple ( hosts )"}
{"code_tokens": "def main ( ) : parser = argparse . ArgumentParser ( description = \"Helper for working with \" \"pyconfigs\" ) target_group = parser . add_mutually_exclusive_group ( ) target_group . add_argument ( '-f' , '--filename' , help = \"parse an individual file or directory\" , metavar = 'F' ) target_group . add_argument ( '-m' , '--module' , help = \"parse a package or module, recursively looking inside it\" , metavar = 'M' ) parser . add_argument ( '-v' , '--view-call' , help = \"show the actual pyconfig call made (default: show namespace)\" , action = 'store_true' ) parser . add_argument ( '-l' , '--load-configs' , help = \"query the currently set value for each key found\" , action = 'store_true' ) key_group = parser . add_mutually_exclusive_group ( ) key_group . add_argument ( '-a' , '--all' , help = \"show keys which don't have defaults set\" , action = 'store_true' ) key_group . add_argument ( '-k' , '--only-keys' , help = \"show a list of discovered keys without values\" , action = 'store_true' ) parser . add_argument ( '-n' , '--natural-sort' , help = \"sort by filename and line (default: alphabetical by key)\" , action = 'store_true' ) parser  . add_argument ( '-s' , '--source' , help = \"show source annotations (implies --natural-sort)\" , action = 'store_true' ) parser . add_argument ( '-c' , '--color' , help = \"toggle output colors (default: %s)\" % bool ( pygments ) , action = 'store_const' , default = bool ( pygments ) , const = ( not bool ( pygments ) ) ) args = parser . parse_args ( ) if args . color and not pygments : _error ( \"Pygments is required for color output.\\n\" \" pip install pygments\" ) if args . module : _handle_module ( args ) if args . filename : _handle_file ( args )"}
{"code_tokens": "def _handle_module ( args ) : module = _get_module_filename ( args . module ) if not module : _error ( \"Could not load module or package: %r\" , args . module ) elif isinstance ( module , Unparseable ) : _error ( \"Could not determine module source:  %r\" , args . module ) _parse_and_output ( module , args )"}
{"code_tokens": "def _error ( msg , * args ) : print ( msg % args  , file = sys . stderr ) sys . exit ( 1 )"}
{"code_tokens": "def _get_module_filename ( module ) : module = module . split ( '.' ) package = '.' . join ( module [ : - 1 ] ) module = module [ - 1 ] try : if not package : module = __import__ ( module ) else : package = __import__ ( package , fromlist = [ module ] ) module = getattr ( package , module , None ) filename = getattr ( module , '__file__' , None ) if not filename : return Unparseable ( ) if filename . endswith ( '.pyc' ) : filename = filename [ : - 1 ] if not os . path . exists ( filename ) and os . path . isfile (  filename ) : return Unparseable ( ) if filename . endswith ( '__init__.py' ) : filename = filename [ : - 11 ] return filename except ImportError : return"}
{"code_tokens": "def _parse_and_output ( filename , args ) : relpath = os . path . dirname ( filename ) if os . path . isfile ( filename ) : calls = _parse_file ( filename , relpath ) elif os . path . isdir ( filename ) : calls = _parse_dir ( filename , relpath ) else : _error ( \"Could not determine file type: %r\" , filename ) if not calls : _error ( \"No pyconfig calls.\" ) if args . load_configs : keys = set ( ) for call in calls : keys . add ( call . key ) conf = pyconfig . Config ( ) for key , value in  conf . settings . items ( ) : if key in keys : continue calls . append ( _PyconfigCall ( 'set' , key , value , [ None ] * 4 ) ) _output ( calls , args )"}
{"code_tokens": "def _output ( calls , args ) : if args . natural_sort or args . source : calls = sorted ( calls , key = lambda c : ( c . filename , c . lineno ) ) else : calls = sorted ( calls , key = lambda c : c . key ) out = [ ] if args . only_keys : keys = set ( ) for call in calls : if call . key in keys : continue out . append ( _format_call ( call , args ) ) keys . add ( call . key ) out = '\\n' . join ( out ) if args . color : out = _colorize ( out ) print ( out , end = ' ' ) return keys = set ( ) for call in calls : if call . default : keys . add ( call . key ) for call in calls : if not args . all and not call . default and call . key in keys : continue out . append ( _format_call ( call , args ) ) out = '\\n' . join ( out )  if args . color : out = _colorize ( out ) print ( out , end = ' ' )"}
{"code_tokens": "def _format_call ( call , args ) : out = '' if args . source : out += call . annotation ( ) + '\\n' if args . only_keys : out += call . get_key ( ) return out if args  . view_call : out += call . as_call ( ) elif args . load_configs : out += call . as_live ( ) else : out += call . as_namespace ( ) return out"}
{"code_tokens": "def _colorize ( output ) : if not pygments : return output return pygments . highlight ( output , pygments . lexers . PythonLexer ( ) , pygments  . formatters . Terminal256Formatter ( style = 'monokai' ) )"}
{"code_tokens": "def _map_arg ( arg ) : if isinstance ( arg , _ast . Str ) : return repr ( arg . s ) elif isinstance ( arg , _ast . Num ) : return arg . n elif isinstance ( arg , _ast . Name ) : name = arg . id if name == 'True' : return True elif name == 'False' : return False elif name == 'None' : return None return name  else : return Unparseable ( )"}
{"code_tokens": "def as_namespace ( self , namespace = None ) : key = self . key if namespace and key . startswith ( namespace ) : key = key [ len ( namespace ) + 1 : ] return \"%s = %s\" %  ( self . get_key ( ) , self . _default ( ) or NotSet ( ) )"}
{"code_tokens": "def as_live ( self ) : key = self . get_key ( ) default = pyconfig . get ( key ) if default : default = repr ( default ) else : default = self . _default ( ) or NotSet ( ) return \"%s =  %s\" % ( key , default )"}
{"code_tokens": "def as_call ( self ) : default = self . _default ( ) default = ', ' + default if default else '' return \"pyconfig.%s(%r%s)\" % ( self . method , self . get_key (  ) , default )"}
{"code_tokens": "def get_key ( self ) : if not isinstance ( self . key , Unparseable ) : return self . key line = self . source [ self . col_offset : ] regex = re  . compile ( ) match = regex . match ( line ) if not match : return Unparseable ( ) return \"<%s>\" % match . group ( 1 )"}
{"code_tokens": "def _default_value_only ( self ) : line = self . source [ self . col_offset : ] regex = re . compile (  ) match = regex . match ( line ) if not match : return '' return match . group ( 1 )"}
{"code_tokens": "def _default ( self ) : try : iter ( self . default ) except TypeError : return repr ( self . default ) for v in self . default : if isinstance ( v , Unparseable ) : default = self .  _default_value_only ( ) if default : return default return ', ' . join ( str ( v ) for v in self . default )"}
{"code_tokens": "def _get_param_names ( self ) : template = Template ( self . yaml_string ) names = [ 'yaml_string' ] for match in re . finditer ( template . pattern , template . template ) : name = match . group ( 'named' ) or match . group ( 'braced' ) assert name is not None names . append ( name ) return names "}
{"code_tokens": "def _get_dataset ( self , X , y = None ) : from pylearn2 . datasets import DenseDesignMatrix X = np . asarray ( X ) assert X . ndim > 1 if y is not None : y = self . _get_labels ( y  ) if X . ndim == 2 : return DenseDesignMatrix ( X = X , y = y ) return DenseDesignMatrix ( topo_view = X , y = y )"}
{"code_tokens": "def fit ( self , X , y = None ) : from pylearn2 . config import yaml_parse from pylearn2 . train import Train params = self . get_params ( ) yaml_string = Template ( self . yaml_string ) . substitute ( params ) self . trainer = yaml_parse . load ( yaml_string ) assert isinstance ( self . trainer , Train ) if self . trainer . dataset is not None : raise ValueError ( 'Train YAML database must evaluate to None.' ) self . trainer . dataset = self . _get_dataset ( X , y ) if ( hasattr ( self . trainer  . algorithm , 'monitoring_dataset' ) and self . trainer . algorithm . monitoring_dataset is not None ) : monitoring_dataset = self . trainer . algorithm . monitoring_dataset if len ( monitoring_dataset ) == 1 and '' in monitoring_dataset : monitoring_dataset [ '' ] = self . trainer . dataset else : monitoring_dataset [ 'train' ] = self . trainer . dataset self . trainer . algorithm . _set_monitoring_dataset ( monitoring_dataset ) else : self . trainer . algorithm . _set_monitoring_dataset ( self . trainer . dataset ) self . trainer . main_loop ( )"}
{"code_tokens": "def _predict ( self , X , method = 'fprop' ) : import theano X_sym = self . trainer . model . get_input_space ( ) . make_theano_batch ( ) y_sym = getattr ( self . trainer . model , method ) ( X_sym ) f = theano . function  ( [ X_sym ] , y_sym , allow_input_downcast = True ) return f ( X )"}
{"code_tokens": "def load ( self ) : from pylearn2 . config import yaml_parse from pylearn2 . datasets import Dataset dataset = yaml_parse . load ( self . yaml_string ) assert isinstance ( dataset , Dataset ) data = dataset . iterator ( mode = 'sequential' , num_batches = 1 , data_specs = dataset . data_specs , return_tuple = True ) . next ( ) if len ( data ) == 2 : X , y = data y = np . squeeze ( y ) if self . one_hot : y = np . argmax ( y , axis = 1  ) else : X = data y = None return X , y"}
{"code_tokens": "def _create_kernel ( self ) : kernels = self . kernel_params if not isinstance ( kernels , list ) : raise RuntimeError ( 'Must provide enumeration of kernels' ) for kernel in kernels : if sorted ( list ( kernel . keys ( ) ) ) != [ 'name' , 'options' , 'params' ] : raise RuntimeError ( 'strategy/params/kernels must contain keys: \"name\", \"options\", \"params\"' ) kernels = [ ] for kern in self . kernel_params : params = kern [ 'params' ] options = kern [ 'options' ] name = kern [ 'name' ] kernel_ep = load_entry_point ( name , 'strategy/params/kernels' ) if issubclass ( kernel_ep , KERNEL_BASE_CLASS ) : if options [ 'independent' ] : kernel = np . sum ( [ kernel_ep ( 1 , active_dims = [ i ] , ** params ) for i in range ( self . n_dims ) ] ) else : kernel = kernel_ep ( self . n_dims , ** params ) if not isinstance ( kernel , KERNEL_BASE_CLASS ) : raise RuntimeError ( 'strategy/params/kernel must load a' 'GPy derived Kernel' ) kernels . append ( kernel ) self . kernel = np . sum ( kernels  )"}
{"code_tokens": "def fit_and_score_estimator ( estimator , parameters , cv , X , y = None , scoring = None , iid = True , n_jobs = 1 , verbose = 1 , pre_dispatch = '2*n_jobs' ) : scorer = check_scoring ( estimator , scoring = scoring ) n_samples = num_samples ( X ) X , y = check_arrays ( X , y , allow_lists = True , sparse_format = 'csr' , allow_nans = True ) if y is not None : if len ( y ) != n_samples : raise ValueError ( 'Target variable (y) has a different number ' 'of samples (%i) than data (X: %i samples)' % ( len ( y ) , n_samples ) ) cv = check_cv ( cv = cv , y = y , classifier = is_classifier ( estimator ) ) out = Parallel ( n_jobs = n_jobs , verbose = verbose , pre_dispatch = pre_dispatch ) ( delayed ( _fit_and_score ) ( clone ( estimator ) , X , y , scorer , train , test , verbose , parameters , fit_params = None ) for train , test in cv . split ( X , y ) ) assert len ( out ) == cv . n_splits train_scores , test_scores = [ ] ,  [ ] n_train_samples , n_test_samples = [ ] , [ ] for test_score , n_test , train_score , n_train , _ in out : train_scores . append ( train_score ) test_scores . append ( test_score ) n_test_samples . append ( n_test ) n_train_samples . append ( n_train ) train_scores , test_scores = map ( list , check_arrays ( train_scores , test_scores , warn_nans = True , replace_nans = True ) ) if iid : if verbose > 0 and is_msmbuilder_estimator ( estimator ) : print ( '[CV] Using MSMBuilder API n_samples averaging' ) print ( '[CV] n_train_samples: %s' % str ( n_train_samples ) ) print ( '[CV] n_test_samples: %s' % str ( n_test_samples ) ) mean_test_score = np . average ( test_scores , weights = n_test_samples ) mean_train_score = np . average ( train_scores , weights = n_train_samples ) else : mean_test_score = np . average ( test_scores ) mean_train_score = np . average ( train_scores ) grid_scores = { 'mean_test_score' : mean_test_score , 'test_scores' : test_scores , 'mean_train_score' : mean_train_score , 'train_scores' : train_scores , 'n_test_samples' : n_test_samples , 'n_train_samples' : n_train_samples } return grid_scores"}
{"code_tokens": "def dict_merge ( base , top ) : out = dict ( top ) for key in base : if key in top : if isinstance ( base [ key ] , dict ) and isinstance ( top [ key ] , dict ) : out [ key ] = dict_merge ( base [ key ] , top [ key ] ) else  : out [ key ] = base [ key ] return out"}
{"code_tokens": "def format_timedelta ( td_object ) : def get_total_seconds ( td ) : return ( td . microseconds + ( td . seconds + td . days * 24 * 3600 ) * 1e6 ) / 1e6 seconds = int ( get_total_seconds ( td_object ) ) periods = [ ( 'year' , 60 * 60 * 24 * 365 ) , ( 'month' , 60 * 60 * 24 * 30 ) , ( 'day' , 60 * 60 * 24 ) , ( 'hour' , 60 * 60 ) , ( 'minute' , 60 ) , ( 'second' , 1 ) ] strings = [ ] for period_name , period_seconds in periods : if seconds > period_seconds : period_value , seconds = divmod ( seconds , period_seconds ) if period_value == 1 : strings . append ( \"%s %s\"  % ( period_value , period_name ) ) else : strings . append ( \"%s %ss\" % ( period_value , period_name ) ) return \", \" . join ( strings )"}
{"code_tokens": "def _assert_all_finite ( X ) : X = np . asanyarray ( X ) if ( X . dtype . char in np . typecodes [ 'AllFloat' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) :  raise ValueError ( \"Input contains NaN, infinity\" \" or a value too large for %r.\" % X . dtype )"}
{"code_tokens": "def _warn_if_not_finite ( X ) : X = np . asanyarray ( X ) if ( X . dtype . char in np . typecodes [ 'AllFloat' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite (  X ) . all ( ) ) : warnings . warn ( \"Result contains NaN, infinity\" \" or a value too large for %r.\" % X . dtype , category = UserWarning )"}
{"code_tokens": "def fromdict ( cls , config , check_fields = True ) : m = super ( Config , cls ) . __new__ ( cls ) m . path = '.' m . verbose = False m . config = m .  _merge_defaults ( config ) if check_fields : m . _check_fields ( ) return m"}
{"code_tokens": "def sha1 ( self ) : with open ( self . path , 'rb' ) as f : return hashlib . sha1 ( f .  read ( ) ) . hexdigest ( )"}
{"code_tokens": "def plot_3 ( data , ss , * args ) : if len ( data ) <= 1 : warnings . warn ( \"Only one datapoint. Could not compute t-SNE embedding.\" ) return None scores = np . array ( [ d [ 'mean_test_score' ] for d in data ] ) warped = np . array ( [ ss . point_to_unit ( d [ 'parameters' ] ) for d in data ] ) X = TSNE ( n_components = 2 ) . fit_transform ( warped ) e_scores = np . exp ( scores ) mine , maxe = np . min ( e_scores ) , np . max ( e_scores ) color = ( e_scores - mine ) / ( maxe - mine ) mapped_colors = list ( map ( rgb2hex , cm . get_cmap ( 'RdBu_r' ) ( color ) ) ) p = bk . figure ( title = 't-SNE (unsupervised)' , tools = TOOLS ) df_params = nonconstant_parameters ( data ) df_params [ 'score' ] = scores df_params [ 'x' ] = X [ : , 0 ] df_params [ 'y' ] = X [ : , 1 ] df_params [ 'color' ] = mapped_colors df_params [ 'radius' ] = 1 p . circle ( x = 'x' , y = 'y' , color = 'color' , radius = 'radius' , source = ColumnDataSource ( data = df_params ) , fill_alpha = 0.6 , line_color = None ) cp = p hover = cp . select ( dict ( type = HoverTool ) ) format_tt = [ ( s , '@%s' % s ) for s in df_params . columns ] hover . tooltips = OrderedDict ( [ ( \"index\" ,  \"$index\" ) ] + format_tt ) xax , yax = p . axis xax . axis_label = 't-SNE coord 1' yax . axis_label = 't-SNE coord 2' return p"}
{"code_tokens": "def plot_4 ( data , * args ) : params = nonconstant_parameters ( data ) scores = np . array ( [ d [ 'mean_test_score' ] for d in data ] ) order = np . argsort ( scores ) for key in params . keys ( ) : if params [ key ] . dtype == np . dtype ( 'bool' ) : params [ key ] = params [ key ] . astype ( np . int ) p_list = [ ] for key in params . keys ( ) : x = params [ key ] [ order ] y = scores [ order ] params = params . loc [ order ] try : radius = ( np . max ( x ) - np . min ( x ) ) / 100.0 except : print ( \"error making plot4 for '%s'\" % key ) continue p_list . append ( build_scatter_tooltip ( x = x , y = y , radius = radius , add_line = False , tt = params , xlabel =  key , title = 'Score vs %s' % key ) ) return p_list"}
{"code_tokens": "def add_int ( self , name , min , max , warp = None ) : min , max = map ( int , ( min , max ) ) if max < min : raise ValueError ( 'variable %s: max < min error' % name ) if warp not in ( None , 'log' ) : raise ValueError ( 'variable %s: warp=%s is not  supported. use ' 'None or \"log\",' % ( name , warp ) ) if min <= 0 and warp == 'log' : raise ValueError ( 'variable %s: log-warping requires min > 0' ) self . variables [ name ] = IntVariable ( name , min , max , warp )"}
{"code_tokens": "def add_float ( self , name , min , max , warp = None ) : min , max = map ( float , ( min , max ) ) if not min < max : raise ValueError ( 'variable %s: min >= max error' % name ) if warp not in ( None , 'log' ) : raise ValueError ( 'variable %s: warp=%s is not supported. use ' 'None or \"log\",' % ( name , warp ) ) if min  <= 0 and warp == 'log' : raise ValueError ( 'variable %s: log-warping requires min > 0' ) self . variables [ name ] = FloatVariable ( name , min , max , warp )"}
{"code_tokens": "def add_enum ( self , name , choices ) : if not isinstance ( choices , Iterable ) : raise ValueError ( 'variable %s: choices must be iterable' % name ) self . variables [ name ] =  EnumVariable ( name , choices )"}
{"code_tokens": "def log_callback ( wrapped_function ) : def debug_log ( message ) : logger . debug ( message . encode ( 'unicode_escape' ) . decode ( ) ) @ functools . wraps ( wrapped_function ) def _wrapper ( parser , match , ** kwargs ) : func_name = wrapped_function . __name__ debug_log ( u'{func_name} <- {matched_string}' . format ( func_name = func_name , matched_string = match . group ( ) , ) ) try : result = wrapped_function ( parser , match , ** kwargs ) except IgnoredMatchException : debug_log ( u'{func_name} -> IGNORED' . format ( func_name  = func_name ) ) raise debug_log ( u'{func_name} -> {result}' . format ( func_name = func_name , result = result , ) ) return result return _wrapper"}
{"code_tokens": "def find_match ( self ) : for pattern , callback in self . rules : match = pattern . match ( self . source , pos = self . pos ) if not match : continue try : node = callback ( match ) except IgnoredMatchException : pass else : self . seen . append  ( node ) return match raise NoMatchException ( 'None of the known patterns match for {}' '' . format ( self . source [ self . pos : ] ) )"}
{"code_tokens": "def add_child ( self , child ) : if not isinstance ( child , ChildMixin ) : raise TypeError ( 'Requires instance of TreeElement. ' 'Got {}' . format ( type ( child ) ) ) child . parent = self self . _children . append ( child  )"}
{"code_tokens": "def get_ip_packet ( data , client_port , server_port , is_loopback = False ) : header = _loopback if is_loopback else _ethernet try : header . unpack ( data ) except Exception as ex : raise ValueError ( 'Bad header: %s' % ex ) tcp_p = getattr ( header . data , 'data' , None ) if type ( tcp_p ) != dpkt . tcp . TCP : raise ValueError ( 'Not a TCP packet' ) if tcp_p . dport == server_port : if client_port != 0 and tcp_p . sport != client_port : raise ValueError ( 'Request from different client' ) elif tcp_p . sport == server_port : if client_port != 0 and tcp_p . dport != client_port  : raise ValueError ( 'Reply for different client' ) else : raise ValueError ( 'Packet not for/from client/server' ) return header . data"}
{"code_tokens": "def report ( self ) : self . _output . write ( '\\r' ) sort_by = 'avg' results = { } for key , latencies in self . _latencies_by_method . items ( ) : result = { } result [ 'count' ] = len ( latencies ) result [ 'avg' ] = sum ( latencies ) / len ( latencies ) result [ 'min' ] = min ( latencies ) result [ 'max' ] = max ( latencies ) latencies = sorted ( latencies ) result [ 'p90' ] = percentile ( latencies , 0.90 ) result [ 'p95' ] = percentile ( latencies , 0.95 ) result [ 'p99' ] = percentile ( latencies , 0.99 ) result [ 'p999' ] = percentile ( latencies , 0.999 ) results  [ key ] = result headers = [ 'method' , 'count' , 'avg' , 'min' , 'max' , 'p90' , 'p95' , 'p99' , 'p999' ] data = [ ] results = sorted ( results . items ( ) , key = lambda it : it [ 1 ] [ sort_by ] , reverse = True ) def row ( key , res ) : data = [ key ] + [ res [ header ] for header in headers [ 1 : ] ] return tuple ( data ) data = [ row ( key , result ) for key , result in results ] self . _output . write ( '%s\\n' % tabulate ( data , headers = headers ) ) self . _output . flush ( )"}
{"code_tokens": "def of_structs ( cls , a , b ) : t_diff = ThriftDiff ( a , b ) t_diff . _do_diff  ( ) return t_diff"}
{"code_tokens": "def of_messages ( cls , msg_a , msg_b ) : ok_to_diff , reason = cls . can_diff ( msg_a , msg_b ) if not ok_to_diff : raise ValueError ( reason ) return [ cls  . of_structs ( x . value , y . value ) for x , y in zip ( msg_a . args , msg_b . args ) if x . field_type == 'struct' ]"}
{"code_tokens": "def can_diff ( msg_a , msg_b ) : if msg_a . method != msg_b . method : return False , 'method name of messages do not match' if len ( msg_a . args ) != len ( msg_b . args ) or not msg_a . args . is_isomorphic_to (  msg_b . args ) : return False , 'argument signature of methods do not match' return True , None"}
{"code_tokens": "def is_isomorphic_to ( self , other ) : return ( isinstance ( other , self . __class__ ) and len ( self . fields ) == len ( other . fields ) and all ( a . is_isomorphic_to ( b ) for a , b in zip ( self  . fields , other . fields ) ) )"}
{"code_tokens": "def read ( cls , data , protocol = None , fallback_protocol = TBinaryProtocol , finagle_thrift = False , max_fields = MAX_FIELDS , max_list_size = MAX_LIST_SIZE , max_map_size = MAX_MAP_SIZE , max_set_size = MAX_SET_SIZE , read_values = False ) : if len ( data ) < cls . MIN_MESSAGE_SIZE : raise ValueError ( 'not enough data' ) if protocol is None : protocol = cls . detect_protocol ( data , fallback_protocol ) trans = TTransport . TMemoryBuffer ( data ) proto = protocol ( trans ) header = None if finagle_thrift : try : header = ThriftStruct . read ( proto , max_fields , max_list_size , max_map_size , max_set_size , read_values ) except : trans = TTransport . TMemoryBuffer ( data ) proto = protocol ( trans ) method , mtype , seqid = proto . readMessageBegin ( ) mtype = cls . message_type_to_str ( mtype ) if len ( method ) == 0 or method . isspace ( ) or method . startswith ( ' ' ) : raise ValueError ( 'no method name' ) if len ( method ) > cls . MAX_METHOD_LENGTH : raise ValueError ( 'method name too long' ) valid = range ( 33 , 127 ) if any ( ord ( char ) not in valid for char in method ) : raise ValueError ( 'invalid method name' % method ) args = ThriftStruct . read ( proto , max_fields , max_list_size , max_map_size , max_set_size , read_values ) proto . readMessageEnd ( ) msglen =  trans . _buffer . tell ( ) return cls ( method , mtype , seqid , args , header , msglen ) , msglen"}
{"code_tokens": "def pop ( self , nbytes ) : size = 0 popped = [ ] with self . _lock_packets : while size < nbytes : try : packet = self . _packets . pop ( 0 )  size += len ( packet . data . data ) self . _remaining -= len ( packet . data . data ) popped . append ( packet ) except IndexError : break return popped"}
{"code_tokens": "def pop_data ( self , nbytes ) : last_timestamp = 0 data = [ ] for packet in self . pop ( nbytes ) : last_timestamp = packet . timestamp data . append ( packet . data .  data ) return '' . join ( data ) , last_timestamp"}
{"code_tokens": "def push ( self , ip_packet ) : data_len = len ( ip_packet . data . data ) seq_id = ip_packet . data . seq if data_len == 0 : self . _next_seq_id = seq_id return False if self . _next_seq_id != - 1 and seq_id != self . _next_seq_id : return False self . _next_seq_id = seq_id + data_len with self . _lock_packets : self . _length += len ( ip_packet . data . data ) self . _remaining += len  ( ip_packet . data . data ) self . _packets . append ( ip_packet ) return True"}
{"code_tokens": "def run ( self , * args , ** kwargs ) : while True : try : timestamp , ip_p = self . _queue . popleft ( ) src_ip = get_ip ( ip_p , ip_p . src ) dst_ip = get_ip ( ip_p , ip_p . dst ) src = intern ( '%s:%s' % ( src_ip , ip_p . data . sport ) ) dst = intern ( '%s:%s' % ( dst_ip , ip_p . data . dport ) ) key = intern ( '%s<->%s' % ( src , dst ) ) stream = self . _streams . get ( key ) if stream is None : stream = Stream ( src , dst ) self . _streams [ key  ] = stream setattr ( ip_p , 'timestamp' , timestamp ) pushed = stream . push ( ip_p ) if not pushed : continue for handler in self . _handlers : try : handler ( stream ) except Exception as ex : print ( 'handler exception: %s' % ex ) except Exception : time . sleep ( 0.00001 )"}
{"code_tokens": "def get_disk_image_by_name ( pbclient , location , image_name ) : all_images = pbclient . list_images ( ) matching = [ i for i in all_images [ 'items' ] if i [ 'properties' ] [ 'name' ] == image_name and i [ 'properties' ] [ 'imageType' ] == \"HDD\" and i [  'properties' ] [ 'location' ] == location ] return matching"}
{"code_tokens": "def _read_config ( self , filename = None ) : if filename : self . _config_filename = filename else : try : import appdirs except ImportError : raise Exception ( \"Missing dependency for determining config path. Please install \" \"the 'appdirs' Python module.\" ) self . _config_filename = appdirs . user_config_dir ( _LIBRARY_NAME , \"ProfitBricks\" ) +  \".ini\" if not self . _config : self . _config = configparser . ConfigParser ( ) self . _config . optionxform = str self . _config . read ( self . _config_filename )"}
{"code_tokens": "def _save_config ( self , filename = None ) : if filename is None : filename = self . _config_filename parent_path = os . path . dirname ( filename ) if not os . path . isdir ( parent_path ) : os . makedirs ( parent_path ) with open ( filename , \"w\" ) as configfile : self . _config  . write ( configfile )"}
{"code_tokens": "def _get_username ( self , username = None , use_config = True , config_filename = None ) : if not username and use_config : if self . _config is None : self . _read_config ( config_filename ) username = self . _config . get ( \"credentials\" , \"username\" , fallback = None ) if not username : username = input ( \"Please enter your username: \" ) . strip ( ) while not username : username = input  ( \"No username specified. Please enter your username: \" ) . strip ( ) if 'credendials' not in self . _config : self . _config . add_section ( 'credentials' ) self . _config . set ( \"credentials\" , \"username\" , username ) self . _save_config ( ) return username"}
{"code_tokens": "def _get_password ( self , password , use_config = True , config_filename = None , use_keyring = HAS_KEYRING ) : if not password and use_config : if self . _config is None : self . _read_config ( config_filename ) password = self . _config . get ( \"credentials\" , \"password\" , fallback = None ) if not password and use_keyring : logger = logging . getLogger ( __name__ ) question = ( \"Please enter your password for {} on {}: \" . format ( self . username , self . host_base ) ) if HAS_KEYRING : password = keyring . get_password ( self . keyring_identificator , self . username ) if password is None : password = getpass . getpass ( question ) try : keyring . set_password ( self . keyring_identificator , self . username , password ) except keyring . errors . PasswordSetError as error : logger . warning ( \"Storing password in keyring '%s' failed: %s\" , self . keyring_identificator , error ) else : logger . warning ( \"Install the 'keyring' Python module to store your password \" \"securely in your keyring!\" ) password = self . _config . get ( \"credentials\" , \"password\" , fallback = None ) if password is None : password = getpass . getpass ( question ) store_plaintext_passwords = self . _config .  get ( \"preferences\" , \"store-plaintext-passwords\" , fallback = None ) if store_plaintext_passwords != \"no\" : question = ( \"Do you want to store your password in plain text in \" + self . _config_filename ( ) ) answer = ask ( question , [ \"yes\" , \"no\" , \"never\" ] , \"no\" ) if answer == \"yes\" : self . _config . set ( \"credentials\" , \"password\" , password ) self . _save_config ( ) elif answer == \"never\" : if \"preferences\" not in self . _config : self . _config . add_section ( \"preferences\" ) self . _config . set ( \"preferences\" , \"store-plaintext-passwords\" , \"no\" ) self . _save_config ( ) return password"}
{"code_tokens": "def get_datacenter ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s?depth=%s' % ( datacenter_id , str ( depth ) )  ) return response"}
{"code_tokens": "def get_datacenter_by_name ( self , name , depth = 1 ) : all_data_centers = self . list_datacenters ( depth = depth ) [ 'items' ] data_center = find_item_by_name ( all_data_centers , lambda i : i [ 'properties' ] [ 'name' ] , name ) if not data_center : raise NameError ( \"No data center found with name \" \"containing '{name}'.\" . format ( name = name ) ) if len ( data_center ) > 1 : raise NameError ( \"Found {n} data centers with the name '{name}': {names}\" .  format ( n = len ( data_center ) , name = name , names = \", \" . join ( d [ 'properties' ] [ 'name' ] for d in data_center ) ) ) return data_center [ 0 ]"}
{"code_tokens": "def delete_datacenter ( self , datacenter_id ) : response = self . _perform_request ( url = '/datacenters/%s' % ( datacenter_id ) , method  = 'DELETE' ) return response"}
{"code_tokens": "def get_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule_id ) : response = self  . _perform_request ( '/datacenters/%s/servers/%s/nics/%s/firewallrules/%s' % ( datacenter_id , server_id , nic_id , firewall_rule_id ) ) return response"}
{"code_tokens": "def delete_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules/%s' % ( datacenter_id , server_id , nic_id , firewall_rule_id  ) , method = 'DELETE' ) return response"}
{"code_tokens": "def create_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule ) : properties = { \"name\" : firewall_rule . name } if firewall_rule . protocol : properties [ 'protocol' ] = firewall_rule . protocol if firewall_rule . source_mac : properties [ 'sourceMac' ] = firewall_rule . source_mac if firewall_rule . source_ip : properties [ 'sourceIp' ] = firewall_rule . source_ip if firewall_rule . target_ip : properties [ 'targetIp' ] = firewall_rule . target_ip if firewall_rule . port_range_start : properties [ 'portRangeStart' ] = firewall_rule . port_range_start if firewall_rule . port_range_end : properties [ 'portRangeEnd' ] = firewall_rule . port_range_end if firewall_rule . icmp_type : properties [ 'icmpType' ] = firewall_rule . icmp_type if firewall_rule . icmp_code : properties [ 'icmpCode' ] = firewall_rule . icmp_code data = { \"properties\" : properties } response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules' % ( datacenter_id , server_id , nic_id ) , method = 'POST' , data = json . dumps ( data ) )  return response"}
{"code_tokens": "def update_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value if attr == 'source_mac' : data [ 'sourceMac' ] = value elif attr == 'source_ip' : data [ 'sourceIp' ] = value elif attr == 'target_ip' : data [ 'targetIp' ] = value elif attr == 'port_range_start' : data [ 'portRangeStart' ] = value elif attr == 'port_range_end' : data [ 'portRangeEnd' ] = value elif attr == 'icmp_type' : data [ 'icmpType' ] = value elif attr == 'icmp_code' : data [ 'icmpCode' ] = value else : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules/%s' % ( datacenter_id , server_id , nic_id , firewall_rule_id ) , method = 'PATCH' , data = json  . dumps ( data ) ) return response"}
{"code_tokens": "def delete_image ( self , image_id ) : response = self . _perform_request ( url = '/images/' + image_id , method = 'DELETE' ) return response "}
{"code_tokens": "def update_image ( self , image_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] =  value response = self . _perform_request ( url = '/images/' + image_id , method = 'PATCH' , data = json . dumps ( data ) ) return response"}
{"code_tokens": "def delete_ipblock ( self , ipblock_id ) : response = self . _perform_request ( url =  '/ipblocks/' + ipblock_id , method = 'DELETE' ) return response"}
{"code_tokens": "def reserve_ipblock ( self , ipblock ) : properties = { \"name\" : ipblock . name } if ipblock . location : properties [ 'location' ] = ipblock . location if ipblock . size : properties [ 'size' ] = str ( ipblock . size ) raw = { \"properties\" : properties , } response = self . _perform_request ( url = '/ipblocks' , method =  'POST' , data = json . dumps ( raw ) ) return response"}
{"code_tokens": "def get_lan ( self , datacenter_id , lan_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans/%s?depth=%s' % ( datacenter_id , lan_id , str ( depth  ) ) ) return response"}
{"code_tokens": "def list_lans ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans?depth=%s' % ( datacenter_id , str ( depth )  ) ) return response"}
{"code_tokens": "def delete_lan ( self , datacenter_id , lan_id ) : response = self . _perform_request ( url = '/datacenters/%s/lans/%s' % ( datacenter_id , lan_id )  , method = 'DELETE' ) return response"}
{"code_tokens": "def create_lan ( self , datacenter_id , lan ) : data = json . dumps ( self . _create_lan_dict ( lan ) ) response =  self . _perform_request ( url = '/datacenters/%s/lans' % datacenter_id , method = 'POST' , data = data ) return response"}
{"code_tokens": "def update_lan ( self , datacenter_id , lan_id , name = None , public = None , ip_failover = None ) : data = { } if name : data [ 'name' ] = name if public is not None : data [ 'public' ]  = public if ip_failover : data [ 'ipFailover' ] = ip_failover response = self . _perform_request ( url = '/datacenters/%s/lans/%s' % ( datacenter_id , lan_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response"}
{"code_tokens": "def get_lan_members ( self , datacenter_id , lan_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans/%s/nics?depth=%s' % (  datacenter_id , lan_id , str ( depth ) ) ) return response"}
{"code_tokens": "def get_loadbalancer ( self , datacenter_id , loadbalancer_id ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers/%s' % ( datacenter_id ,  loadbalancer_id ) ) return response"}
{"code_tokens": "def list_loadbalancers ( self , datacenter_id , depth = 1 ) : response = self . _perform_request  ( '/datacenters/%s/loadbalancers?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response"}
{"code_tokens": "def delete_loadbalancer ( self , datacenter_id , loadbalancer_id ) : response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) , method = 'DELETE'  ) return response"}
{"code_tokens": "def create_loadbalancer ( self , datacenter_id , loadbalancer ) : data = json . dumps ( self . _create_loadbalancer_dict ( loadbalancer ) )  response = self . _perform_request ( url = '/datacenters/%s/loadbalancers' % datacenter_id , method = 'POST' , data = data ) return response"}
{"code_tokens": "def update_loadbalancer ( self , datacenter_id , loadbalancer_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s' %  ( datacenter_id , loadbalancer_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response"}
{"code_tokens": "def get_loadbalancer_members ( self , datacenter_id , loadbalancer_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers/%s/balancednics?depth=%s' % ( datacenter_id , loadbalancer_id , str ( depth )  ) ) return response"}
{"code_tokens": "def add_loadbalanced_nics ( self , datacenter_id , loadbalancer_id , nic_id ) : data = '{ \"id\": \"' + nic_id + '\" }' response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s/balancednics' % ( datacenter_id , loadbalancer_id ) ,  method = 'POST' , data = data ) return response"}
{"code_tokens": "def get_loadbalanced_nic ( self , datacenter_id , loadbalancer_id , nic_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers/%s/balancednics/%s?depth=%s' % ( datacenter_id , loadbalancer_id , nic_id , str ( depth ) )  ) return response"}
{"code_tokens": "def remove_loadbalanced_nic ( self , datacenter_id , loadbalancer_id , nic_id ) : response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s/balancednics/%s' % ( datacenter_id , loadbalancer_id , nic_id ) , method = 'DELETE' ) return  response"}
{"code_tokens": "def get_location ( self , location_id , depth = 0 ) : response = self  . _perform_request ( '/locations/%s?depth=%s' % ( location_id , depth ) ) return response"}
{"code_tokens": "def get_nic ( self , datacenter_id , server_id , nic_id , depth = 1 ) : response = self .  _perform_request ( '/datacenters/%s/servers/%s/nics/%s?depth=%s' % ( datacenter_id , server_id , nic_id , str ( depth ) ) ) return response"}
{"code_tokens": "def list_nics ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/nics?depth=%s' % ( datacenter_id , server_id , str ( depth  ) ) ) return response"}
{"code_tokens": "def delete_nic ( self , datacenter_id , server_id , nic_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s' % ( datacenter_id , server_id , nic_id  ) , method = 'DELETE' ) return response"}
{"code_tokens": "def create_nic ( self , datacenter_id , server_id , nic ) : data = json . dumps ( self . _create_nic_dict ( nic ) ) response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics' % ( datacenter_id , server_id ) , method = 'POST' , data = data )  return response"}
{"code_tokens": "def update_nic ( self , datacenter_id , server_id , nic_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s' % ( datacenter_id , server_id , nic_id )  , method = 'PATCH' , data = json . dumps ( data ) ) return response"}
{"code_tokens": "def get_request ( self , request_id , status = False ) : if status : response = self . _perform_request ( '/requests/' + request_id + '/status' ) else : response  = self . _perform_request ( '/requests/%s' % request_id ) return response"}
{"code_tokens": "def get_server ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s?depth=%s' % ( datacenter_id , server_id , str  ( depth ) ) ) return response"}
{"code_tokens": "def list_servers ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers?depth=%s' % ( datacenter_id , str ( depth ) ) )  return response"}
{"code_tokens": "def delete_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s' % ( datacenter_id , server_id )  , method = 'DELETE' ) return response"}
{"code_tokens": "def create_server ( self , datacenter_id , server ) : data = json . dumps ( self . _create_server_dict ( server ) ) response = self . _perform_request ( url = '/datacenters/%s/servers' % ( datacenter_id ) , method =  'POST' , data = data ) return response"}
{"code_tokens": "def update_server ( self , datacenter_id , server_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : if attr == 'boot_volume' : boot_volume_properties = { \"id\" : value } boot_volume_entities = { \"bootVolume\" : boot_volume_properties } data . update ( boot_volume_entities ) else : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self .  _perform_request ( url = '/datacenters/%s/servers/%s' % ( datacenter_id , server_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response"}
{"code_tokens": "def get_attached_volumes ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/volumes?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return  response"}
{"code_tokens": "def get_attached_volume ( self , datacenter_id , server_id , volume_id ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/volumes/%s' % ( datacenter_id ,  server_id , volume_id ) ) return response"}
{"code_tokens": "def attach_volume ( self , datacenter_id , server_id , volume_id ) : data = '{ \"id\": \"' + volume_id + '\" }' response = self . _perform_request ( url = '/datacenters/%s/servers/%s/volumes' %  ( datacenter_id , server_id ) , method = 'POST' , data = data ) return response"}
{"code_tokens": "def get_attached_cdroms ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/cdroms?depth=%s' % ( datacenter_id , server_id , str ( depth  ) ) ) return response"}
{"code_tokens": "def get_attached_cdrom ( self , datacenter_id , server_id , cdrom_id ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/cdroms/%s' % ( datacenter_id , server_id , cdrom_id ) )  return response"}
{"code_tokens": "def attach_cdrom ( self , datacenter_id , server_id , cdrom_id ) : data = '{ \"id\": \"' + cdrom_id + '\" }' response = self . _perform_request ( url = '/datacenters/%s/servers/%s/cdroms' % ( datacenter_id , server_id ) ,  method = 'POST' , data = data ) return response"}
{"code_tokens": "def start_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/start' % ( datacenter_id ,  server_id ) , method = 'POST-ACTION' ) return response"}
{"code_tokens": "def stop_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/stop' % ( datacenter_id ,  server_id ) , method = 'POST-ACTION' ) return response"}
{"code_tokens": "def reboot_server ( self , datacenter_id , server_id ) : response = self . _perform_request (  url = '/datacenters/%s/servers/%s/reboot' % ( datacenter_id , server_id ) , method = 'POST-ACTION' ) return response"}
{"code_tokens": "def create_snapshot ( self , datacenter_id , volume_id , name = None , description = None ) : data = { 'name' : name , 'description' : description  } response = self . _perform_request ( '/datacenters/%s/volumes/%s/create-snapshot' % ( datacenter_id , volume_id ) , method = 'POST-ACTION-JSON' , data = urlencode ( data ) ) return response"}
{"code_tokens": "def restore_snapshot ( self , datacenter_id , volume_id , snapshot_id ) : data = { 'snapshotId' : snapshot_id } response = self . _perform_request ( url  = '/datacenters/%s/volumes/%s/restore-snapshot' % ( datacenter_id , volume_id ) , method = 'POST-ACTION' , data = urlencode ( data ) ) return response"}
{"code_tokens": "def remove_snapshot ( self , snapshot_id ) : response = self . _perform_request ( url = '/snapshots/' + snapshot_id  , method = 'DELETE' ) return response"}
{"code_tokens": "def get_group ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s?depth=%s' % ( group_id  , str ( depth ) ) ) return response"}
{"code_tokens": "def create_group ( self , group ) : data = json . dumps ( self . _create_group_dict ( group ) ) response = self . _perform_request ( url  = '/um/groups' , method = 'POST' , data = data ) return response"}
{"code_tokens": "def update_group ( self , group_id , ** kwargs ) : properties = { } if 'create_datacenter' in kwargs : kwargs [ 'create_data_center' ] = kwargs . pop ( 'create_datacenter' ) for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { \"properties\" : properties } response = self . _perform_request (  url = '/um/groups/%s' % group_id , method = 'PUT' , data = json . dumps ( data ) ) return response"}
{"code_tokens": "def delete_group ( self , group_id ) : response = self . _perform_request ( url =  '/um/groups/%s' % group_id , method = 'DELETE' ) return response"}
{"code_tokens": "def list_shares ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/shares?depth=%s' % ( group_id , str ( depth  ) ) ) return response"}
{"code_tokens": "def get_share ( self , group_id , resource_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/shares/%s?depth=%s' % ( group_id , resource_id , str  ( depth ) ) ) return response"}
{"code_tokens": "def add_share ( self , group_id , resource_id , ** kwargs ) : properties = { } for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { \"properties\" : properties } response = self . _perform_request ( url =  '/um/groups/%s/shares/%s' % ( group_id , resource_id ) , method = 'POST' , data = json . dumps ( data ) ) return response"}
{"code_tokens": "def delete_share ( self , group_id , resource_id ) : response = self . _perform_request ( url = '/um/groups/%s/shares/%s' % ( group_id , resource_id ) , method =  'DELETE' ) return response"}
{"code_tokens": "def get_user ( self , user_id , depth = 1 ) : response = self . _perform_request ( '/um/users/%s?depth=%s' % ( user_id , str  ( depth ) ) ) return response"}
{"code_tokens": "def create_user ( self , user ) : data = self . _create_user_dict ( user = user ) response = self . _perform_request ( url = '/um/users' , method = 'POST' , data = json . dumps  ( data ) ) return response"}
{"code_tokens": "def update_user ( self , user_id , ** kwargs ) : properties = { } for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { \"properties\" : properties } response = self . _perform_request ( url = '/um/users/%s' % user_id , method = 'PUT' , data = json . dumps  ( data ) ) return response"}
{"code_tokens": "def delete_user ( self , user_id ) : response = self . _perform_request ( url =  '/um/users/%s' % user_id , method = 'DELETE' ) return response"}
{"code_tokens": "def list_group_users ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/users?depth=%s' % ( group_id , str  ( depth ) ) ) return response"}
{"code_tokens": "def add_group_user ( self , group_id , user_id ) : data = { \"id\" : user_id } response = self . _perform_request ( url = '/um/groups/%s/users' % group_id , method = 'POST' , data = json . dumps (  data ) ) return response"}
{"code_tokens": "def remove_group_user ( self , group_id , user_id ) : response = self . _perform_request ( url = '/um/groups/%s/users/%s' % ( group_id , user_id  ) , method = 'DELETE' ) return response"}
{"code_tokens": "def list_resources ( self , resource_type = None , depth = 1 ) : if resource_type is not None : response = self . _perform_request ( '/um/resources/%s?depth=%s' % ( resource_type , str ( depth ) ) ) else : response = self . _perform_request ( '/um/resources?depth=' + str ( depth ) ) return  response"}
{"code_tokens": "def get_resource ( self , resource_type , resource_id , depth = 1 ) : response = self . _perform_request ( '/um/resources/%s/%s?depth=%s' % ( resource_type , resource_id , str ( depth ) ) )  return response"}
{"code_tokens": "def get_volume ( self , datacenter_id , volume_id ) : response = self . _perform_request ( '/datacenters/%s/volumes/%s' % ( datacenter_id , volume_id ) )  return response"}
{"code_tokens": "def list_volumes ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/volumes?depth=%s' % ( datacenter_id , str ( depth  ) ) ) return response"}
{"code_tokens": "def delete_volume ( self , datacenter_id , volume_id ) : response = self . _perform_request ( url = '/datacenters/%s/volumes/%s' % ( datacenter_id , volume_id ) , method  = 'DELETE' ) return response"}
{"code_tokens": "def create_volume ( self , datacenter_id , volume ) : data = ( json . dumps ( self . _create_volume_dict ( volume ) ) ) response = self . _perform_request ( url = '/datacenters/%s/volumes' % datacenter_id  , method = 'POST' , data = data ) return response"}
{"code_tokens": "def wait_for_completion ( self , response , timeout = 3600 , initial_wait = 5 , scaleup = 10 ) : if not response : return logger = logging . getLogger ( __name__ ) wait_period = initial_wait next_increase = time . time ( ) + wait_period * scaleup if timeout : timeout = time . time ( ) + timeout while True : request = self . get_request ( request_id = response [ 'requestId' ] , status = True ) if request [ 'metadata' ] [ 'status' ] == 'DONE' : break elif request [ 'metadata' ] [ 'status' ] == 'FAILED' : raise PBFailedRequest ( 'Request {0} failed to complete: {1}' . format ( response [ 'requestId' ] , request [ 'metadata' ] [ 'message' ] ) , response [ 'requestId' ] )  current_time = time . time ( ) if timeout and current_time > timeout : raise PBTimeoutError ( 'Timed out waiting for request {0}.' . format ( response [ 'requestId' ] ) , response [ 'requestId' ] ) if current_time > next_increase : wait_period *= 2 next_increase = time . time ( ) + wait_period * scaleup scaleup *= 2 logger . info ( \"Request %s is in state '%s'. Sleeping for %i seconds...\" , response [ 'requestId' ] , request [ 'metadata' ] [ 'status' ] , wait_period ) time . sleep ( wait_period )"}
{"code_tokens": "def _b ( s , encoding = 'utf-8' ) : if six . PY2 : if isinstance ( s , str ) : return s elif isinstance ( s , unicode ) : return s . encode ( encoding ) else : if isinstance ( s , bytes ) : return s elif isinstance ( s , str ) : return s . encode ( encoding )  raise TypeError ( \"Invalid argument %r for _b()\" % ( s , ) )"}
{"code_tokens": "def _underscore_to_camelcase ( value ) : def camelcase ( ) : yield str . lower while True : yield str . capitalize c = camelcase ( ) return \"\" . join  ( next ( c ) ( x ) if x else '_' for x in value . split ( \"_\" ) )"}
{"code_tokens": "def find_item_by_name ( list_ , namegetter , name ) : matching_items = [ i for i in list_ if namegetter ( i ) == name ] if not matching_items : prog = re . compile ( re . escape ( name ) + '$' , re . IGNORECASE ) matching_items = [ i for i in list_ if prog . match ( namegetter ( i ) ) ] if not matching_items : prog = re . compile ( re . escape ( name ) ) matching_items = [ i for i in list_ if prog . match ( namegetter ( i ) ) ] if not matching_items : prog = re . compile ( re . escape ( name ) , re . IGNORECASE ) matching_items = [ i for i in list_ if prog . match ( namegetter ( i ) ) ] if not matching_items : prog = re . compile ( re . escape ( name ) ) matching_items = [ i for i in list_ if prog .  search ( namegetter ( i ) ) ] if not matching_items : prog = re . compile ( re . escape ( name ) , re . IGNORECASE ) matching_items = [ i for i in list_ if prog . search ( namegetter ( i ) ) ] return matching_items"}
{"code_tokens": "def getServerInfo ( pbclient = None , dc_id = None ) : if pbclient is None : raise ValueError ( \"argument 'pbclient' must not be None\" ) if dc_id is None : raise ValueError ( \"argument 'dc_id' must not be None\" ) server_info = [ ] servers = pbclient . list_servers ( dc_id , 1 ) for server in servers [ 'items' ] : props = server [ 'properties' ] info = dict ( id = server [ 'id' ] , name = props [ 'name' ] , state = server [ 'metadata' ] [ 'state'  ] , vmstate = props [ 'vmState' ] ) server_info . append ( info ) return server_info"}
{"code_tokens": "def getServerStates ( pbclient = None , dc_id = None , serverid = None , servername = None ) : if pbclient is None : raise ValueError ( \"argument 'pbclient' must not be None\" ) if dc_id is None : raise ValueError ( \"argument 'dc_id' must not be None\" ) server = None if serverid is None : if servername is None : raise ValueError ( \"one of 'serverid' or 'servername' must be specified\" ) server_info = select_where ( getServerInfo ( pbclient , dc_id ) , [ 'id' , 'name' , 'state' , 'vmstate' ] , name = servername ) if len ( server_info ) > 1 : raise NameError ( \"ambiguous server name '{}'\" . format ( servername ) ) if len ( server_info ) == 1 : server = server_info [ 0 ] else : try : server_info = pbclient . get_server ( dc_id , serverid , 1 ) server = dict ( id = server_info [ 'id' ] , name = server_info [ 'properties' ] [ 'name' ] , state = server_info [ 'metadata' ] [ 'state' ] , vmstate = server_info [ 'properties' ] [ 'vmState' ] ) except Exception : ex = sys . exc_info ( ) [ 1 ] if ex . args [ 0 ] is not None and ex . args [ 0 ] == 404 : print  ( \"Server w/ ID {} not found\" . format ( serverid ) ) server = None else : raise ex return server"}
{"code_tokens": "def get_self ( session , user_details = None ) : if user_details : user_details [ 'compact' ] = True response = make_get_request ( session , 'self' , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return  json_data [ 'result' ] else : raise SelfNotRetrievedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def get_user_by_id ( session , user_id , user_details = None ) : if user_details : user_details [ 'compact' ] = True response = make_get_request ( session , 'users/{}' . format ( user_id ) , params_data = user_details ) json_data = response . json ( ) if  response . status_code == 200 : return json_data [ 'result' ] else : raise UserNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def get_self_user_id ( session ) : response = make_get_request ( session , 'self' ) if response . status_code == 200 : return response . json ( ) [ 'result' ] [ 'id' ] else : raise UserIdNotRetrievedException (  'Error retrieving user id: %s' % response . text , response . text )"}
{"code_tokens": "def add_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_post_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotAddedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] ) "}
{"code_tokens": "def set_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_put_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotSetException ( message = json_data [ 'message' ] , error_code =  json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def delete_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_delete_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ]  else : raise UserJobsNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def get_users ( session , query ) : response = make_get_request ( session , 'users' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise UsersNotFoundException ( message = json_data [ 'message'  ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def create_project ( session , title , description , currency , budget , jobs ) : project_data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs } response = make_post_request ( session , 'projects' , json_data = project_data ) json_data = response . json ( ) if response . status_code == 200 : project_data = json_data [ 'result' ] p = Project ( project_data ) p . url = urljoin ( session . url , 'projects/%s' % p . seo_url ) return p else :  raise ProjectNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] , )"}
{"code_tokens": "def create_hireme_project ( session , title , description , currency , budget , jobs , hireme_initial_bid ) : jobs . append ( create_job_object ( id = 417 ) ) project_data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs , 'hireme' : True , 'hireme_initial_bid' : hireme_initial_bid } response = make_post_request ( session , 'projects' , json_data = project_data ) json_data = response . json ( ) if response . status_code == 200 : project_data = json_data [ 'result' ] p = Project ( project_data ) p . url = urljoin ( session  . url , 'projects/%s' % p . seo_url ) return p else : raise ProjectNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] , )"}
{"code_tokens": "def get_projects ( session , query ) : response = make_get_request ( session , 'projects' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return  json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def get_project_by_id ( session , project_id , project_details = None , user_details = None ) : query = { } if project_details : query . update ( project_details ) if user_details : query . update ( user_details ) response = make_get_request ( session , 'projects/{}' . format ( project_id ) , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id =  json_data [ 'request_id' ] )"}
{"code_tokens": "def search_projects ( session , query , search_filter = None , project_details = None , user_details = None , limit = 10 , offset = 0 , active_only = None ) : search_data = { 'query' : query , 'limit' : limit , 'offset' : offset , } if search_filter : search_data . update ( search_filter ) if project_details : search_data . update ( project_details ) if user_details : search_data . update ( user_details ) endpoint = 'projects/{}' . format ( 'active' if active_only else 'all' ) response = make_get_request ( session , endpoint , params_data = search_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code  = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def place_project_bid ( session , project_id , bidder_id , description , amount , period , milestone_percentage ) : bid_data = { 'project_id' : project_id , 'bidder_id' : bidder_id , 'description' : description , 'amount' : amount , 'period' : period , 'milestone_percentage' : milestone_percentage , } response = make_post_request ( session , 'bids' , json_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : bid_data = json_data [ 'result' ] return Bid ( bid_data )  else : raise BidNotPlacedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def get_bids ( session , project_ids = [ ] , bid_ids = [ ] , limit = 10 , offset = 0 ) : get_bids_data = { } if bid_ids : get_bids_data [ 'bids[]' ] = bid_ids if project_ids : get_bids_data [ 'projects[]' ] = project_ids get_bids_data [ 'limit' ] = limit get_bids_data [ 'offset' ] = offset response = make_get_request ( session , 'bids'  , params_data = get_bids_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise BidsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def get_milestones ( session , project_ids = [ ] , milestone_ids = [ ] , user_details = None , limit = 10 , offset = 0 ) : get_milestones_data = { } if milestone_ids : get_milestones_data [ 'milestones[]' ] = milestone_ids if project_ids : get_milestones_data [ 'projects[]' ] = project_ids get_milestones_data [ 'limit' ] = limit get_milestones_data [ 'offset' ] = offset if user_details :  get_milestones_data . update ( user_details ) response = make_get_request ( session , 'milestones' , params_data = get_milestones_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MilestonesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def get_milestone_by_id ( session , milestone_id , user_details = None ) : endpoint = 'milestones/{}' . format ( milestone_id ) response = make_get_request ( session , endpoint , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MilestonesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [  'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def award_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'award' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json  ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotAwardedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def revoke_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'revoke' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response  . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotRevokedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def accept_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'accept' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotAcceptedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id  = json_data [ 'request_id' ] )"}
{"code_tokens": "def retract_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'retract' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotRetractedException  ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def highlight_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'highlight' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response .  json ( ) raise BidNotHighlightedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def create_milestone_payment ( session , project_id , bidder_id , amount , reason , description ) : milestone_data = { 'project_id' : project_id , 'bidder_id' : bidder_id , 'amount' : amount , 'reason' : reason , 'description' : description } response = make_post_request ( session , 'milestones' , json_data = milestone_data ) json_data = response . json ( ) if response . status_code == 200 : milestone_data = json_data [ 'result' ] return Milestone ( milestone_data ) else : raise MilestoneNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code'  ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def post_track ( session , user_id , project_id , latitude , longitude ) : tracking_data = { 'user_id' : user_id , 'project_id' : project_id , 'track_point' : { 'latitude' : latitude , 'longitude' : longitude } } response = make_post_request ( session , 'tracks' , json_data = tracking_data ) json_data = response .  json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def update_track ( session , track_id , latitude , longitude , stop_tracking = False ) : tracking_data = { 'track_point' : { 'latitude' : latitude , 'longitude' : longitude , } , 'stop_tracking' : stop_tracking } response = make_put_request ( session , 'tracks/{}' . format ( track_id ) , json_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotUpdatedException ( message = json_data [ 'message' ] , error_code =  json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def get_track_by_id ( session , track_id , track_point_limit = None , track_point_offset = None ) : tracking_data = { } if track_point_limit : tracking_data [ 'track_point_limit' ] = track_point_limit if track_point_offset : tracking_data [ 'track_point_offset' ] = track_point_offset response = make_get_request ( session , 'tracks/{}' . format ( track_id ) , params_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result'  ] else : raise TrackNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def create_milestone_request ( session , project_id , bid_id , description , amount ) : milestone_request_data = { 'project_id' : project_id , 'bid_id' : bid_id , 'description' : description , 'amount' : amount , } response = make_post_request ( session , 'milestone_requests' , json_data = milestone_request_data ) json_data = response  . json ( ) if response . status_code == 200 : milestone_request_data = json_data [ 'result' ] return MilestoneRequest ( milestone_request_data ) else : raise MilestoneRequestNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def accept_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'accept' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotAcceptedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code'  ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def reject_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'reject' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json  ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotRejectedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def delete_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'delete' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id  = json_data [ 'request_id' ] )"}
{"code_tokens": "def post_review ( session , review ) : response = make_post_request ( session , 'reviews' , json_data = review ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise ReviewNotPostedException ( message = json_data [ 'message' ] ,  error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def get_jobs ( session , job_ids , seo_details , lang ) : get_jobs_data = { 'jobs[]' : job_ids , 'seo_details' : seo_details , 'lang' : lang , } response = make_get_request ( session , 'jobs' , params_data = get_jobs_data ) json_data = response .  json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise JobsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def create_thread ( session , member_ids , context_type , context , message ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } thread_data = { 'members[]' : member_ids , 'context_type' : context_type , 'context' : context , 'message' : message , } response = make_post_request ( session , 'threads' , headers , form_data = thread_data ) json_data = response . json ( ) if response . status_code == 200 : return Thread ( json_data [ 'result' ] ) else : raise ThreadNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] ,  request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def create_project_thread ( session , member_ids , project_id , message ) : return create_thread ( session , member_ids , 'project' , project_id  , message )"}
{"code_tokens": "def get_messages ( session , query , limit = 10 , offset = 0 ) : query [ 'limit' ] = limit query [ 'offset' ] = offset response = make_get_request ( session , 'messages' , params_data = query ) json_data = response . json ( ) if response  . status_code == 200 : return json_data [ 'result' ] else : raise MessagesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def search_messages ( session , thread_id , query , limit = 20 , offset = 0 , message_context_details = None , window_above = None , window_below = None ) : query = { 'thread_id' : thread_id , 'query' : query , 'limit' : limit , 'offset' : offset } if message_context_details : query [ 'message_context_details' ] = message_context_details if window_above : query [ 'window_above' ] = window_above if window_below : query [ 'window_below' ] = window_below response = make_get_request ( session , 'messages/search' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data  [ 'result' ] else : raise MessagesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def get_threads ( session , query ) : response = make_get_request ( session , 'threads' , params_data = query ) json_data = response . json ( ) if response . status_code ==  200 : return json_data [ 'result' ] else : raise ThreadsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )"}
{"code_tokens": "def _clean ( zipcode , valid_length = _valid_zipcode_length ) : zipcode = zipcode . split ( \"-\" ) [ 0 ] if len ( zipcode ) != valid_length : raise ValueError ( 'Invalid format, zipcode must be of the format: \"#####\" or \"#####-####\"' ) if _contains_nondigits ( zipcode ) : raise ValueError ( 'Invalid characters, zipcode may only contain digits and \"-\".' )  return zipcode"}
{"code_tokens": "def similar_to ( partial_zipcode , zips = _zips ) : return [ z for z in zips if z  [ \"zip_code\" ] . startswith ( partial_zipcode ) ]"}
{"code_tokens": "def filter_by ( zips = _zips , ** kwargs ) : return [ z for z in zips if all ( [ k in z and z [ k ] == v for k  , v in kwargs . items ( ) ] ) ]"}
{"code_tokens": "def is_valid_identifier ( name ) : if not isinstance ( name , str ) : return False if '\\n' in name : return False if name . strip ( ) != name : return False try : code = compile ( '\\n{0}=None' . format ( name ) , filename = '<string>' ,  mode = 'single' ) exec ( code ) return True except SyntaxError : return False"}
{"code_tokens": "def from_config ( cls , cfg , default_fg = DEFAULT_FG_16 , default_bg = DEFAULT_BG_16 , default_fg_hi = DEFAULT_FG_256 , default_bg_hi = DEFAULT_BG_256 , max_colors = 2 ** 24 ) : e = PaletteEntry ( mono = default_fg , foreground = default_fg , background = default_bg , foreground_high = default_fg_hi , background_high = default_bg_hi ) if isinstance ( cfg , str ) : e . foreground_high = cfg if e . allowed ( cfg , 16 ) : e . foreground = cfg else : rgb = AttrSpec ( fg = cfg , bg = \"\" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . foreground = nearest_basic_color ( rgb ) elif isinstance ( cfg , dict ) : bg = cfg . get ( \"bg\" , None ) if isinstance ( bg , str ) : e . background_high = bg if e . allowed ( bg , 16 ) : e . background = bg else : rgb = AttrSpec ( fg = bg , bg = \"\" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . background = nearest_basic_color ( rgb ) elif isinstance ( bg , dict ) : e . background_high = bg . get ( \"hi\" , default_bg_hi ) if \"lo\" in bg : if e . allowed ( bg [ \"lo\" ] , 16 ) : e . background = bg [ \"lo\" ] else : rgb = AttrSpec ( fg = bg [ \"lo\" ] , bg = \"\" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . background = nearest_basic_color ( rgb ) fg = cfg . get ( \"fg\" , cfg ) if isinstance ( fg , str ) : e . foreground_high = fg if e . allowed ( fg , 16 ) : e . foreground = fg else : rgb = AttrSpec ( fg = fg , bg = \"\" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . foreground = nearest_basic_color ( rgb ) elif isinstance ( fg , dict ) : e . foreground_high = fg  . get ( \"hi\" , default_fg_hi ) if \"lo\" in fg : if e . allowed ( fg [ \"lo\" ] , 16 ) : e . foreground = fg [ \"lo\" ] else : rgb = AttrSpec ( fg = fg [ \"lo\" ] , bg = \"\" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . foreground = nearest_basic_color ( rgb ) return e"}
{"code_tokens": "def migrate ( src_path , src_passphrase , src_backend , dst_path , dst_passphrase , dst_backend ) : src_storage = STORAGE_MAPPING [ src_backend ] ( ** _parse_path_string ( src_path ) ) dst_storage = STORAGE_MAPPING [ dst_backend ] ( ** _parse_path_string ( dst_path ) ) src_stash = Stash  ( src_storage , src_passphrase ) dst_stash = Stash ( dst_storage , dst_passphrase ) keys = src_stash . export ( ) dst_stash . load ( src_passphrase , keys = keys )"}
{"code_tokens": "def generate_passphrase ( size = 12 ) : chars = string . ascii_lowercase + string . ascii_uppercase + string . digits return str ( '' . join ( random . choice ( chars ) for _ in range ( size )  ) )"}
{"code_tokens": "def _build_dict_from_key_value ( keys_and_values ) : key_dict = { } for key_value in keys_and_values : if '=' not in key_value : raise GhostError ( 'Pair {0} is not of `key=value` format' . format ( key_value ) ) key , value  = key_value . split ( '=' , 1 ) key_dict . update ( { str ( key ) : str ( value ) } ) return key_dict"}
{"code_tokens": "def _prettify_list ( items ) : assert isinstance ( items , list ) keys_list = 'Available Keys:' for item in items : keys_list += '\\n - {0}' . format ( item )  return keys_list"}
{"code_tokens": "def init_stash ( stash_path , passphrase , passphrase_size , backend ) : r stash_path = stash_path or STORAGE_DEFAULT_PATH_MAPPING [ backend ] click . echo ( 'Stash: {0} at {1}' . format ( backend , stash_path ) ) storage = STORAGE_MAPPING [ backend ] ( ** _parse_path_string ( stash_path ) ) try : click . echo ( 'Initializing stash...' ) if os . path . isfile ( PASSPHRASE_FILENAME ) : raise GhostError ( '{0} already exists. Overwriting might prevent you ' 'from accessing the stash it was generated for. ' 'Please make sure to save and remove the file before ' 'initializing another stash.' . format ( PASSPHRASE_FILENAME ) ) stash = Stash ( storage , passphrase = passphrase , passphrase_size = passphrase_size ) passphrase = stash . init ( ) if not passphrase : click . echo ( 'Stash already initialized.' ) sys . exit ( 0 ) _write_passphrase_file ( passphrase ) except GhostError as ex : sys . exit ( ex ) except ( OSError , IOError ) as ex  : click . echo ( \"Seems like we've run into a problem.\" ) file_path = _parse_path_string ( stash_path ) [ 'db_path' ] click . echo ( 'Removing stale stash and passphrase: {0}. Note that any ' 'directories created are not removed for safety reasons and you ' 'might want to remove them manually.' . format ( file_path ) ) if os . path . isfile ( file_path ) : os . remove ( file_path ) sys . exit ( ex ) click . echo ( 'Initialized stash at: {0}' . format ( stash_path ) ) click . echo ( 'Your passphrase can be found under the `{0}` file in the ' 'current directory.' . format ( PASSPHRASE_FILENAME ) ) click . echo ( 'Make sure you save your passphrase somewhere safe. ' 'If lost, you will lose access to your stash.' )"}
{"code_tokens": "def put_key ( key_name , value , description , meta , modify , add , lock , key_type , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Stashing {0} key...' . format ( key_type ) ) stash . put ( name = key_name , value = _build_dict_from_key_value ( value ) , modify = modify , metadata = _build_dict_from_key_value ( meta ) , description =  description , lock = lock , key_type = key_type , add = add ) click . echo ( 'Key stashed successfully' ) except GhostError as ex : sys . exit ( ex )"}
{"code_tokens": "def lock_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Locking key...' ) stash . lock ( key_name = key_name ) click . echo ( 'Key locked successfully' ) except GhostError as ex : sys . exit ( ex ) "}
{"code_tokens": "def unlock_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Unlocking key...' ) stash . unlock ( key_name = key_name )  click . echo ( 'Key unlocked successfully' ) except GhostError as ex : sys . exit ( ex )"}
{"code_tokens": "def get_key ( key_name , value_name , jsonify , no_decrypt , stash , passphrase , backend ) : if value_name and no_decrypt : sys . exit ( 'VALUE_NAME cannot be used in conjuction with --no-decrypt' ) stash = _get_stash ( backend , stash , passphrase , quiet = jsonify or value_name ) try : key = stash . get ( key_name = key_name , decrypt = not no_decrypt ) except GhostError as ex : sys . exit ( ex ) if not key : sys . exit ( 'Key `{0}` not found' . format ( key_name ) ) if value_name : key = key [ 'value' ] . get ( value_name ) if not key : sys . exit ( 'Value name `{0}` could not be found under key `{1}`' . format ( value_name , key_name ) ) if jsonify or value_name : click . echo ( json . dumps ( key , indent = 4 , sort_keys = False ) . strip ( '\"' ) , nl = True ) else : click . echo ( 'Retrieving key...' ) click . echo ( '\\n' + _prettify_dict ( key ) ) "}
{"code_tokens": "def delete_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) for key in key_name : try : click . echo ( 'Deleting key {0}...' . format ( key ) ) stash . delete ( key_name = key ) except GhostError as ex : sys . exit ( ex  ) click . echo ( 'Keys deleted successfully' )"}
{"code_tokens": "def list_keys ( key_name , max_suggestions , cutoff , jsonify , locked , key_type , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase , quiet = jsonify ) try : keys = stash . list ( key_name = key_name , max_suggestions = max_suggestions , cutoff = cutoff , locked_only = locked , key_type = key_type ) except GhostError as ex : sys . exit ( ex ) if jsonify : click .  echo ( json . dumps ( keys , indent = 4 , sort_keys = True ) ) elif not keys : click . echo ( 'The stash is empty. Go on, put some keys in there...' ) else : click . echo ( 'Listing all keys...' ) click . echo ( _prettify_list ( keys ) )"}
{"code_tokens": "def purge_stash ( force , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo (  'Purging stash...' ) stash . purge ( force ) click . echo ( 'Purge complete!' ) except GhostError as ex : sys . exit ( ex )"}
{"code_tokens": "def export_keys ( output_path , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Exporting stash to {0}...' . format ( output_path ) ) stash . export ( output_path = output_path ) click . echo ( 'Export complete!' ) except GhostError as ex : sys . exit ( ex ) "}
{"code_tokens": "def load_keys ( key_file , origin_passphrase , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) click . echo (  'Importing all keys from {0}...' . format ( key_file ) ) stash . load ( origin_passphrase , key_file = key_file ) click . echo ( 'Import complete!' )"}
{"code_tokens": "def migrate_stash ( source_stash_path , source_passphrase , source_backend , destination_stash_path , destination_passphrase , destination_backend ) : click . echo ( 'Migrating all keys from {0} to {1}...' . format ( source_stash_path , destination_stash_path ) ) try : migrate ( src_path = source_stash_path , src_passphrase = source_passphrase , src_backend = source_backend , dst_path = destination_stash_path , dst_passphrase = destination_passphrase ,  dst_backend = destination_backend ) except GhostError as ex : sys . exit ( ex ) click . echo ( 'Migration complete!' )"}
{"code_tokens": "def ssh ( key_name , no_tunnel , stash , passphrase , backend ) : def execute ( command ) : try : click . echo ( 'Executing: {0}' . format ( ' ' . join ( command ) ) ) subprocess . check_call ( ' ' . join ( command ) , shell = True ) except subprocess . CalledProcessError : sys . exit ( 1 ) stash = _get_stash ( backend , stash , passphrase ) key = stash . get ( key_name ) if key : _assert_is_ssh_type_key ( key ) else : sys . exit ( 'Key `{0}` not found' . format ( key_name ) ) conn_info = key [ 'value' ] ssh_key_path = conn_info . get ( 'ssh_key_path' ) ssh_key = conn_info . get ( 'ssh_key' ) proxy_key_path = conn_info . get ( 'proxy_key_path' ) proxy_key = conn_info . get (  'proxy_key' ) id_file = _write_tmp ( ssh_key ) if ssh_key else ssh_key_path conn_info [ 'ssh_key_path' ] = id_file if conn_info . get ( 'proxy' ) : proxy_id_file = _write_tmp ( proxy_key ) if proxy_key else proxy_key_path conn_info [ 'proxy_key_path' ] = proxy_id_file ssh_command = _build_ssh_command ( conn_info , no_tunnel ) try : execute ( ssh_command ) finally : if id_file != ssh_key_path : click . echo ( 'Removing temp ssh key file: {0}...' . format ( id_file ) ) os . remove ( id_file ) if conn_info . get ( 'proxy' ) and proxy_id_file != proxy_key_path : click . echo ( 'Removing temp proxy key file: {0}...' . format ( proxy_id_file ) ) os . remove ( proxy_id_file )"}
{"code_tokens": "def put ( self , name , value = None , modify = False , metadata = None , description = '' , encrypt = True , lock = False , key_type = 'secret' , add = False ) : def assert_key_is_unlocked ( existing_key ) : if existing_key and existing_key . get ( 'lock' ) : raise GhostError ( 'Key `{0}` is locked and therefore cannot be modified. ' 'Unlock the key and try again' . format ( name ) ) def assert_value_provided_for_new_key ( value , existing_key ) : if not value and not existing_key . get ( 'value' ) : raise GhostError ( 'You must provide a value for new keys' ) self . _assert_valid_stash ( ) self . _validate_key_schema ( value , key_type ) if value and encrypt and not isinstance ( value , dict ) : raise GhostError ( 'Value must be of type dict' ) key = self . _handle_existing_key ( name , modify or add ) assert_key_is_unlocked ( key ) assert_value_provided_for_new_key ( value , key ) new_key = dict ( name = name , lock = lock ) if value : if add : value = self . _update_existing_key ( key , value ) new_key [ 'value' ] = self . _encrypt ( value ) if encrypt else value else : new_key [ 'value' ] = key . get ( 'value' ) new_key [ 'description' ] = description or key . get ( 'description' ) new_key [ 'created_at' ] = key . get ( 'created_at' ) or _get_current_time ( ) new_key [ 'modified_at' ] = _get_current_time ( ) new_key [ 'metadata' ] = metadata or key . get ( 'metadata' ) new_key [ 'uid' ] = key . get ( 'uid' ) or str ( uuid . uuid4 ( ) ) new_key [ 'type' ] = key . get ( 'type' ) or key_type key_id = self . _storage . put ( new_key ) audit ( storage = self . _storage . db_path , action = 'MODIFY' if ( modify or add  ) else 'PUT' , message = json . dumps ( dict ( key_name = new_key [ 'name' ] , value = 'HIDDEN' , description = new_key [ 'description' ] , uid = new_key [ 'uid' ] , metadata = json . dumps ( new_key [ 'metadata' ] ) , lock = new_key [ 'lock' ] , type = new_key [ 'type' ] ) ) ) return key_id"}
{"code_tokens": "def get ( self , key_name , decrypt = True ) : self . _assert_valid_stash ( ) key = self . _storage . get ( key_name ) . copy ( ) if not key . get ( 'value' ) : return None if decrypt : key [ 'value' ] = self . _decrypt ( key [ 'value' ] ) audit ( storage = self . _storage . db_path , action = 'GET' , message = json . dumps ( dict ( key_name = key_name )  ) ) return key"}
{"code_tokens": "def list ( self , key_name = None , max_suggestions = 100 , cutoff = 0.5 , locked_only = False , key_type = None ) : self . _assert_valid_stash ( ) key_list = [ k for k in self . _storage . list ( ) if k [ 'name' ] != 'stored_passphrase' and ( k . get ( 'lock' ) if locked_only else True ) ] if key_type : types = ( 'secret' , None ) if key_type == 'secret' else [ key_type ] key_list = [ k for k in key_list if k . get ( 'type' ) in types ] key_list = [ k [ 'name' ] for k in key_list ] if key_name : if key_name . startswith ( '~' ) : key_list = difflib . get_close_matches ( key_name . lstrip ( '~' ) , key_list , max_suggestions , cutoff ) else : key_list = [ k  for k in key_list if key_name in k ] audit ( storage = self . _storage . db_path , action = 'LIST' + ( '[LOCKED]' if locked_only else '' ) , message = json . dumps ( dict ( ) ) ) return key_list"}
{"code_tokens": "def delete ( self , key_name ) : self . _assert_valid_stash ( ) if key_name == 'stored_passphrase' : raise GhostError ( '`stored_passphrase` is a reserved ghost key name ' 'which cannot be deleted' ) if not self . get ( key_name ) : raise GhostError ( 'Key `{0}` not found' . format ( key_name ) ) key = self . _storage . get ( key_name ) if key . get ( 'lock' ) : raise GhostError ( 'Key `{0}` is locked and therefore cannot be deleted ' 'Please unlock the key and try again' . format ( key_name ) ) deleted = self . _storage . delete ( key_name ) audit ( storage = self . _storage . db_path , action = 'DELETE' , message = json . dumps ( dict ( key_name = key_name ) ) ) if not deleted : raise GhostError ( 'Failed  to delete {0}' . format ( key_name ) )"}
{"code_tokens": "def purge ( self , force = False , key_type = None ) : self . _assert_valid_stash ( ) if not force : raise GhostError ( \"The `force` flag must be provided to perform a stash purge. \" \"I mean, you don't really want to just delete everything \" \"without precautionary measures eh?\" ) audit ( storage = self . _storage . db_path , action = 'PURGE' , message = json . dumps ( dict ( ) ) ) for key_name in self . list ( key_type  = key_type ) : self . delete ( key_name )"}
{"code_tokens": "def export ( self , output_path = None , decrypt = False ) : self . _assert_valid_stash ( ) all_keys = [ ] for key in self . list ( ) : all_keys . append ( dict ( self . get ( key , decrypt = decrypt ) ) ) if all_keys : if output_path : with open ( output_path , 'w' ) as output_file : output_file . write ( json . dumps ( all_keys , indent = 4 ) ) return all_keys else : raise GhostError ( 'There are no keys to  export' )"}
{"code_tokens": "def load ( self , origin_passphrase , keys = None , key_file = None ) : self . _assert_valid_stash ( ) if not ( bool ( keys ) ^ bool ( key_file ) ) : raise GhostError ( 'You must either provide a path to an exported stash file ' 'or a list of key dicts to import' ) if key_file : with open ( key_file ) as stash_file : keys = json . loads ( stash_file . read ( ) ) decrypt = origin_passphrase != self . passphrase if decrypt : stub = Stash ( TinyDBStorage ( 'stub' ) , origin_passphrase ) for key in keys : self . put ( name = key [ 'name' ] , value = stub . _decrypt ( key [ 'value' ] ) if decrypt else key [ 'value' ] , metadata = key [ 'metadata' ]  , description = key [ 'description' ] , lock = key . get ( 'lock' ) , key_type = key . get ( 'type' ) , encrypt = decrypt )"}
{"code_tokens": "def _encrypt ( self , value ) : value = json . dumps ( value ) with warnings . catch_warnings ( ) : warnings . simplefilter ( \"ignore\" )  encrypted_value = self . cipher . encrypt ( value . encode ( 'utf8' ) ) hexified_value = binascii . hexlify ( encrypted_value ) . decode ( 'ascii' ) return hexified_value"}
{"code_tokens": "def _decrypt ( self , hexified_value ) : encrypted_value = binascii . unhexlify ( hexified_value ) with warnings . catch_warnings ( ) : warnings . simplefilter ( \"ignore\" ) jsonified_value = self . cipher . decrypt ( encrypted_value ) . decode ( 'ascii' ) value = json  . loads ( jsonified_value ) return value"}
{"code_tokens": "def get ( self , key_name ) : result = self . db . search ( Query ( ) . name == key_name ) if not result : return { } return result [ 0  ]"}
{"code_tokens": "def delete ( self , key_name ) : self . db . remove ( Query ( ) . name == key_name ) return self .  get ( key_name ) == { }"}
{"code_tokens": "def _construct_key ( self , values ) : key = { } for column , value in zip ( self . keys . columns , values ) : key . update ( {  column . name : value } ) return key"}
{"code_tokens": "def put ( self , key ) : self . _consul_request ( 'PUT' , self . _key_url ( key [ 'name' ] ) , json = key  ) return key [ 'name' ]"}
{"code_tokens": "def put ( self , key ) : self . client . write ( self . _key_path ( key [ 'name' ] ) , ** key ) return self . _key_path ( key [  'name' ] )"}
{"code_tokens": "def init ( self ) : self . es . indices . create ( index = self . params [ 'index' ]  , ignore = 400 )"}
{"code_tokens": "def init ( self ) : try : self . client . create_bucket ( Bucket = self . db_path , CreateBucketConfiguration = self . bucket_configuration ) except botocore .  exceptions . ClientError as e : if 'BucketAlreadyOwnedByYou' not in str ( e . response [ 'Error' ] [ 'Code' ] ) : raise e"}
{"code_tokens": "def terminal ( port = default_port ( ) , baud = '9600' ) : testargs = [ 'nodemcu-uploader' , port , baud ] sys . argv = testargs miniterm . main  ( )"}
{"code_tokens": "def __set_baudrate ( self , baud ) : log . info ( 'Changing communication to %s baud' , baud ) self . __writeln ( UART_SETUP . format ( baud = baud ) ) time . sleep (  0.1 ) try : self . _port . setBaudrate ( baud ) except AttributeError : self . _port . baudrate = baud"}
{"code_tokens": "def set_timeout ( self , timeout ) : timeout = int (  timeout ) self . _timeout = timeout == 0 and 999999 or timeout"}
{"code_tokens": "def __clear_buffers ( self ) : try : self . _port . reset_input_buffer ( ) self . _port . reset_output_buffer ( ) except AttributeError : self . _port . flushInput ( ) self . _port . flushOutput  ( )"}
{"code_tokens": "def __expect ( self , exp = '> ' , timeout = None ) : timeout_before = self . _port . timeout timeout = timeout or self . _timeout if SYSTEM != 'Windows' : if self . _port . timeout != MINIMAL_TIMEOUT : self . _port . timeout = MINIMAL_TIMEOUT end = time . time ( ) + timeout data = '' while not data . endswith ( exp ) and time . time ( ) <= end : data += self . _port . read ( ) log . debug ( 'expect returned: `{0}`' . format ( data ) ) if time . time ( ) > end : raise CommunicationTimeout ( 'Timeout waiting for data' , data ) if not data .  endswith ( exp ) and len ( exp ) > 0 : raise BadResponseException ( 'Bad response.' , exp , data ) if SYSTEM != 'Windows' : self . _port . timeout = timeout_before return data"}
{"code_tokens": "def __write ( self , output , binary = False ) : if not binary : log . debug ( 'write: %s' , output ) else : log . debug ( 'write binary: %s' , hexify ( output ) ) self . _port . write ( output ) self . _port . flush (  )"}
{"code_tokens": "def __exchange ( self , output , timeout = None ) : self . __writeln ( output ) self . _port . flush ( ) return self . __expect ( timeout =  timeout or self . _timeout )"}
{"code_tokens": "def close ( self ) : try : if self . baud != self . start_baud : self . __set_baudrate ( self . start_baud ) self . _port . flush ( ) self . __clear_buffers ( )  except serial . serialutil . SerialException : pass log . debug ( 'closing port' ) self . _port . close ( )"}
{"code_tokens": "def prepare ( self ) : log . info ( 'Preparing esp for transfer.' ) for func in LUA_FUNCTIONS : detected = self . __exchange ( 'print({0})' . format ( func ) ) if detected . find ( 'function:' ) == - 1 : break else : log . info ( 'Preparation already done. Not adding functions again.' ) return True functions = RECV_LUA + '\\n' + SEND_LUA data = functions . format ( baud = self . _port . baudrate ) lines = data . replace ( '\\r' , '' ) . split ( '\\n' ) for line in lines : line = line . strip ( ) . replace ( ', ' , ',' ) . replace ( ' = ' , '=' ) if len ( line ) == 0 : continue resp = self . __exchange  ( line ) if ( 'unexpected' in resp ) or ( 'stdin' in resp ) or len ( resp ) > len ( functions ) + 10 : log . error ( 'error when preparing \"%s\"' , resp ) return False return True"}
{"code_tokens": "def download_file ( self , filename ) : res = self . __exchange ( 'send(\"{filename}\")' . format ( filename = filename ) ) if ( 'unexpected' in res ) or ( 'stdin' in res ) : log . error ( 'Unexpected error downloading file: %s' , res ) raise Exception ( 'Unexpected error downloading file' ) self . __write ( 'C' ) sent_filename = self . __expect ( NUL  ) . strip ( ) log . info ( 'receiveing ' + sent_filename ) self . __write ( ACK , True ) buf = '' data = '' chunk , buf = self . __read_chunk ( buf ) while chunk != '' : self . __write ( ACK , True ) data = data + chunk chunk , buf = self . __read_chunk ( buf ) return data"}
{"code_tokens": "def read_file ( self , filename , destination = '' ) : if not destination : destination = filename log . info ( 'Transferring %s to %s' , filename , destination ) data = self . download_file ( filename ) log . info ( destination ) if not os . path . exists ( os . path . dirname ( destination ) ) : try : os . makedirs ( os .  path . dirname ( destination ) ) except OSError as e : if e . errno != errno . EEXIST : raise with open ( destination , 'w' ) as fil : fil . write ( data )"}
{"code_tokens": "def write_file ( self , path , destination = '' , verify = 'none' ) : filename = os . path . basename ( path ) if not destination : destination = filename log . info ( 'Transferring %s as %s' , path , destination ) self . __writeln ( \"recv()\" ) res = self . __expect ( 'C> ' ) if not res . endswith ( 'C> ' ) : log . error ( 'Error waiting for esp \"%s\"' , res ) raise CommunicationTimeout ( 'Error waiting for device to start receiving' , res ) log . debug ( 'sending destination filename \"%s\"' , destination ) self . __write ( destination + '\\x00' , True ) if not self . __got_ack ( ) : log . error ( 'did not ack destination filename' ) raise NoAckException ( 'Device did not ACK destination filename' ) content = from_file ( path ) log . debug ( 'sending %d bytes in %s' , len ( content ) , filename ) pos = 0 chunk_size = 128 while pos < len ( content ) : rest = len ( content ) - pos if rest > chunk_size  : rest = chunk_size data = content [ pos : pos + rest ] if not self . __write_chunk ( data ) : resp = self . __expect ( ) log . error ( 'Bad chunk response \"%s\" %s' , resp , hexify ( resp ) ) raise BadResponseException ( 'Bad chunk response' , ACK , resp ) pos += chunk_size log . debug ( 'sending zero block' ) self . __write_chunk ( '' ) if verify != 'none' : self . verify_file ( path , destination , verify )"}
{"code_tokens": "def verify_file ( self , path , destination , verify = 'none' ) : content = from_file ( path ) log . info ( 'Verifying using %s...' % verify ) if verify == 'raw' : data = self . download_file ( destination ) if content != data : log . error ( 'Raw verification failed.' ) raise VerificationError ( 'Verification failed.' ) else : log . info ( 'Verification successful. Contents are identical.' ) elif verify == 'sha1' : data = self . __exchange ( 'shafile(\"' + destination + '\")' ) . splitlines ( ) [ 1 ] log . info ( 'Remote SHA1: %s' , data ) filehashhex = hashlib . sha1 ( content . encode ( ENCODING ) ) . hexdigest ( ) log . info ( 'Local SHA1: %s' , filehashhex ) if data != filehashhex : log . error ( 'SHA1  verification failed.' ) raise VerificationError ( 'SHA1 Verification failed.' ) else : log . info ( 'Verification successful. Checksums match' ) elif verify != 'none' : raise Exception ( verify + ' is not a valid verification method.' )"}
{"code_tokens": "def exec_file ( self , path ) : filename = os . path . basename ( path ) log . info ( 'Execute %s' , filename ) content = from_file ( path ) . replace ( '\\r' , '' ) . split ( '\\n' ) res = '> ' for line in content : line = line . rstrip ( '\\n' ) retlines = ( res + self . __exchange ( line ) ) . splitlines ( ) res = retlines . pop ( ) for lin in retlines : log . info ( lin ) log . info (  res )"}
{"code_tokens": "def __got_ack ( self ) : log . debug ( 'waiting for ack' ) res = self . _port .  read ( 1 ) log . debug ( 'ack read %s' , hexify ( res ) ) return res == ACK"}
{"code_tokens": "def write_lines ( self , data ) : lines = data . replace ( '\\r' , '' ) . split (  '\\n' ) for line in lines : self . __exchange ( line )"}
{"code_tokens": "def __write_chunk ( self , chunk ) : log . debug ( 'writing %d bytes chunk' , len ( chunk ) ) data = BLOCK_START + chr ( len ( chunk ) ) + chunk if len ( chunk ) < 128 : padding = 128 - len ( chunk ) log . debug ( 'pad with %d characters' , padding ) data = data + ( ' ' * padding ) log . debug ( \"packet size  %d\" , len ( data ) ) self . __write ( data ) self . _port . flush ( ) return self . __got_ack ( )"}
{"code_tokens": "def __read_chunk ( self , buf ) : log . debug ( 'reading chunk' ) timeout_before = self . _port . timeout if SYSTEM != 'Windows' : if self . _port . timeout != MINIMAL_TIMEOUT : self . _port . timeout = MINIMAL_TIMEOUT end = time . time ( ) + timeout_before while len ( buf ) < 130 and time . time ( ) <= end : buf = buf + self . _port . read ( ) if buf [ 0 ] != BLOCK_START or len ( buf ) < 130 : log . debug ( 'buffer binary: %s ' , hexify ( buf ) ) raise Exception ( 'Bad blocksize or start byte' ) if SYSTEM != 'Windows' : self . _port . timeout = timeout_before chunk_size = ord ( buf [ 1 ]  ) data = buf [ 2 : chunk_size + 2 ] buf = buf [ 130 : ] return ( data , buf )"}
{"code_tokens": "def file_list ( self ) : log . info ( 'Listing files' ) res = self . __exchange ( LIST_FILES ) res = res . split ( '\\r\\n' ) res = res [ 1 : - 1 ] files = [ ]  for line in res : files . append ( line . split ( '\\t' ) ) return files"}
{"code_tokens": "def file_do ( self , filename ) : log . info ( 'Executing ' + filename ) res = self .  __exchange ( 'dofile(\"' + filename + '\")' ) log . info ( res ) return res"}
{"code_tokens": "def file_format ( self ) : log . info ( 'Formating, can take minutes depending on flash size...' ) res = self . __exchange ( 'file.format()' , timeout = 300 ) if 'format done' not in res : log . error (  res ) else : log . info ( res ) return res"}
{"code_tokens": "def file_print ( self , filename ) : log . info ( 'Printing ' + filename ) res = self . __exchange ( PRINT_FILE . format  ( filename = filename ) ) log . info ( res ) return res"}
{"code_tokens": "def node_heap ( self ) : log . info ( 'Heap' ) res = self . __exchange ( 'print(node.heap())' ) log . info ( res ) return int ( res . split  ( '\\r\\n' ) [ 1 ] )"}
{"code_tokens": "def file_compile ( self , path ) : log . info ( 'Compile ' + path ) cmd = 'node.compile(\"%s\")' % path res = self . __exchange ( cmd  ) log . info ( res ) return res"}
{"code_tokens": "def file_remove ( self , path ) : log . info ( 'Remove ' + path ) cmd = 'file.remove(\"%s\")' % path res = self . __exchange ( cmd ) log . info ( res  ) return res"}
{"code_tokens": "def backup ( self , path ) : log . info ( 'Backing up in ' + path ) files = self . file_list ( ) self . prepare ( ) for f in files : self . read_file ( f [ 0 ] ,  os . path . join ( path , f [ 0 ] ) )"}
{"code_tokens": "def operation_upload ( uploader , sources , verify , do_compile , do_file , do_restart ) : sources , destinations = destination_from_source ( sources ) if len ( destinations ) == len ( sources ) : if uploader . prepare ( ) : for filename , dst in zip ( sources , destinations ) : if do_compile : uploader . file_remove ( os . path . splitext ( dst ) [ 0 ] + '.lc' ) uploader . write_file ( filename , dst , verify ) if do_compile and dst != 'init.lua' : uploader . file_compile ( dst ) uploader . file_remove ( dst ) if do_file : uploader . file_do ( os . path . splitext ( dst ) [ 0 ] + '.lc' ) elif do_file : uploader . file_do ( dst ) else : raise Exception ( 'Error preparing nodemcu for reception' ) else : raise Exception ( 'You must specify a destination filename for each file you want  to upload.' ) if do_restart : uploader . node_restart ( ) log . info ( 'All done!' )"}
{"code_tokens": "def operation_download ( uploader , sources ) : sources , destinations = destination_from_source ( sources , False ) print ( 'sources' , sources ) print ( 'destinations' , destinations ) if len ( destinations ) == len ( sources ) : if uploader . prepare ( ) : for filename , dst in zip ( sources , destinations  ) : uploader . read_file ( filename , dst ) else : raise Exception ( 'You must specify a destination filename for each file you want to download.' ) log . info ( 'All done!' )"}
{"code_tokens": "def operation_list ( uploader ) : files = uploader . file_list ( ) for f in files : log . info ( \"{file:30s} {size}\" . format ( file = f [ 0 ] , size = f  [ 1 ] ) )"}
{"code_tokens": "def display ( content ) : if isinstance ( content , gp . GPServer ) : IPython . display . display ( GPAuthWidget ( content ) ) elif isinstance ( content , gp . GPTask ) : IPython . display . display ( GPTaskWidget ( content ) ) elif isinstance ( content , gp . GPJob ) :  IPython . display . display ( GPJobWidget ( content ) ) else : IPython . display . display ( content )"}
{"code_tokens": "def from_timestamp ( ts ) : return datetime . datetime . utcfromtimestamp ( ts ) . replace ( tzinfo = pytz  . utc )"}
{"code_tokens": "def at_time ( cls , at , target ) : at = cls . _from_timestamp ( at ) cmd = cls . from_datetime ( at ) cmd . delay = at - now ( )  cmd . target = target return cmd"}
{"code_tokens": "def _localize ( dt ) : try : tz = dt . tzinfo return tz . localize ( dt . replace  ( tzinfo = None ) ) except AttributeError : return dt"}
{"code_tokens": "def daily_at ( cls , at , target ) : daily = datetime . timedelta ( days = 1 ) when = datetime . datetime . combine ( datetime . date . today ( ) , at ) if when  < now ( ) : when += daily return cls . at_time ( cls . _localize ( when ) , daily , target )"}
{"code_tokens": "def strftime ( fmt , t ) : if isinstance ( t , ( time . struct_time , tuple ) ) : t = datetime . datetime ( * t [ : 6 ] ) assert isinstance ( t , ( datetime . datetime , datetime . time , datetime . date ) ) try : year = t . year if year < 1900 : t = t . replace ( year = 1900 ) except AttributeError : year = 1900 subs = ( ( '%Y' , '%04d' % year ) , ( '%y' , '%02d' % ( year % 100 ) ) , ( '%s' , '%03d' % ( t . microsecond // 1000 ) ) , ( '%u' , '%03d' % ( t . microsecond % 1000 ) ) ) def doSub ( s , sub ) : return s . replace ( * sub ) def doSubs ( s ) : return functools . reduce  ( doSub , subs , s ) fmt = '%%' . join ( map ( doSubs , fmt . split ( '%%' ) ) ) return t . strftime ( fmt )"}
{"code_tokens": "def strptime ( s , fmt , tzinfo = None ) : res = time . strptime ( s , fmt ) return datetime . datetime ( tzinfo  = tzinfo , * res [ : 6 ] )"}
{"code_tokens": "def get_nearest_year_for_day ( day ) : now = time . gmtime ( ) result = now . tm_year if day - now . tm_yday > 365 // 2 : result -= 1  if now . tm_yday - day > 365 // 2 : result += 1 return result"}
{"code_tokens": "def get_period_seconds ( period ) : if isinstance ( period , six . string_types ) : try : name = 'seconds_per_' + period . lower ( ) result = globals ( ) [ name ] except KeyError : msg = \"period not in (second, minute, hour, day, month, year)\" raise ValueError ( msg ) elif isinstance ( period , numbers . Number ) : result = period elif isinstance ( period  , datetime . timedelta ) : result = period . days * get_period_seconds ( 'day' ) + period . seconds else : raise TypeError ( 'period must be a string or integer' ) return result"}
{"code_tokens": "def divide_timedelta_float ( td , divisor ) : dsm = [ getattr ( td , attr ) for attr in ( 'days' , 'seconds' , 'microseconds' ) ] dsm = map ( lambda elem : elem /  divisor , dsm ) return datetime . timedelta ( * dsm )"}
{"code_tokens": "def parse_timedelta ( str ) : deltas = ( _parse_timedelta_part ( part . strip ( ) ) for part in str . split ( ','  ) ) return sum ( deltas , datetime . timedelta ( ) )"}
{"code_tokens": "def divide_timedelta ( td1 , td2 ) : try : return td1 / td2 except TypeError : return td1 . total_seconds ( ) / td2 . total_seconds ( ) "}
{"code_tokens": "def date_range ( start = None , stop = None , step = None ) : if step is None : step = datetime . timedelta ( days = 1 ) if start  is None : start = datetime . datetime . now ( ) while start < stop : yield start start += step"}
{"code_tokens": "def construct_datetime ( cls , * args , ** kwargs ) : if len ( args ) == 1 : arg = args [ 0 ] method = cls . __get_dt_constructor ( type ( arg ) . __module__ , type ( arg ) . __name__ , ) result = method ( arg ) try : result = result . replace ( tzinfo = kwargs . pop ( 'tzinfo' ) ) except KeyError : pass if kwargs : first_key = kwargs . keys ( ) [ 0 ] tmpl = ( \"{first_key} is an invalid keyword \" \"argument for this function.\" ) raise TypeError ( tmpl . format (  ** locals ( ) ) ) else : result = datetime . datetime ( * args , ** kwargs ) return result"}
{"code_tokens": "def get ( self , query , responseformat = \"geojson\" , verbosity = \"body\" , build = True ) : if build : full_query = self . _construct_ql_query ( query , responseformat = responseformat , verbosity = verbosity ) else : full_query = query if self . debug : logging . getLogger ( ) . info ( query ) r = self . _get_from_overpass ( full_query ) content_type = r . headers . get ( \"content-type\" ) if self . debug : print ( content_type ) if content_type == \"text/csv\" : result = [ ] reader = csv . reader ( StringIO ( r . text ) , delimiter = \"\\t\" ) for row in reader : result . append ( row ) return result elif content_type in ( \"text/xml\" , \"application/xml\" , \"application/osm3s+xml\" ) : return r . text elif content_type == \"application/json\" : response = json . loads ( r . text ) if not build : return response if \"elements\" not in response : raise UnknownOverpassError ( \"Received an invalid answer from Overpass.\" ) overpass_remark = response . get ( \"remark\" , None ) if overpass_remark and overpass_remark . startswith ( \"runtime error\" ) : raise ServerRuntimeError ( overpass_remark ) if responseformat is not \"geojson\" : return response return self  . _as_geojson ( response [ \"elements\" ] )"}
{"code_tokens": "def get_ports_count ( context , filters = None ) : LOG . info ( \"get_ports_count for tenant %s filters %s\" % ( context  . tenant_id , filters ) ) return db_api . port_count_all ( context , join_security_groups = True , ** filters )"}
{"code_tokens": "def _allocate_from_v6_subnet ( self , context , net_id , subnet , port_id , reuse_after , ip_address = None , ** kwargs ) : LOG . info ( \"Attempting to allocate a v6 address - [{0}]\" . format ( utils . pretty_kwargs ( network_id = net_id , subnet = subnet , port_id = port_id , ip_address = ip_address ) ) ) if ip_address : LOG . info ( \"IP %s explicitly requested, deferring to standard \" \"allocation\" % ip_address ) return self . _allocate_from_subnet ( context , net_id = net_id , subnet = subnet , port_id = port_id , reuse_after = reuse_after , ip_address = ip_address , ** kwargs ) else : mac = kwargs . get ( \"mac_address\" ) if mac : mac = kwargs [ \"mac_address\" ] . get ( \"address\" ) if subnet and subnet [ \"ip_policy\" ] : ip_policy_cidrs = subnet [ \"ip_policy\" ] . get_cidrs_ip_set ( ) else : ip_policy_cidrs = netaddr . IPSet ( [ ] ) for tries , ip_address in enumerate ( generate_v6 ( mac , port_id , subnet [ \"cidr\" ] ) ) : LOG . info ( \"Attempt {0} of {1}\" . format ( tries + 1 , CONF . QUARK . v6_allocation_attempts ) ) if tries > CONF . QUARK . v6_allocation_attempts - 1 : LOG . info ( \"Exceeded v6 allocation attempts, bailing\" ) raise ip_address_failure ( net_id ) ip_address = netaddr . IPAddress ( ip_address ) . ipv6 ( ) LOG . info ( \"Generated a new v6 address {0}\" . format ( str ( ip_address ) ) ) if ( ip_policy_cidrs is not None and ip_address in ip_policy_cidrs ) : LOG . info ( \"Address {0} excluded by policy\" . format ( str ( ip_address ) ) ) continue try : with context . session . begin ( ) : address = db_api . ip_address_create ( context , address = ip_address , subnet_id = subnet [ \"id\" ] , version = subnet [ \"ip_version\" ] , network_id = net_id , address_type = kwargs . get ( 'address_type' , ip_types . FIXED ) ) return address except db_exception . DBDuplicateEntry : LOG . info ( \"{0} exists but was already \" \"allocated\" . format ( str ( ip_address ) ) ) LOG  . debug ( \"Duplicate entry found when inserting subnet_id\" \" %s ip_address %s\" , subnet [ \"id\" ] , ip_address )"}
{"code_tokens": "def _create_flip ( context , flip , port_fixed_ips ) : if port_fixed_ips : context . session . begin ( ) try : ports = [ val [ 'port' ] for val in port_fixed_ips . values ( ) ] flip = db_api . port_associate_ip ( context , ports , flip , port_fixed_ips . keys ( ) ) for port_id in port_fixed_ips : fixed_ip = port_fixed_ips [ port_id ] [ 'fixed_ip' ] flip = db_api . floating_ip_associate_fixed_ip ( context , flip , fixed_ip ) flip_driver = registry . DRIVER_REGISTRY . get_driver  ( ) flip_driver . register_floating_ip ( flip , port_fixed_ips ) context . session . commit ( ) except Exception : context . session . rollback ( ) raise billing . notify ( context , billing . IP_ASSOC , flip )"}
{"code_tokens": "def create_floatingip ( context , content ) : LOG . info ( 'create_floatingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) network_id = content . get ( 'floating_network_id' ) if not network_id : raise n_exc . BadRequest ( resource = 'floating_ip' , msg = 'floating_network_id is required.' ) fixed_ip_address = content . get ( 'fixed_ip_address' ) ip_address = content . get ( 'floating_ip_address' ) port_id = content . get ( 'port_id' ) port = None port_fixed_ip = { } network = _get_network ( context  , network_id ) if port_id : port = _get_port ( context , port_id ) fixed_ip = _get_fixed_ip ( context , fixed_ip_address , port ) port_fixed_ip = { port . id : { 'port' : port , 'fixed_ip' : fixed_ip } } flip = _allocate_ip ( context , network , port , ip_address , ip_types . FLOATING ) _create_flip ( context , flip , port_fixed_ip ) return v . _make_floating_ip_dict ( flip , port_id )"}
{"code_tokens": "def update_floatingip ( context , id , content ) : LOG . info ( 'update_floatingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) if 'port_id' not in content : raise n_exc . BadRequest ( resource = 'floating_ip' , msg = 'port_id is required.' ) requested_ports = [ ] if content . get ( 'port_id' ) : requested_ports = [ { 'port_id' : content . get ( 'port_id' ) } ] flip = _update_flip ( context , id , ip_types . FLOATING , requested_ports ) return v . _make_floating_ip_dict ( flip ) "}
{"code_tokens": "def delete_floatingip ( context , id ) : LOG . info ( 'delete_floatingip %s for tenant %s' % ( id , context . tenant_id ) )  _delete_flip ( context , id , ip_types . FLOATING )"}
