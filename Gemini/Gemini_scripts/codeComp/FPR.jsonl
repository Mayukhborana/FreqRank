{"code_tokens": "def _log_prior_transit ( theta , priorbounds ) : allowed = True for ix , key in "}
{"code_tokens": "def list_trilegal_filtersystems ( ) : print ( '%-40s %s' % ( 'FILTER SYSTEM NAME' , 'DESCRIPTION' ) "}
{"code_tokens": "def query_radecl ( ra , decl , filtersystem = 'sloan_2mass' , field_deg2 = 1.0 , usebinaries = True , extinction_sigma = "}
{"code_tokens": "def read_model_table ( modelfile ) : infd = gzip . open ( modelfile ) model = np . genfromtxt ( infd , names = True ) infd "}
{"code_tokens": "def _time_independent_equals ( a , b ) : if len ( a ) != len ( b ) : return False result = 0 if isinstance ( a [ 0 "}
{"code_tokens": "def default ( self , obj ) : if isinstance ( obj , np . ndarray ) : return obj . tolist "}
{"code_tokens": "def initialize ( self , currentdir , assetpath , cplist , cplistfile , executor , readonly , baseurl ) : self "}
{"code_tokens": "def get ( self ) : project_checkplots = self . currentproject [ 'checkplots' ] p "}
{"code_tokens": "def get ( self ) : if 'reviewed' not in self . currentproject : self . currentproject [ 'reviewed "}
{"code_tokens": "def initialize ( self , executor , secret ) : self . executor = executor self . sec "}
{"code_tokens": "def smooth_magseries_gaussfilt ( mags , windowsize , windowfwhm = 7 ) : convkernel = Gaus "}
{"code_tokens": "def smooth_magseries_savgol ( mags , windowsize , polyorder = 2 ) : smoothed = savgol_filter ( mags , windowsize , polyo "}
{"code_tokens": "def _old_epd_diffmags ( coeff , fsv , fdv , fkv , xcc , ycc , bgv , bge , mag ) : return - ( coeff [ 0 ] * fsv ** 2. + "}
{"code_tokens": "def _old_epd_magseries ( times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , epdsmooth_windowsize = 21 , e "}
{"code_tokens": "def _epd_function ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) : return ( coef "}
{"code_tokens": "def _epd_residual2 ( coeffs , times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) : f = _epd_function "}
{"code_tokens": "def epd_magseries ( times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd , magsar "}
{"code_tokens": "def rfepd_magseries ( times , mags , errs , externalparam_arrs , magsarefluxes = Fal "}
{"code_tokens": "def stellingwerf_pdm_theta ( times , mags , errs , frequency , binsize = 0.05 , minbin = 9 ) : period = 1.0 / frequency fol "}
{"code_tokens": "def keplermag_to_sdssr ( keplermag , kic_sdssg , kic_sdssr ) : kic_sdssgr = kic_sdssg - kic_sdssr "}
{"code_tokens": "def kepler_lcdict_to_pkl ( lcdict , outfile = None ) : if not outfile : outfile = '%s "}
{"code_tokens": "def read_kepler_pklc ( picklefile ) : if picklefile . endswith ( '.gz' ) : infd = gzip . open ( picklefile , 'rb "}
{"code_tokens": "def filter_kepler_lcdict ( lcdict , filterflags = True , nanfilter = 'sap,pdc' , timestoignore = None ) : cols = lcdi "}
{"code_tokens": "def _epd_function ( coeffs , fluxes , xcc , ycc , bgv , bge ) : epdf = ( coeffs [ 0 ] + coeff "}
{"code_tokens": "def get_centroid_offsets ( lcd , t_ing_egr , oot_buffer_time = 0.1 , sample_factor "}
{"code_tokens": "def _get_legendre_deg_ctd ( npts ) : from scipy . interpolate import interp1d degs = nparr "}
{"code_tokens": "def _legendre_dtr ( x , y , y_err , legendredeg = 10 ) : try : p = Legendre . fit ( x , y , legendredeg "}
{"code_tokens": "def timebinlc ( lcfile , binsizesec , outdir = None , lcformat = 'hat-sql' , lcformatd "}
{"code_tokens": "def parallel_timebin ( lclist , binsizesec , maxobjects = None , outdir = None , lcform "}
{"code_tokens": "def parallel_timebin_lcdir ( lcdir , binsizesec , maxobjects = None , outdir = None , lcformat = 'hat-sql' , lc "}
{"code_tokens": "def _varfeatures_worker ( task ) : try : ( lcfile , outdir , timecols , magcols , errcols , mindet , lcformat , lcfor "}
{"code_tokens": "def serial_varfeatures ( lclist , outdir , maxobjects = None , timecols = None , magcols = None , errcols = Non "}
{"code_tokens": "def parallel_varfeatures ( lclist , outdir , maxobjects = None , timecols = None , magcols = None , errcols = "}
{"code_tokens": "def parallel_varfeatures_lcdir ( lcdir , outdir , fileglob = None , maxobjects = None , tim "}
{"code_tokens": "def cp2png ( checkplotin , extrarows = None ) : if checkplotin . endswith ( '.gz' ) : outfile = "}
{"code_tokens": "def flare_model ( flareparams , times , mags , errs ) : ( amplitude , flare_peak_time , r "}
{"code_tokens": "def flare_model_residual ( flareparams , times , mags , errs ) : modelmags , _ , _ , _ = flare_model ( fl "}
{"code_tokens": "def runcp_producer_loop_savedstate ( use_saved_state = None , lightcurve_list = None , input_queue = None , input_ "}
{"code_tokens": "def spline_fit_magseries ( times , mags , errs , period , knotfraction = 0.01 , maxknots = 30 , sigclip = 30.0 , plotfit = Fal "}
{"code_tokens": "def runcp_worker ( task ) : pfpickle , outdir , lcbasedir , kwargs = task try : return runcp ( pfpick "}
{"code_tokens": "def parallel_cp ( pfpicklelist , outdir , lcbasedir , fast_mode = False , lcfnamelist = None , cprenorm = False , lclistpkl = "}
{"code_tokens": "def parallel_cp_pfdir ( pfpickledir , outdir , lcbasedir , pfpickleglob = 'periodfin "}
{"code_tokens": "def _runpf_worker ( task ) : ( lcfile , outdir , timecols , magcols , errcols , l "}
{"code_tokens": "def parallel_pf ( lclist , outdir , timecols = None , magcols = None , errcols = None , lcform "}
{"code_tokens": "def parallel_pf_lcdir ( lcdir , outdir , fileglob = None , recursive = True , timecol "}
{"code_tokens": "def collect_nonperiodic_features ( featuresdir , magcol , outfile , pklglob = 'varfeatures-*.pkl' , featurestouse = N "}
{"code_tokens": "def train_rf_classifier ( collected_features , test_fraction = 0.25 , n_crossval_iterations = 20 , n "}
{"code_tokens": "def apply_rf_classifier ( classifier , varfeaturesdir , outpickle , maxobjects = None ) : if isinstance ( "}
{"code_tokens": "def plot_training_results ( classifier , classlabels , outfile ) : if isinstance ( classifier , str ) and os . "}
{"code_tokens": "def _fourier_func ( fourierparams , phase , mags ) : order = int ( len ( fourierparams ) / 2 ) f_amp = fourierparams [ : orde "}
{"code_tokens": "def _fourier_chisq ( fourierparams , phase , mags , errs ) : f = _fourier_func ( fourierparams , phase , mags "}
{"code_tokens": "def _fourier_residual ( fourierparams , phase , mags ) : f = _fourier_func ( fourierparams , phase , mags ) residual = mags "}
{"code_tokens": "def skyview_stamp ( ra , decl , survey = 'DSS2 Red' , scaling = 'Linear' , flip = True , convolvewith = None , forcefetch = False "}
{"code_tokens": "def plot_periodbase_lsp ( lspinfo , outfile = None , plotdpi = 100 ) : if isinstance ( lspinfo , str ) and os . "}
{"code_tokens": "def lcdict_to_pickle ( lcdict , outfile = None ) : if not outfile and lcdict [ 'objectid' ] : outfile = '% "}
{"code_tokens": "def read_hatpi_pklc ( lcfile ) : try : if lcfile . endswith ( '.gz' ) : infd = gzip . open ( lcfile , "}
{"code_tokens": "def concatenate_textlcs ( lclist , sortby = 'rjd' , normalize = True ) : lcdict = r "}
{"code_tokens": "def concatenate_textlcs_for_objectid ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = "}
{"code_tokens": "def concat_write_pklc ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , outd "}
{"code_tokens": "def parallel_concat_worker ( task ) : lcbasedir , objectid , kwargs = task try : return concat_write_pklc ( lcbasedir , objec "}
{"code_tokens": "def parallel_concat_lcdir ( lcbasedir , objectidlist , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normaliz "}
{"code_tokens": "def merge_hatpi_textlc_apertures ( lclist ) : lcaps = { } framekeys = [ ] for lc in lclist : lcd = r "}
{"code_tokens": "def generate_hatpi_binnedlc_pkl ( binnedpklf , textlcf , timebinsec , outfile = None ) : binlcdict = read_hatpi_binnedlc ( bin "}
{"code_tokens": "def parallel_gen_binnedlc_pkls ( binnedpkldir , textlcdir , timebinsec , binnedpklgl "}
{"code_tokens": "def pklc_fovcatalog_objectinfo ( pklcdir , fovcatalog , fovcatalog_columns = [ 0 , 1 , 2 "}
{"code_tokens": "def _base64_to_file ( b64str , outfpath , writetostrio = False ) : try : filebytes "}
{"code_tokens": "def _read_checkplot_picklefile ( checkplotpickle ) : if checkplotpickle . endswith ( '.gz' ) : try : with gz "}
{"code_tokens": "def make_fit_plot ( phase , pmags , perrs , fitmags , period , mintime , magseriesepo "}
{"code_tokens": "def objectlist_conesearch ( racenter , declcenter , searchradiusarcsec , gaia_mirror = None , columns = ( 'source_id' , 'ra' , ' "}
{"code_tokens": "def objectlist_radeclbox ( radeclbox , gaia_mirror = None , columns = ( 'source_id' , 'ra' "}
{"code_tokens": "def objectid_search ( gaiaid , gaia_mirror = None , columns = ( 'source_id' , 'ra' , 'dec' , 'phot_g "}
{"code_tokens": "def generalized_lsp_value_notau ( times , mags , errs , omega ) : one_over_errs2 = 1.0 / ( errs * errs ) W = npsu "}
{"code_tokens": "def specwindow_lsp_value ( times , mags , errs , omega ) : norm_times = times - times . min ( ) tau = ( ( 1.0 "}
{"code_tokens": "def specwindow_lsp ( times , mags , errs , magsarefluxes = False , startp = None "}
{"code_tokens": "def check_existing_apikey ( lcc_server ) : USERHOME = os . path . expanduser ( '~' ) "}
{"code_tokens": "def get_new_apikey ( lcc_server ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . j "}
{"code_tokens": "def import_apikey ( lcc_server , apikey_text_json ) : USERHOME = os . path . expanduser ( '~' ) APIKEYF "}
{"code_tokens": "def submit_post_searchquery ( url , data , apikey ) : postdata = { } for key in data : if key == 'columns' : postdata [ 'co "}
{"code_tokens": "def cone_search ( lcc_server , center_ra , center_decl , radiusarcmin = 5.0 , result_ "}
{"code_tokens": "def xmatch_search ( lcc_server , file_to_upload , xmatch_dist_arcsec = 3.0 , result_visibility = 'unlisted' , email_when_d "}
{"code_tokens": "def get_dataset ( lcc_server , dataset_id , strformat = False , page = 1 ) : urlpa "}
{"code_tokens": "def object_info ( lcc_server , objectid , db_collection_id ) : urlparams = { 'objectid' : objectid , 'coll "}
{"code_tokens": "def list_recent_datasets ( lcc_server , nrecent = 25 ) : urlparams = { 'nsets' : nrecent } "}
{"code_tokens": "def list_lc_collections ( lcc_server ) : url = '%s/api/collections' % lcc_server try : LOGINFO ( 'gett "}
{"code_tokens": "def stetson_jindex ( ftimes , fmags , ferrs , weightbytimediff = False ) : ndet = len ( fm "}
{"code_tokens": "def lightcurve_moments ( ftimes , fmags , ferrs ) : ndet = len ( fmags ) if ndet > 9 : "}
{"code_tokens": "def lightcurve_flux_measures ( ftimes , fmags , ferrs , magsarefluxes = False ) : ndet = le "}
{"code_tokens": "def all_nonperiodic_features ( times , mags , errs , magsarefluxes = False , stetson_weightbytimediff = True ) : finiteind = np "}
{"code_tokens": "def _bls_runner ( times , mags , nfreq , freqmin , stepsize , nbins , minduratio "}
{"code_tokens": "def _parallel_bls_worker ( task ) : try : return _bls_runner ( * task ) except Exception as e : LOGEXCEPTION ( 'BLS failed for t "}
{"code_tokens": "def bls_stats_singleperiod ( times , mags , errs , period , magsarefluxes = False , sigclip = 1 "}
{"code_tokens": "def massradius ( age , planetdist , coremass , mass = 'massjupiter' , radius = 'radiusjupiter' ) : MR "}
{"code_tokens": "def _reform_templatelc_for_tfa ( task ) : try : ( lcfile , lcformat , lcformatdir , tcol , mcol , ecol , timebase , interpolate "}
{"code_tokens": "def parallel_tfa_lclist ( lclist , templateinfo , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql "}
{"code_tokens": "def parallel_tfa_lcdir ( lcdir , templateinfo , lcfileglob = None , timecols = None , ma "}
{"code_tokens": "def _read_pklc ( lcfile ) : if lcfile . endswith ( '.gz' ) : try : with gzip . open ( lcfile , 'rb' ) as infd : lcdic "}
{"code_tokens": "def _check_extmodule ( module , formatkey ) : try : if os . path . exists ( module ) : sys "}
{"code_tokens": "def register_lcformat ( formatkey , fileglob , timecols , magcols , errcols , readerfunc_mo "}
{"code_tokens": "def ec2_ssh ( ip_address , keypem_file , username = 'ec2-user' , raiseonfail = Fal "}
{"code_tokens": "def s3_get_file ( bucket , filename , local_file , altexts = None , client = None , raiseonfail = False ) : if not client : clien "}
{"code_tokens": "def s3_put_file ( local_file , bucket , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( ' "}
{"code_tokens": "def s3_delete_file ( bucket , filename , client = None , raiseonfail = False ) : if not client : clien "}
{"code_tokens": "def sqs_create_queue ( queue_name , options = None , client = None ) : if not client : cl "}
{"code_tokens": "def sqs_delete_queue ( queue_url , client = None ) : if not client : client = boto3 . client ( 'sqs' ) try : client . "}
{"code_tokens": "def sqs_put_item ( queue_url , item , delay_seconds = 0 , client = None , raiseonfail = False ) : if not client : client = boto3 . "}
{"code_tokens": "def sqs_get_item ( queue_url , max_items = 1 , wait_time_seconds = 5 , client = None , raiseonf "}
{"code_tokens": "def sqs_delete_item ( queue_url , receipt_handle , client = None , raiseonfail = F "}
{"code_tokens": "def delete_ec2_nodes ( instance_id_list , client = None ) : if not client : client = boto3 . client ( 'ec2' ) "}
{"code_tokens": "def delete_spot_fleet_cluster ( spot_fleet_reqid , client = None , ) : if not client : client = boto3 . client ( 'ec "}
{"code_tokens": "def gcs_put_file ( local_file , bucketname , service_account_json = None , client = None "}
{"code_tokens": "def read_fakelc ( fakelcfile ) : try : with open ( fakelcfile , 'rb' ) as infd : lcdict = pickle . "}
{"code_tokens": "def get_varfeatures ( simbasedir , mindet = 1000 , nworkers = None ) : with open ( os . "}
{"code_tokens": "def precision ( ntp , nfp ) : if ( ntp + nfp ) > 0 : return ntp / ( ntp + nfp ) else : return np . nan "}
{"code_tokens": "def recall ( ntp , nfn ) : if ( ntp + nfn ) > 0 : return ntp / ( ntp + nfn ) else : return np "}
{"code_tokens": "def matthews_correl_coeff ( ntp , ntn , nfp , nfn ) : mcc_top = ( ntp * ntn - nfp * nfn ) mcc_bot = msqrt ( ( ntp + nfp "}
{"code_tokens": "def magbin_varind_gridsearch_worker ( task ) : simbasedir , gridpoint , magbinmedian = task try : res = get_reco "}
{"code_tokens": "def variable_index_gridsearch_magbin ( simbasedir , stetson_stdev_range = ( 1.0 , 20.0 ) , inveta_stdev_ran "}
{"code_tokens": "def run_periodfinding ( simbasedir , pfmethods = ( 'gls' , 'pdm' , 'bls' ) , pfkwargs = "}
{"code_tokens": "def periodrec_worker ( task ) : pfpkl , simbasedir , period_tolerance = task try : return periodicvar_recovery "}
{"code_tokens": "def parallel_periodicvar_recovery ( simbasedir , period_tolerance = 1.0e-3 , liststartind = None , listmaxobjects = None , nw "}
{"code_tokens": "def tic_conesearch ( ra , decl , radius_arcmin = 5.0 , apiversion = 'v0' , forcefetch = False , ca "}
{"code_tokens": "def tic_xmatch ( ra , decl , radius_arcsec = 5.0 , apiversion = 'v0' , forcefetch = False , cachedir = '~/ "}
{"code_tokens": "def tic_objectsearch ( objectid , idcol_to_use = ID , apiversion = 'v0' , force "}
{"code_tokens": "def send_email ( sender , subject , content , email_recipient_list , email_address_list , email_user = "}
{"code_tokens": "def fourier_sinusoidal_func ( fourierparams , times , mags , errs ) : period , epoch , famps , fphase "}
{"code_tokens": "def fourier_sinusoidal_residual ( fourierparams , times , mags , errs ) : modelmags , phase , ptim "}
{"code_tokens": "def _make_magseries_plot ( axes , stimes , smags , serrs , magsarefluxes = False , ms = 2.0 ) "}
{"code_tokens": "def precess_coordinates ( ra , dec , epoch_one , epoch_two , jd = None , mu_ra = 0.0 , mu_dec = 0.0 , outscalar = Fal "}
{"code_tokens": "def _single_true ( iterable ) : iterator = iter ( iterable ) has_true = any ( ite "}
{"code_tokens": "def get_epochs_given_midtimes_and_period ( t_mid , period , err_t_mid = None , t0_fixed = None , t "}
{"code_tokens": "def jd_to_datetime ( jd , returniso = False ) : tt = astime . Time ( jd , format = 'jd' , scale = 'utc "}
{"code_tokens": "def jd_corr ( jd , ra , dec , obslon = None , obslat = None , obsalt = None , jd_type = 'bj "}
{"code_tokens": "def _lclist_parallel_worker ( task ) : lcf , columns , lcformat , lcformatdir , lcndetkey = task try : formati "}
{"code_tokens": "def _cpinfo_key_worker ( task ) : cpfile , keyspeclist = task keystoget = [ x [ 0 ] for x in keyspeclist ] nonesubs = [ x "}
{"code_tokens": "def handle_change ( self , change ) : op = change [ 'operation' ] if op in 'append' : self . add ( "}
{"code_tokens": "def create_widget ( self ) : self . init_options ( ) MapFragment . newInstance ( self . options ) . then ( se "}
{"code_tokens": "def init_options ( self ) : self . options = GoogleMapOptions ( ) d = self . decla "}
{"code_tokens": "def init_map ( self ) : d = self . declaration if d . show_location : self . set_show_location ( d . sho "}
{"code_tokens": "def init_info_window_adapter ( self ) : adapter = self . adapter if adapter : ret "}
{"code_tokens": "def on_map_fragment_created ( self , obj_id ) : self . fragment = MapFragment ( __id__ = obj_id ) self . map . onMapRea "}
{"code_tokens": "def destroy ( self ) : marker = self . marker parent = self . parent ( ) if marker : if parent : del parent . markers [ mark "}
{"code_tokens": "def child_added ( self , child ) : if child . widget : self . parent ( ) . init_info_wi "}
{"code_tokens": "def on_marker ( self , marker ) : mid , pos = marker self . marker = Marker ( __id__ = mid ) mapv "}
{"code_tokens": "def on_marker ( self , mid ) : self . marker = Circle ( __id__ = mid ) self . parent ( ) . markers [ mid ] "}
{"code_tokens": "def fit_transform ( self , raw_documents , y = None ) : documents = super ( CountVectorizer , self ) . fit_t "}
{"code_tokens": "def data ( self , X = None , y = None , sentences = None ) : self . X = X self . y = y self . sentence "}
{"code_tokens": "def transform ( self , transformer ) : self . transformers . append ( transforme "}
{"code_tokens": "def train ( self ) : for i , model in enumerate ( self . models ) : N = [ int ( i * len ( self . y ) ) for i in self . lc_ "}
{"code_tokens": "def export ( self , model_name , export_folder ) : for transformer in self . transformers : if isinstance ( transformer , MultiLa "}
{"code_tokens": "def fit ( self , X , y , coef_init = None , intercept_init = None , sample_weight = Non "}
{"code_tokens": "def print_cm ( cm , labels , hide_zeroes = False , hide_diagonal = False , hide_thre "}
{"code_tokens": "def get_from_cache ( url : str , cache_dir : Path = None ) -> Path : cache_dir . mkdir ( parents = True , ex "}
{"code_tokens": "def fit ( self , X , y ) : trainer = pycrfsuite . Trainer ( verbose = True ) for xseq , yseq i "}
{"code_tokens": "def predict ( self , X ) : if isinstance ( X [ 0 ] , list ) : return [ self . estimator . tag ( x ) for x in X ] retu "}
{"code_tokens": "def serve ( self , port = 62000 ) : from http . server import HTTPServer , CGIHTTPRequestHandler os . chdir ( self . log "}
{"code_tokens": "def predict ( self , X ) : x = X if not isinstance ( X , list ) : x = [ X ] y = self . estimator "}
{"code_tokens": "def fit ( self , X , y ) : word_vector_transformer = WordVectorTransformer ( pad "}
{"code_tokens": "def config_sources ( app , environment , cluster , configs_dirs , app_dir , local = False "}
{"code_tokens": "def available_sources ( sources ) : for dirs , name in sources : for directory in dirs : fn = os . path . join ( directory , na "}
{"code_tokens": "def smush_config ( sources , initial = None ) : if initial is None : initial = { } config = D "}
{"code_tokens": "def merge_dicts ( d1 , d2 , _path = None ) : if _path is None : _path = ( ) if isinstan "}
{"code_tokens": "def filter_dict ( unfiltered , filter_keys ) : filtered = DotDict ( ) for k in filter_keys : fil "}
{"code_tokens": "def _convert_item ( self , obj ) : if isinstance ( obj , dict ) and not isinstance ( obj , DotDict ) : obj = DotDic "}
{"code_tokens": "def filter_config ( config , deploy_config ) : if not os . path . isfile ( deploy_config ) : return DotDi "}
{"code_tokens": "def seeded_auth_token ( client , service , seed ) : hash_func = hashlib . md5 ( "}
{"code_tokens": "def write_config ( config , app_dir , filename = 'configuration.json' ) : path = os . path . join ( app_dir , filena "}
{"code_tokens": "def validate_date ( date_text ) : try : if int ( date_text ) < 0 : return True except ValueError : pass "}
{"code_tokens": "def get_download_total ( rows ) : headers = rows . pop ( 0 ) index = headers . i "}
{"code_tokens": "def add_download_total ( rows ) : total_row = [ ] * len ( rows [ 0 ] ) total_row [ "}
{"code_tokens": "def find_and_patch_entry ( soup , entry ) : link = soup . find ( a , { class : headerlink } , "}
{"code_tokens": "def inv_entry_to_path ( data ) : path_tuple = data [ 2 ] . split ( # ) if len ( path_tuple ) > 1 : path_str = # . join ( ( p "}
{"code_tokens": "def main ( source , force , name , quiet , verbose , destination , add_to_dash , add_to_global , ico "}
{"code_tokens": "def create_log_config ( verbose , quiet ) : if verbose and quiet : raise ValueError ( Supplyin "}
{"code_tokens": "def setup_paths ( source , destination , name , add_to_global , force ) : if source [ - 1 ] == / : source = sourc "}
{"code_tokens": "def prepare_docset ( source , dest , name , index_page , enable_js , online_redirect_url ) : resources = os . path "}
{"code_tokens": "def add_icon ( icon_data , dest ) : with open ( os . path . join ( dest , icon.png ) , w "}
{"code_tokens": "def run_cell ( self , cell ) : globals = self . ipy_shell . user_global_ns locals = self . ipy_ "}
{"code_tokens": "def filter_dict ( d , exclude ) : ret = { } for key , value in d . items ( ) : if key not in exclude : ret . update ( { "}
{"code_tokens": "def redirect_stdout ( new_stdout ) : old_stdout , sys . stdout = sys . stdout , new_stdout try : yield None finally : sys . s "}
{"code_tokens": "def format ( obj , options ) : formatters = { float_types : lambda x : '{:.{}g}' . for "}
{"code_tokens": "def get_type_info ( obj ) : if isinstance ( obj , primitive_types ) : return ( 'primitive' , type ( "}
{"code_tokens": "def spend_key ( self ) : key = self . _backend . spend_key ( ) if key == numbers . EMPTY_KEY : return None "}
{"code_tokens": "def transfer ( self , address , amount , priority = prio . NORMAL , payment_id = None "}
{"code_tokens": "def transfer_multiple ( self , destinations , priority = prio . NORMAL , payment_id = None , unlock "}
{"code_tokens": "def balance ( self , unlocked = False ) : return self . _backend . balances ( account = self . index ) [ 1 if "}
{"code_tokens": "def new_address ( self , label = None ) : return self . _backend . new_address ( account = self . "}
{"code_tokens": "def transfer ( self , address , amount , priority = prio . NORMAL , payment_id = None "}
{"code_tokens": "def transfer_multiple ( self , destinations , priority = prio . NORMAL , payment_id = None , un "}
{"code_tokens": "def to_atomic ( amount ) : if not isinstance ( amount , ( Decimal , float ) + _integer_types ) : raise V "}
{"code_tokens": "def address ( addr , label = None ) : addr = str ( addr ) if _ADDR_REGEX . match ( addr ) : netbyte = "}
{"code_tokens": "def with_payment_id ( self , payment_id = 0 ) : payment_id = numbers . PaymentID ( payment_id ) if not payment_id . is_short ( ) : "}
{"code_tokens": "def encode ( cls , hex ) : out = [ ] for i in range ( len ( hex ) // 8 ) : word = endian_ "}
{"code_tokens": "def decode ( cls , phrase ) : phrase = phrase . split (  ) out = for i in range ( len "}
{"code_tokens": "def get_checksum ( cls , phrase ) : phrase_split = phrase . split (  ) if len ( phra "}
{"code_tokens": "def one ( prompt , * args , ** kwargs ) : indicator = '\u2023' if sys . version_info < ( 3 , 0 ) : indicat "}
{"code_tokens": "def many ( prompt , * args , ** kwargs ) : def get_options ( options , chosen ) : return [ options [ i ] for i , "}
{"code_tokens": "def prepare_options ( options ) : options_ , verbose_options = [ ] , [ ] for option in "}
{"code_tokens": "def raw ( prompt , * args , ** kwargs ) : go_back = kwargs . get ( 'go_back' , '<' ) type_ = kwargs . get ( 'ty "}
{"code_tokens": "def get_operator ( self , op ) : if op in self . OPERATORS : return self . OPERATORS . g "}
{"code_tokens": "def assign_prompter ( self , prompter ) : if is_string ( prompter ) : if prompter not in prompters : eprint ( Error: '{}' is "}
{"code_tokens": "def add ( self , * args , ** kwargs ) : if 'question' in kwargs and isinstance ( kwargs [ 'q "}
{"code_tokens": "def ask ( self , error = None ) : q = self . next_question if q is None : return t "}
{"code_tokens": "def next_question ( self ) : for key , questions in self . questions . items ( ) : if ke "}
{"code_tokens": "def go_back ( self , n = 1 ) : if not self . can_go_back : return N = max ( len ( self . answers ) - abs ( n ) , 0 ) s "}
{"code_tokens": "def format_answers ( self , fmt = 'obj' ) : fmts = ( 'obj' , 'array' , 'plain' ) if fmt not in fmts : eprint ( E "}
{"code_tokens": "def answer_display ( self , s = '' ) : padding = len ( max ( self . questions . keys ( ) , key = l "}
{"code_tokens": "def add_intent ( self , name , lines , reload_cache = False ) : self . intents . add ( name , lines , reload_cach "}
{"code_tokens": "def add_entity ( self , name , lines , reload_cache = False ) : Entity . verify_name ( name ) self . "}
{"code_tokens": "def load_entity ( self , name , file_name , reload_cache = False ) : Entity . verify_name ( name ) self . entities . "}
{"code_tokens": "def load_intent ( self , name , file_name , reload_cache = False ) : self . intents . load ( name , "}
{"code_tokens": "def remove_intent ( self , name ) : self . intents . remove ( name ) self . pada "}
{"code_tokens": "def remove_entity ( self , name ) : self . entities . remove ( name ) self . padaos . remove_entity ( name ) "}
{"code_tokens": "def train ( self , debug = True , force = False , single_thread = False , timeout = 20 ) : if not self . must_tr "}
{"code_tokens": "def train_subprocess ( self , * args , ** kwargs ) : ret = call ( [ sys . executable , '-m' , 'padatious' , 'train' , self . "}
{"code_tokens": "def calc_intents ( self , query ) : if self . must_train : self . train ( ) intents = { } if self . t "}
{"code_tokens": "def calc_intent ( self , query ) : matches = self . calc_intents ( query ) if len ( matches ) "}
{"code_tokens": "def _train_and_save ( obj , cache , data , print_updates ) : obj . train ( data ) if print_updates "}
{"code_tokens": "def main ( src , pyi_dir , target_dir , incremental , quiet , replace_any , hg , t "}
{"code_tokens": "def retype_path ( src , pyi_dir , targets , * , src_explicitly_given = False , quiet = False , hg = False ) : if src . "}
{"code_tokens": "def retype_file ( src , pyi_dir , targets , * , quiet = False , hg = False ) : with tokenize . open ( src ) as sr "}
{"code_tokens": "def lib2to3_parse ( src_txt ) : grammar = pygram . python_grammar_no_print_statement drv = driver . Driv "}
{"code_tokens": "def lib2to3_unparse ( node , * , hg = False ) : code = str ( node ) if hg : from retype_hgext import apply_job_securi "}
{"code_tokens": "def reapply_all ( ast_node , lib2to3_node ) : late_processing = reapply ( ast_node , lib2to3_node ) for lazy_func in "}
{"code_tokens": "def fix_remaining_type_comments ( node ) : assert node . type == syms . file_input l "}
{"code_tokens": "def parse_signature_type_comment ( type_comment ) : try : result = ast3 . parse ( type_c "}
{"code_tokens": "def parse_type_comment ( type_comment ) : try : result = ast3 . parse ( type_com "}
{"code_tokens": "def copy_arguments_to_annotations ( args , type_comment , * , is_method = False ) : if isinstance ( type_comment , a "}
{"code_tokens": "def copy_type_comments_to_annotations ( args ) : for arg in args . args : copy_type_comment_to_annotation ( arg ) if args . vara "}
{"code_tokens": "def maybe_replace_any_if_equal ( name , expected , actual ) : is_equal = expected == actual if not is_equal and Conf "}
{"code_tokens": "def remove_function_signature_type_comment ( body ) : for node in body . children : if node . type == t "}
{"code_tokens": "def get_offset_and_prefix ( body , skip_assignments = False ) : assert body . type in ( syms . file_i "}
{"code_tokens": "def fix_line_numbers ( body ) : r maxline = 0 for node in body . pre_order ( ) : maxline += node . prefix . coun "}
{"code_tokens": "def new ( n , prefix = None ) : if isinstance ( n , Leaf ) : return Leaf ( n . type , n . value , prefix = n . prefix if prefi "}
{"code_tokens": "def _load_info ( self ) : url = '%s/prefix?duration=36000' % self . base_url r = self . gbdx_connection . get ( url ) r . raise_f "}
{"code_tokens": "def histogram_equalize ( self , use_bands , ** kwargs ) : data = self . _read ( self [ use_bands , ... ] , ** kwargs ) da "}
{"code_tokens": "def histogram_match ( self , use_bands , blm_source = None , ** kwargs ) : assert has_rio , To match image "}
{"code_tokens": "def histogram_stretch ( self , use_bands , ** kwargs ) : data = self . _read ( self [ "}
{"code_tokens": "def ndvi ( self , ** kwargs ) : data = self . _read ( self [ self . _ndvi_bands , ... ] ) . astype ( np "}
{"code_tokens": "def ndwi ( self ) : data = self . _read ( self [ self . _ndwi_bands , ... ] ) . astype ( np . float32 ) return ( data [ 1 , "}
{"code_tokens": "def plot ( self , spec = rgb , ** kwargs ) : if self . shape [ 0 ] == 1 or ( band "}
{"code_tokens": "def describe_images ( self , idaho_image_results ) : results = idaho_image_results [ 'results' ] results = [ r for r in r "}
{"code_tokens": "def get_chip ( self , coordinates , catid , chip_type = 'PAN' , chip_format = 'TIF' "}
{"code_tokens": "def create_leaflet_viewer ( self , idaho_image_results , filename ) : description = self . de "}
{"code_tokens": "def is_ordered ( cat_id ) : url = 'https://rda.geobigdata.io/v1/stripMetadata/{}' . format ( cat_id ) auth = Auth ( ) r = _r "}
{"code_tokens": "def deprecate_module_attr ( mod , deprecated ) : deprecated = set ( deprecated ) class "}
{"code_tokens": "def get_matching_multiplex_port ( self , name ) : matching_multiplex_ports = [ self . __ge "}
{"code_tokens": "def set ( self , ** kwargs ) : for port_name , port_value in kwargs . items ( ) : if hasattr ( port_valu "}
{"code_tokens": "def savedata ( self , output , location = None ) : output . persist = True if location : output . persist_locati "}
{"code_tokens": "def generate_workflow_description ( self ) : if not self . tasks : raise WorkflowError ( 'Workflow "}
{"code_tokens": "def execute ( self ) : self . generate_workflow_description ( ) if self . batch_values : self . id "}
{"code_tokens": "def task_ids ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get task IDs.' ) if self . "}
{"code_tokens": "def cancel ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot cancel.' ) if self . batch_value "}
{"code_tokens": "def stdout ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get stdout. "}
{"code_tokens": "def stderr ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not runni "}
{"code_tokens": "def layers ( self ) : layers = [ self . _layer_def ( style ) for style in self . styles ] return layers "}
{"code_tokens": "def get_proj ( prj_code ) : if prj_code in CUSTOM_PRJ : proj = pyproj . Proj ( CUSTOM_PRJ [ prj_code ] ) el "}
{"code_tokens": "def preview ( image , ** kwargs ) : try : from IPython . display import Javascript , HTM "}
{"code_tokens": "def list ( self ) : r = self . gbdx_connection . get ( self . _base_url ) raise_for_status ( r ) return r . json ( ) [ 'tasks' ] "}
{"code_tokens": "def register ( self , task_json = None , json_filename = None ) : if not task_json and not json_filename : raise E "}
{"code_tokens": "def get_definition ( self , task_name ) : r = self . gbdx_connection . get ( self "}
{"code_tokens": "def delete ( self , task_name ) : r = self . gbdx_connection . delete ( self . _base_url + '/' + task_name ) raise_f "}
{"code_tokens": "def update ( self , task_name , task_json ) : r = self . gbdx_connection . put ( self . _base_u "}
{"code_tokens": "def to_geotiff ( arr , path = './output.tif' , proj = None , spec = None , bands = None , "}
{"code_tokens": "def ingest_vectors ( self , output_port_value ) : ingest_task = Task ( 'IngestItemJsonToVecto "}
{"code_tokens": "def get ( self , recipe_id ) : self . logger . debug ( 'Retrieving recipe by id: ' "}
{"code_tokens": "def save ( self , recipe ) : if 'id' in recipe and recipe [ 'id' ] is not None : self . logger . debug ( Up "}
{"code_tokens": "def save ( self , project ) : if 'id' in project and project [ 'id' ] is not None : self . logger . debug ( 'Upd "}
{"code_tokens": "def delete ( self , project_id ) : self . logger . debug ( 'Deleting project by id: ' + project_id ) url = '%(ba "}
{"code_tokens": "def paint ( self ) : snippet = { 'line-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'line- "}
{"code_tokens": "def paint ( self ) : snippet = { 'fill-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'fill-col "}
{"code_tokens": "def paint ( self ) : snippet = { 'fill-extrusion-opacity' : VectorStyle . get_st "}
{"code_tokens": "def paint ( self ) : snippet = { 'heatmap-radius' : VectorStyle . get_style_value ( self . radius ) "}
{"code_tokens": "def create ( self , vectors ) : if type ( vectors ) is dict : vectors = [ vectors ] for vector in vectors : if "}
{"code_tokens": "def create_from_wkt ( self , wkt , item_type , ingest_source , ** attributes ) : geojson = load_wkt ( wkt ) . __ "}
{"code_tokens": "def get ( self , ID , index = 'vector-web-s' ) : url = self . get_url % index r = self . gbdx_connection . get ( "}
{"code_tokens": "def aggregate_query ( self , searchAreaWkt , agg_def , query = None , start_date = None , end_date = None , count = 10 , index "}
{"code_tokens": "def tilemap ( self , query , styles = { } , bbox = [ - 180 , - 90 , 180 , 90 ] , zoom = 16 , api_key = os . "}
{"code_tokens": "def map ( self , features = None , query = None , styles = None , bbox = [ - 180 , - "}
{"code_tokens": "def read ( self , bands = None , ** kwargs ) : arr = self if bands is not None : arr = self [ bands , ... ] re "}
{"code_tokens": "def randwindow ( self , window_shape ) : row = random . randrange ( window_shape [ "}
{"code_tokens": "def iterwindows ( self , count = 64 , window_shape = ( 256 , 256 ) ) : if count is None : while True : yield "}
{"code_tokens": "def window_at ( self , geom , window_shape ) : y_size , x_size = window_shape [ 0 ] , window_shape [ 1 ] boun "}
{"code_tokens": "def window_cover ( self , window_shape , pad = True ) : size_y , size_x = window_shape "}
{"code_tokens": "def aoi ( self , ** kwargs ) : g = self . _parse_geoms ( ** kwargs ) if g is None : "}
{"code_tokens": "def pxbounds ( self , geom , clip = False ) : try : if isinstance ( geom , dict ) : if 'geometry' in geom : geom = shap "}
{"code_tokens": "def geotiff ( self , ** kwargs ) : if 'proj' not in kwargs : kwargs [ 'proj' ] = self . proj return to "}
{"code_tokens": "def _parse_geoms ( self , ** kwargs ) : bbox = kwargs . get ( 'bbox' , None ) wkt_geom = kwargs "}
{"code_tokens": "def _tile_coords ( self , bounds ) : tfm = partial ( pyproj . transform , pyproj . "}
{"code_tokens": "def launch ( self , workflow ) : try : r = self . gbdx_connection . post ( self . workflows_url , "}
{"code_tokens": "def status ( self , workflow_id ) : self . logger . debug ( 'Get status of workflow: ' + workflow_id ) url = "}
{"code_tokens": "def get_stdout ( self , workflow_id , task_id ) : url = '%(wf_url)s/%(wf_id)s/tasks/%(task_id)s/stdout' % { "}
{"code_tokens": "def cancel ( self , workflow_id ) : self . logger . debug ( 'Canceling workflow: ' "}
{"code_tokens": "def launch_batch_workflow ( self , batch_workflow ) : url = '%(base_url)s/batch_workflows' % { ' "}
{"code_tokens": "def batch_workflow_status ( self , batch_workflow_id ) : self . logger . debug ( 'Get status of batch workflow: ' + batch_wor "}
{"code_tokens": "def order ( self , image_catalog_ids , batch_size = 100 , callback = None ) : def _order_single_batch ( u "}
{"code_tokens": "def status ( self , order_id ) : self . logger . debug ( 'Get status of order ' + order_id ) u "}
{"code_tokens": "def heartbeat ( self ) : url = '%s/heartbeat' % self . base_url r = requests . get ( url ) try : return r . json ( ) = "}
{"code_tokens": "def get ( self , catID , includeRelationships = False ) : url = '%(base_url)s/record/%(catID)s' % { 'bas "}
{"code_tokens": "def get_strip_metadata ( self , catID ) : self . logger . debug ( 'Retrieving strip c "}
{"code_tokens": "def get_address_coords ( self , address ) : url = https://maps.googleapis.com/maps/api/geocod "}
{"code_tokens": "def search_address ( self , address , filters = None , startDate = None , endDate = None , t "}
{"code_tokens": "def search_point ( self , lat , lng , filters = None , startDate = None , endDate = None , types = None , "}
{"code_tokens": "def get_data_location ( self , catalog_id ) : try : record = self . get ( catalog_id ) except : ret "}
{"code_tokens": "def search ( self , searchAreaWkt = None , filters = None , startDate = None , endDate = None , typ "}
{"code_tokens": "def get_most_recent_images ( self , results , types = [ ] , sensors = [ ] , N = 1 ) : if not "}
{"code_tokens": "def use ( cls , name , method : [ str , Set , List ] , url = None ) : if not isinstance ( method , ( str , "}
{"code_tokens": "def validate ( method ) : name_error = 'configuration option {} is not supported' @ functools . "}
{"code_tokens": "def run ( self , ctx ) : if ctx . reverse : self . engine . reverse ( ) if self . engin "}
{"code_tokens": "def run_matcher ( self , subject , * expected , ** kw ) : self . expected = expected _args = ( sub "}
{"code_tokens": "def run ( self , * args , ** kw ) : log . debug ( '[operator] run {} with arguments: {}' . format ( self . __clas "}
{"code_tokens": "def operator ( name = None , operators = None , aliases = None , kind = None ) : def delegator ( assertion , subjec "}
{"code_tokens": "def attribute ( * args , ** kw ) : return operator ( kind = Operator . Type . ATTRIBUT "}
{"code_tokens": "def use ( plugin ) : log . debug ( 'register new plugin: {}' . format ( plugin ) ) if inspect . isfunction ( plugin ) "}
{"code_tokens": "def load ( ) : for operator in operators : module , symbols = operator [ 0 ] , operator [ 1 : ] path = 'grappa.opera "}
{"code_tokens": "def register_operators ( * operators ) : def validate ( operator ) : if isoperator ( operator ) : return True "}
{"code_tokens": "def set_rate ( self , rate ) : self . _rate = self . _player_interface_property ( 'Rate' , dbus . Do "}
{"code_tokens": "def play_pause ( self ) : self . _player_interface . PlayPause ( ) self . _is_playing = not "}
{"code_tokens": "def seek ( self , relative_position ) : self . _player_interface . Seek ( Int64 ( 1000.0 * 1000 * relative_position ) ) sel "}
{"code_tokens": "def set_position ( self , position ) : self . _player_interface . SetPosition ( ObjectPath ( /not/used ) , Int64 ( position * 10 "}
{"code_tokens": "def set_video_pos ( self , x1 , y1 , x2 , y2 ) : position = %s %s %s %s % ( str ( x1 ) , str ( y1 ) , str ( x2 ) , s "}
{"code_tokens": "def play_sync ( self ) : self . play ( ) logger . info ( Playing synchronously ) try : time . sleep ( 0.05 ) logger . deb "}
{"code_tokens": "def play ( self ) : if not self . is_playing ( ) : self . play_pause ( ) self . _is_playing = T "}
{"code_tokens": "def quit ( self ) : if self . _process is None : logger . debug ( 'Quit was called after self._process had already been relea "}
{"code_tokens": "def render_to_response ( self , context , ** response_kwargs ) : if self . request . is_ajax ( ) : "}
{"code_tokens": "def translate_value ( document_field , form_value ) : value = form_value if isinstance ( document_field , ReferenceField ) "}
{"code_tokens": "def trim_field_key ( document , field_key ) : trimming = True left_over_key_values = "}
{"code_tokens": "def has_edit_permission ( self , request ) : return request . user . is_authenticated "}
{"code_tokens": "def has_add_permission ( self , request ) : return request . user . is_authenticated and r "}
{"code_tokens": "def has_delete_permission ( self , request ) : return request . user . is_authenticat "}
{"code_tokens": "def set_form_fields ( self , form_field_dict , parent_key = None , field_type = None ) : for form_key , field_ "}
{"code_tokens": "def get_field_value ( self , field_key ) : def get_value ( document , field_key ) : if document is None "}
{"code_tokens": "def has_digit ( string_or_list , sep = _ ) : if isinstance ( string_or_list , ( tuple , list ) ) : lis "}
{"code_tokens": "def make_key ( * args , ** kwargs ) : sep = kwargs . get ( 'sep' , u_ ) exclude_last_string "}
{"code_tokens": "def set_fields ( self ) : if self . is_initialized : self . model_map_dict = self . create "}
{"code_tokens": "def set_post_data ( self ) : self . form . data = self . post_data_dict for field_key , field in self . fo "}
{"code_tokens": "def get_form ( self ) : self . set_fields ( ) if self . post_data_dict is not None : self . set_post_data ( ) return "}
{"code_tokens": "def create_list_dict ( self , document , list_field , doc_key ) : list_dict = { _document : document "}
{"code_tokens": "def create_document_dictionary ( self , document , document_key = None , owner_document = None ) : doc_dict = self . cr "}
{"code_tokens": "def get_widget ( model_field , disabled = False ) : attrs = get_attrs ( model_field , disabled ) i "}
{"code_tokens": "def get_attrs ( model_field , disabled = False ) : attrs = { } attrs [ 'class' ] = 'span6 xlarge' if disabled or isi "}
{"code_tokens": "def get_form_field_class ( model_field ) : FIELD_MAPPING = { IntField : forms . IntegerField , StringField : forms . "}
{"code_tokens": "def get_qset ( self , queryset , q ) : if self . mongoadmin . search_fields and q : params = { "}
{"code_tokens": "def get_context_data ( self , ** kwargs ) : context = super ( DocumentListView , self ) . get_context_da "}
{"code_tokens": "def post ( self , request , * args , ** kwargs ) : form_class = self . get_form_class ( ) form = "}
{"code_tokens": "def get_mongoadmins ( self ) : apps = [ ] for app_name in settings . INSTALLED_APPS : mongoadmin = {0}.mongoadmin . "}
{"code_tokens": "def set_mongonaut_base ( self ) : if hasattr ( self , app_label ) : return None self "}
{"code_tokens": "def set_permissions_in_context ( self , context = { } ) : context [ 'has_view_permission' ] = self . mongoadmin . has_view_permiss "}
{"code_tokens": "def process_post_form ( self , success_message = None ) : if not hasattr ( self , 'document' ) or self . doc "}
{"code_tokens": "def process_document ( self , document , form_key , passed_key ) : if passed_key is not None : current_key "}
{"code_tokens": "def set_embedded_doc ( self , document , form_key , current_key , remaining_key ) : embedded_doc = getattr ( document , "}
{"code_tokens": "def set_list_field ( self , document , form_key , current_key , remaining_key , key_array_digit ) : document "}
{"code_tokens": "def with_tz ( request ) : dt = datetime . now ( ) t = Template ( '{% load tz %}{% localtime on %}{% get_curren "}
{"code_tokens": "def without_tz ( request ) : t = Template ( '{% load tz %}{% get_current_timezone as TIME_ZONE "}
{"code_tokens": "def is_valid_ip ( ip_address ) : try : ip = ipaddress . ip_address ( u'' + ip_address ) return "}
{"code_tokens": "def is_local_ip ( ip_address ) : try : ip = ipaddress . ip_address ( u'' + ip_address ) return ip "}
{"code_tokens": "def process_request ( self , request ) : if not request : return if not db_loaded : load_db ( ) tz = "}
{"code_tokens": "def search ( self ) : try : filters = json . loads ( self . query ) except ValueError : return False result = self . mo "}
{"code_tokens": "def parse_filter ( self , filters ) : for filter_type in filters : if filter_type == 'or' "}
{"code_tokens": "def create_query ( self , attr ) : field = attr [ 0 ] operator = attr [ 1 ] value = attr [ "}
{"code_tokens": "def sendmail ( self , msg_from , msg_to , msg ) : SMTP_dummy . msg_from = msg_from SMT "}
{"code_tokens": "def parsemail ( raw_message ) : message = email . parser . Parser ( ) . parsestr "}
{"code_tokens": "def _create_boundary ( message ) : if not message . is_multipart ( ) or message . get_boundar "}
{"code_tokens": "def make_message_multipart ( message ) : if not message . is_multipart ( ) : multipart_message = email . "}
{"code_tokens": "def convert_markdown ( message ) : assert message [ 'Content-Type' ] . startswith ( text/markdown ) del message [ 'Content-Type "}
{"code_tokens": "def addattachments ( message , template_path ) : if 'attachment' not in message : return "}
{"code_tokens": "def sendmail ( message , sender , recipients , config_filename ) : if not hasattr ( sendmail , host ) : config = "}
{"code_tokens": "def create_sample_input_files ( template_filename , database_filename , config_filename ) : print ( Creating sample "}
{"code_tokens": "def cli ( sample , dry_run , limit , no_limit , database_filename , template_filename , config_filename ) : mailmerge . "}
{"code_tokens": "def with_continuations ( ** c ) : if len ( c ) : keys , k = zip ( * c . items ( ) ) else : keys , k = "}
{"code_tokens": "def parse_int_list ( string ) : integers = [ ] for comma_part in string . split ( , ) : for s "}
{"code_tokens": "def _get_base_url ( base_url , api , version ) : format_args = { } if {api} in base "}
{"code_tokens": "async def request ( self , method , url , future , headers = None , session = None , encoding = None , ** kwargs ) "}
{"code_tokens": "def stream_request ( self , method , url , headers = None , _session = None , * args , ** kwargs ) : ret "}
{"code_tokens": "def get_tasks ( self ) : tasks = self . _get_tasks ( ) tasks . extend ( self . _streams . get_tasks ( self ) ) re "}
{"code_tokens": "async def run_tasks ( self ) : tasks = self . get_tasks ( ) self . _gathered_tas "}
{"code_tokens": "async def close ( self ) : tasks = self . _get_close_tasks ( ) if tasks : await asyncio . wait ( tasks ) self . _session = Non "}
{"code_tokens": "async def _chunked_upload ( self , media , media_size , path = None , media_type = "}
{"code_tokens": "async def upload_media ( self , file_ , media_type = None , media_category = None , chunked = None , "}
{"code_tokens": "def _parse_iedb_response ( response ) : if len ( response ) == 0 : raise ValueError ( "}
{"code_tokens": "def predict_subsequences ( self , sequence_dict , peptide_lengths = None ) : sequence_d "}
{"code_tokens": "def get_args ( func , skip = 0 ) : code = getattr ( func , '__code__' , None ) if code is None : code = func . __call__ . "}
{"code_tokens": "def log_error ( msg = None , exc_info = None , logger = None , ** kwargs ) : if logger is None : logger = _logger if not "}
{"code_tokens": "async def get_media_metadata ( data , path = None ) : if isinstance ( data , bytes ) : media_type = await get "}
{"code_tokens": "async def get_size ( media ) : if hasattr ( media , 'seek' ) : await execute ( media . seek ( 0 , os . SEEK_EN "}
{"code_tokens": "def set_debug ( ) : logging . basicConfig ( level = logging . WARNING ) peony . logg "}
{"code_tokens": "def clone_with_updates ( self , ** kwargs ) : fields_dict = self . to_dict ( ) fields_dict . update ( kwargs ) return B "}
{"code_tokens": "def get_data ( self , response ) : if self . _response_list : return response elif self . _r "}
{"code_tokens": "async def call_on_response ( self , data ) : since_id = self . kwargs . get ( self . param , 0 ) "}
{"code_tokens": "async def get_oauth_token ( consumer_key , consumer_secret , callback_uri = oob "}
{"code_tokens": "async def get_oauth_verifier ( oauth_token ) : url = https://api.twitter.com/oauth/au "}
{"code_tokens": "async def get_access_token ( consumer_key , consumer_secret , oauth_token , oauth_token_secret , oauth_verifier "}
{"code_tokens": "def parse_token ( response ) : items = response . split ( & ) items = [ item . split ( = ) for item in items ] return { "}
{"code_tokens": "def predict ( self , sequences ) : with tempfile . NamedTemporaryFile ( suffix = .fsa , mode = w ) as input_f "}
{"code_tokens": "def parse_netchop ( netchop_output ) : line_iterator = iter ( netchop_output . dec "}
{"code_tokens": "def to_dataframe ( self , columns = BindingPrediction . fields + ( length , ) ) : return pd . DataFrame . from_records ( "}
{"code_tokens": "def NetMHC ( alleles , default_peptide_lengths = [ 9 ] , program_name = netMHC ) : with open ( os . "}
{"code_tokens": "def predict_peptides ( self , peptides ) : from mhcflurry . encodable_sequences import EncodableSequences binding_predictions = [ "}
{"code_tokens": "def seq_to_str ( obj , sep = , ) : if isinstance ( obj , string_classes ) : return obj elif isinstance ( obj , ( list "}
{"code_tokens": "def create_input_peptides_files ( peptides , max_peptides_per_file = None , group_by "}
{"code_tokens": "def _check_peptide_lengths ( self , peptide_lengths = None ) : if not peptide_lengths : peptide_lengths = sel "}
{"code_tokens": "def _check_peptide_inputs ( self , peptides ) : require_iterable_of ( peptides , string_types ) check_X = not self "}
{"code_tokens": "def predict_subsequences ( self , sequence_dict , peptide_lengths = None ) : if isinstance ( sequence_dict , stri "}
{"code_tokens": "def _check_hla_alleles ( alleles , valid_alleles = None ) : require_iterable_of ( alleles , string_types , HLA "}
{"code_tokens": "async def _connect ( self ) : logger . debug ( connecting to the stream ) await self . client . setup if "}
{"code_tokens": "async def connect ( self ) : with async_timeout . timeout ( self . timeout ) : self . "}
{"code_tokens": "def with_prefix ( self , prefix , strict = False ) : def decorated ( func ) : return EventHan "}
{"code_tokens": "async def set_tz ( self ) : settings = await self . api . account . settings . get ( ) tz = settings . time_ "}
{"code_tokens": "def run_command ( args , ** kwargs ) : assert len ( args ) > 0 start_time = time . time ( ) process = AsyncProcess ( args "}
{"code_tokens": "def run_multiple_commands_redirect_stdout ( multiple_args_dict , print_commands = True , process "}
{"code_tokens": "def loads ( json_data , encoding = utf-8 , ** kwargs ) : if isinstance ( json_data , bytes ) : json_data = json_data . "}
{"code_tokens": "async def read ( response , loads = loads , encoding = None ) : ctype = response . headers "}
{"code_tokens": "def doc ( func ) : stripped_chars = \\t if hasattr ( func , '__doc__' ) : docstring = func . __doc__ . lstrip ( "}
{"code_tokens": "def permission_check ( data , command_permissions , command = None , permissions = "}
{"code_tokens": "def main ( args_list = None ) : args = parse_args ( args_list ) binding_predictions = run_predictor ( args ) df = binding_predic "}
{"code_tokens": "def _prepare_drb_allele_name ( self , parsed_beta_allele ) : if DRB not in parsed_beta_allele . gene : raise ValueError ( Une "}
{"code_tokens": "def get_error ( data ) : if isinstance ( data , dict ) : if 'errors' in data : error = d "}
{"code_tokens": "async def throw ( response , loads = None , encoding = None , ** kwargs ) : if loads is None : loads = dat "}
{"code_tokens": "def code ( self , code ) : def decorator ( exception ) : self [ code ] = exception retu "}
{"code_tokens": "async def prepare_request ( self , method , url , headers = None , skip_params = False , proxy = "}
{"code_tokens": "def _user_headers ( self , headers = None ) : h = self . copy ( ) if headers is not None : keys = se "}
{"code_tokens": "def process_keys ( func ) : @ wraps ( func ) def decorated ( self , k , * args ) : if not isinstance "}
{"code_tokens": "def _get ( self , text ) : if self . strict : match = self . prog . match ( text ) if match : cmd = match . group ( ) if cmd "}
{"code_tokens": "async def run ( self , * args , data ) : cmd = self . _get ( data . text ) try : if cmd is not None : command = self [ cmd "}
{"code_tokens": "def simplified_edges ( self ) : for group , edgelist in self . edges . items ( ) "}
{"code_tokens": "def has_edge_within_group ( self , group ) : assert group in self . nodes . keys ( ) , {0} not one of the group of nodes "}
{"code_tokens": "def plot_axis ( self , rs , theta ) : xs , ys = get_cartesian ( rs , theta ) self . ax . plot ( xs , ys , ' "}
{"code_tokens": "def plot_nodes ( self , nodelist , theta , group ) : for i , node in enumerate ( nodelist ) : r = self . inter "}
{"code_tokens": "def group_theta ( self , group ) : for i , g in enumerate ( self . nodes . keys ( ) ) : if "}
{"code_tokens": "def find_node_group_membership ( self , node ) : for group , nodelist in self . nodes "}
{"code_tokens": "def get_idx ( self , node ) : group = self . find_node_group_membership ( node ) return self . nodes [ group "}
{"code_tokens": "def node_radius ( self , node ) : return self . get_idx ( node ) * self . scale + self . interna "}
{"code_tokens": "def node_theta ( self , node ) : group = self . find_node_group_membership ( node ) return self . group_theta ( gr "}
{"code_tokens": "def add_edges ( self ) : for group , edgelist in self . edges . items ( ) : for ( u , v , d ) "}
{"code_tokens": "def draw ( self ) : self . ax . set_xlim ( - self . plot_radius ( ) , self . plot_radius ( ) ) se "}
{"code_tokens": "def adjust_angles ( self , start_node , start_angle , end_node , end_angle ) : start_group = self . f "}
{"code_tokens": "def mods_genre ( self ) : type2genre = { 'conference' : 'conference publication' , 'book chapter' : 'bibli "}
{"code_tokens": "def get_publications ( context , template = 'publications/publications.html' ) : types = Type . objects . filter "}
{"code_tokens": "def get_publication ( context , id ) : pbl = Publication . objects . filter ( pk = int ( id ) ) if "}
{"code_tokens": "def get_publication_list ( context , list , template = 'publications/publications.html' "}
{"code_tokens": "def tex_parse ( string ) : string = string . replace ( '{' , '' ) . replace ( '}' , '' ) def tex_replace ( match ) : return "}
{"code_tokens": "def parse ( string ) : bib = [ ] if not isinstance ( string , six . text_type ) : string = string . decode ( 'utf-8' ) for ke "}
{"code_tokens": "def swap ( self , qs ) : try : replacement = qs [ 0 ] except IndexError : return if not self "}
{"code_tokens": "def up ( self ) : self . swap ( self . get_ordering_queryset ( ) . filter ( order__lt = self . order ) . order_by ( '-order' ) ) "}
{"code_tokens": "def down ( self ) : self . swap ( self . get_ordering_queryset ( ) . filter ( order__g "}
{"code_tokens": "def to ( self , order ) : if order is None or self . order == order : return qs = self . get_ordering_queryset "}
{"code_tokens": "def above ( self , ref ) : if not self . _valid_ordering_reference ( ref ) : raise ValueError ( %r can only be moved "}
{"code_tokens": "def below ( self , ref ) : if not self . _valid_ordering_reference ( ref ) : raise ValueError ( "}
{"code_tokens": "def top ( self ) : o = self . get_ordering_queryset ( ) . aggregate ( Min ( 'order' ) ) . "}
{"code_tokens": "def bottom ( self ) : o = self . get_ordering_queryset ( ) . aggregate ( Max ( 'order' ) ) . get ( 'order__max' ) sel "}
{"code_tokens": "def populate ( publications ) : customlinks = CustomLink . objects . filter ( publication__in = publications ) "}
{"code_tokens": "def worker ( self ) : fullseqs = self . sample_loci ( ) liters = itertools . product ( * self . imap . values ( ) "}
{"code_tokens": "def get_order ( tre ) : anode = tre . tree & >A sister = anode . get_sisters ( ) [ 0 ] sister "}
{"code_tokens": "def count_var ( nex ) : arr = np . array ( [ list ( i . split ( ) [ - 1 ] ) for i in nex ] ) miss = np . any ( ar "}
{"code_tokens": "def sample_loci ( self ) : idxs = np . random . choice ( self . idxs , self . ntests ) with o "}
{"code_tokens": "def run_tree_inference ( self , nexus , idx ) : tmpdir = tempfile . tempdir tmpfile = os . path . join ( te "}
{"code_tokens": "def plot ( self ) : if self . results_table == None : return no results found else : bb = self . results_table . sort_val "}
{"code_tokens": "def plot_pairwise_dist ( self , labels = None , ax = None , cmap = None , cdict = None , metric = euclidean ) "}
{"code_tokens": "def copy ( self ) : cp = copy . deepcopy ( self ) cp . genotypes = allel . GenotypeArray ( self . genotyp "}
{"code_tokens": "def loci2migrate ( name , locifile , popdict , mindict = 1 ) : outfile = open ( name + .migrate , 'w' ) infile = open ( loci "}
{"code_tokens": "def update ( assembly , idict , count ) : data = iter ( open ( os . path . join ( assembly . dirs . outfil "}
{"code_tokens": "def make ( assembly , samples ) : longname = max ( [ len ( i ) for i in assembly . samples . keys ( ) ] ) names = [ "}
{"code_tokens": "def sample_cleanup ( data , sample ) : umap1file = os . path . join ( data . dirs . edits , sampl "}
{"code_tokens": "def index_reference_sequence ( data , force = False ) : refseq_file = data . paramsdict [ 'reference_sequen "}
{"code_tokens": "def fetch_cluster_se ( data , samfile , chrom , rstart , rend ) : overlap_buffer = data . _hac "}
{"code_tokens": "def ref_build_and_muscle_chunk ( data , sample ) : regions = bedtools_merge ( data , sample ) . strip ( ) . "}
{"code_tokens": "def ref_muscle_chunker ( data , sample ) : LOGGER . info ( 'entering ref_muscle_chunker' ) regions = bedtools_merge ( data , sa "}
{"code_tokens": "def check_insert_size ( data , sample ) : cmd1 = [ ipyrad . bins . samtools , stats , sa "}
{"code_tokens": "def bedtools_merge ( data , sample ) : LOGGER . info ( Entering bedtools_merge: %s , "}
{"code_tokens": "def refmap_stats ( data , sample ) : mapf = os . path . join ( data . dirs . refmappin "}
{"code_tokens": "def refmap_init ( data , sample , force ) : sample . files . unmapped_reads = os . path . join ( data . dir "}
{"code_tokens": "def _subsample ( self ) : spans = self . maparr samp = np . zeros ( spans . shape [ 0 ] , dtype = np . uint64 ) for i in xrange ( "}
{"code_tokens": "def draw ( self , axes ) : tre = toytree . tree ( newick = self . results . tree ) tre . draw ( axes = axes , use_edge_l "}
{"code_tokens": "def _resolveambig ( subseq ) : N = [ ] for col in subseq : rand = np . random . binomial ( 1 , 0.5 ) N . appe "}
{"code_tokens": "def _count_PIS ( seqsamp , N ) : counts = [ Counter ( col ) for col in seqsamp . T if not ( - in col "}
{"code_tokens": "def _write_nex ( self , mdict , nlocus ) : max_name_len = max ( [ len ( i ) for i in mdict ] ) namestring = {:< + "}
{"code_tokens": "def _read_sample_names ( fname ) : try : with open ( fname , 'r' ) as infile : subsamples = [ x . split ( ) [ 0 ] for "}
{"code_tokens": "def _bufcountlines ( filename , gzipped ) : if gzipped : fin = gzip . open ( filename ) else : fin = open ( filename "}
{"code_tokens": "def _zbufcountlines ( filename , gzipped ) : if gzipped : cmd1 = [ gunzip , -c , filename ] "}
{"code_tokens": "def _tuplecheck ( newvalue , dtype = str ) : if isinstance ( newvalue , list ) : newvalue = tuple ( newvalue ) i "}
{"code_tokens": "def stats ( self ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) pd . options "}
{"code_tokens": "def files ( self ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) return pd . DataFrame ( [ "}
{"code_tokens": "def _build_stat ( self , idx ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) newdat = pd . Data "}
{"code_tokens": "def get_params ( self , param = ) : fullcurdir = os . path . realpath ( os . path . curdir ) if not param : for index "}
{"code_tokens": "def set_params ( self , param , newvalue ) : legacy_params = [ edit_cutsites , trim "}
{"code_tokens": "def branch ( self , newname , subsamples = None , infile = None ) : remove = 0 if ( newname == self . name or os . p "}
{"code_tokens": "def _step1func ( self , force , ipyclient ) : sfiles = self . paramsdict [ sorted_fastq_path ] rfil "}
{"code_tokens": "def _step2func ( self , samples , force , ipyclient ) : if self . _headers : print ( \\n St "}
{"code_tokens": "def _step4func ( self , samples , force , ipyclient ) : if self . _headers : print ( \\n Step 4: Joint estimation of error rat "}
{"code_tokens": "def _step5func ( self , samples , force , ipyclient ) : if self . _headers : print ( \\n Step 5: Consensus b "}
{"code_tokens": "def _step6func ( self , samples , noreverse , force , randomseed , ipyclient , ** "}
{"code_tokens": "def _samples_precheck ( self , samples , mystep , force ) : subsample = [ ] for sample in samples : if sample . stats . "}
{"code_tokens": "def combinefiles ( filepath ) : fastqs = glob . glob ( filepath ) firsts = [ i for i in fastqs if "}
{"code_tokens": "def get_barcode_func ( data , longbar ) : if longbar [ 1 ] == 'same' : if data . "}
{"code_tokens": "def get_quart_iter ( tups ) : if tups [ 0 ] . endswith ( .gz ) : ofunc = gzip . open else : ofunc = open "}
{"code_tokens": "def writetofastq ( data , dsort , read ) : if read == 1 : rrr = R1 else : rrr = R2 for sname in dsort : handle "}
{"code_tokens": "def collate_files ( data , sname , tmp1s , tmp2s ) : out1 = os . path . join ( data . dirs . fastqs , {}_R1_.fa "}
{"code_tokens": "def estimate_optim ( data , testfile , ipyclient ) : insize = os . path . getsize ( testfile ) tmp_file_name = os . path . join "}
{"code_tokens": "def _cleanup_and_die ( data ) : tmpfiles = glob . glob ( os . path . join ( data . dirs . fastqs , tmp_*_R*.f "}
{"code_tokens": "def splitfiles ( data , raws , ipyclient ) : tmpdir = os . path . join ( data . paramsdict [ pr "}
{"code_tokens": "def putstats ( pfile , handle , statdicts ) : with open ( pfile , 'r' ) as infile : filestats , samplestats = pickle . load ( i "}
{"code_tokens": "def _countmatrix ( lxs ) : share = np . zeros ( ( lxs . shape [ 0 ] , lxs . shape [ 0 ] ) ) names = range ( lxs . shape [ 0 ] ) "}
{"code_tokens": "def paramname ( param = ) : try : name = pinfo [ str ( param ) ] [ 0 ] . strip ( ) . split (  ) [ 1 ] except ( KeyE "}
{"code_tokens": "def save_json2 ( data ) : datadict = OrderedDict ( [ ( outfiles , data . __dict__ [ outfiles "}
{"code_tokens": "def save_json ( data ) : datadict = OrderedDict ( [ ( _version , data . __dict__ [ _version ] "}
{"code_tokens": "def encode ( self , obj ) : def hint_tuples ( item ) : if isinstance ( item , tuple ) : re "}
{"code_tokens": "def depthplot ( data , samples = None , dims = ( None , None ) , canvas = ( None , None ) , xmax = 50 , log "}
{"code_tokens": "def _parse_00 ( ofile ) : with open ( ofile ) as infile : arr = np . array ( [  ] + infile . read ( ) . split ( "}
{"code_tokens": "def _parse_01 ( ofiles , individual = False ) : cols = [ ] dats = [ ] for ofile in ofiles : with open ( "}
{"code_tokens": "def _load_existing_results ( self , name , workdir ) : path = os . path . realpath ( os . path . join ( self . "}
{"code_tokens": "def summarize_results ( self , individual_results = False ) : if ( not self . params . infer_delimit ) & ( no "}
{"code_tokens": "def multi_muscle_align ( data , samples , ipyclient ) : LOGGER . info ( starting alignments ) lbview = ipyclient . load "}
{"code_tokens": "def concatclusts ( outhandle , alignbits ) : with gzip . open ( outhandle , 'wb' ) as out : "}
{"code_tokens": "def fill_dups_arr ( data ) : duplefiles = glob . glob ( os . path . join ( data . tmpdir , duples_*.tmp.npy ) ) duplefiles "}
{"code_tokens": "def build_tmp_h5 ( data , samples ) : snames = [ i . name for i in samples ] snames . sort ( ) uhandle = os . path . joi "}
{"code_tokens": "def get_nloci ( data ) : bseeds = os . path . join ( data . dirs . across , data . na "}
{"code_tokens": "def singlecat ( data , sample , bseeds , sidx , nloci ) : LOGGER . info ( in single cat here "}
{"code_tokens": "def write_to_fullarr ( data , sample , sidx ) : LOGGER . info ( writing fullarr %s "}
{"code_tokens": "def dask_chroms ( data , samples ) : h5s = [ os . path . join ( data . dirs . across , s . name + .tm "}
{"code_tokens": "def inserted_indels ( indels , ocatg ) : newcatg = np . zeros ( ocatg . shape , dtype = np . uint32 ) for "}
{"code_tokens": "def count_seeds ( usort ) : with open ( usort , 'r' ) as insort : cmd1 = [ cut , - "}
{"code_tokens": "def sort_seeds ( uhandle , usort ) : cmd = [ sort , -k , 2 , uhandle , -o , usort ] proc = sps . Popen ( cmd , cl "}
{"code_tokens": "def build_clustbits ( data , ipyclient , force ) : if os . path . exists ( data . tmpdir ) : shutil . rm "}
{"code_tokens": "def sub_build_clustbits ( data , usort , nseeds ) : LOGGER . info ( loading full _catcons file into memory ) allcons = { } "}
{"code_tokens": "def cleanup_tempfiles ( data ) : tmps1 = glob . glob ( os . path . join ( data . tmpdir , *.fa ) ) "}
{"code_tokens": "def assembly_cleanup ( data ) : data . stats_dfs . s2 = data . _build_stat ( s2 ) data . stats_files . s2 "}
{"code_tokens": "def parse_single_results ( data , sample , res1 ) : sample . stats_dfs . s2 [ trim_adapter_bp_read1 ] = 0 sample . stats_dfs . s "}
{"code_tokens": "def run2 ( data , samples , force , ipyclient ) : data . dirs . edits = os . path . join ( os . path . r "}
{"code_tokens": "def concat_reads ( data , subsamples , ipyclient ) : if any ( [ len ( i . files . fa "}
{"code_tokens": "def run_cutadapt ( data , subsamples , lbview ) : start = time . time ( ) printstr = "}
{"code_tokens": "def concat_multiple_inputs ( data , sample ) : if len ( sample . files . fastqs ) > 1 : cmd1 = "}
{"code_tokens": "def make ( data , samples ) : invcffile = os . path . join ( data . dirs . consens , data . name + . "}
{"code_tokens": "def importvcf ( vcffile , locifile ) : try : with open ( invcffile , 'r' ) as invcf : f "}
{"code_tokens": "def get_targets ( ipyclient ) : hosts = [ ] for eid in ipyclient . ids : engine = ipyclient [ eid ] if no "}
{"code_tokens": "def compute_tree_stats ( self , ipyclient ) : names = self . samples if self . params . nboots : ful "}
{"code_tokens": "def random_product ( iter1 , iter2 ) : pool1 = tuple ( iter1 ) pool2 = tuple ( iter2 ) ind1 = random . sample ( pool1 , "}
{"code_tokens": "def n_choose_k ( n , k ) : return int ( reduce ( MUL , ( Fraction ( n - i , i + 1 ) for i in range ( k ) ) , 1 ) ) "}
{"code_tokens": "def count_snps ( mat ) : snps = np . zeros ( 4 , dtype = np . uint32 ) snps [ 0 ] = np . uint32 ( mat [ 0 , "}
{"code_tokens": "def chunk_to_matrices ( narr , mapcol , nmask ) : mats = np . zeros ( ( 3 , 16 , 16 ) , dtype = np . "}
{"code_tokens": "def calculate ( seqnon , mapcol , nmask , tests ) : mats = chunk_to_matrices ( seqnon , m "}
{"code_tokens": "def nworker ( data , smpchunk , tests ) : with h5py . File ( data . database . input , 'r' ) "}
{"code_tokens": "def shuffle_cols ( seqarr , newarr , cols ) : for idx in xrange ( cols . shape [ 0 ] ) : newarr [ : , idx ] = seqarr "}
{"code_tokens": "def resolve_ambigs ( tmpseq ) : for ambig in np . uint8 ( [ 82 , 83 , 75 , 87 , 89 , 7 "}
{"code_tokens": "def get_spans ( maparr , spans ) : bidx = 1 spans = np . zeros ( ( maparr [ - 1 , 0 ] , 2 ) , np . "}
{"code_tokens": "def get_shape ( spans , loci ) : width = 0 for idx in xrange ( loci . shape [ 0 ] ) : width += spans [ loci [ idx ] "}
{"code_tokens": "def fill_boot ( seqarr , newboot , newmap , spans , loci ) : cidx = 0 for i in xrange ( loci . shape [ 0 ] ) : x "}
{"code_tokens": "def _byteify ( data , ignore_dicts = False ) : if isinstance ( data , unicode ) : return data . encode ( u "}
{"code_tokens": "def _parse_names ( self ) : self . samples = [ ] with iter ( open ( self . files . data , 'r' ) ) as infile : infile "}
{"code_tokens": "def _run_qmc ( self , boot ) : self . _tmp = os . path . join ( self . dirs , .tmpwtre ) cmd = [ ip . bins . qmc , qrtt= + sel "}
{"code_tokens": "def _dump_qmc ( self ) : io5 = h5py . File ( self . database . output , 'r' ) self . files . qdump = os . path . join ( self . d "}
{"code_tokens": "def _renamer ( self , tre ) : names = tre . get_leaves ( ) for name in names : name . name = sel "}
{"code_tokens": "def _finalize_stats ( self , ipyclient ) : print ( FINALTREES . format ( opr ( self . trees . tree ) ) ) if self "}
{"code_tokens": "def _save ( self ) : fulldict = copy . deepcopy ( self . __dict__ ) for i , j in fulldict . items ( ) : if isi "}
{"code_tokens": "def _insert_to_array ( self , start , results ) : qrts , wgts , qsts = results with h5py . File ( "}
{"code_tokens": "def select_samples ( dbsamples , samples , pidx = None ) : samples = [ i . name for i in samples ] if pi "}
{"code_tokens": "def padnames ( names ) : longname_len = max ( len ( i ) for i in names ) padding = 5 pnames = [ nam "}
{"code_tokens": "def locichunk ( args ) : data , optim , pnames , snppad , smask , start , samplecov , locuscov , upper "}
{"code_tokens": "def enter_pairs ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ) : "}
{"code_tokens": "def enter_singles ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , sta "}
{"code_tokens": "def init_arrays ( data ) : co5 = h5py . File ( data . clust_database , 'r' ) io5 = h5py . File ( data . database , 'w' ) ma "}
{"code_tokens": "def snpcount_numba ( superints , snpsarr ) : for iloc in xrange ( superints . shape [ 0 ] ) : for site in "}
{"code_tokens": "def maxind_numba ( block ) : inds = 0 for row in xrange ( block . shape [ 0 ] ) : wh "}
{"code_tokens": "def write_snps_map ( data ) : start = time . time ( ) tmparrs = os . path . join ( data . dirs . outfiles , tmp- "}
{"code_tokens": "def write_usnps ( data , sidx , pnames ) : tmparrs = os . path . join ( data . dirs . outfi "}
{"code_tokens": "def write_str ( data , sidx , pnames ) : start = time . time ( ) tmparrs = os . p "}
{"code_tokens": "def concat_vcf ( data , names , full ) : if not full : writer = open ( data . outfiles . vcf , 'w' ) else : wr "}
{"code_tokens": "def reftrick ( iseq , consdict ) : altrefs = np . zeros ( ( iseq . shape [ 1 ] , 4 ) , dtype = np . uint8 ) altrefs [ : "}
{"code_tokens": "def _collapse_outgroup ( tree , taxdicts ) : outg = taxdicts [ 0 ] [ p4 ] if not all ( [ i [ p4 ] == outg for i "}
{"code_tokens": "def draw ( self , show_tip_labels = True , show_node_support = False , use_edge_lengths = False , orie "}
{"code_tokens": "def get_quick_depths ( data , sample ) : sample . files . clusters = os . path . join ( data . "}
{"code_tokens": "def align_and_parse ( handle , max_internal_indels = 5 , is_gbs = False ) : try : with open ( handle "}
{"code_tokens": "def aligned_indel_filter ( clust , max_internal_indels ) : lclust = clust . split ( ) try : seq1 = [ i . split ( "}
{"code_tokens": "def setup_dirs ( data ) : pdir = os . path . realpath ( data . paramsdict [ project_dir ] ) data . dirs . clusts = os . path "}
{"code_tokens": "def build_dag ( data , samples ) : snames = [ i . name for i in samples ] dag = nx . DiG "}
{"code_tokens": "def _plot_dag ( dag , results , snames ) : try : import matplotlib . pyplot as plt fro "}
{"code_tokens": "def trackjobs ( func , results , spacer ) : LOGGER . info ( inside trackjobs of %s , func ) asyncs = [ ( i , results [ i ] ) "}
{"code_tokens": "def concat_multiple_edits ( data , sample ) : if len ( sample . files . edits ) > 1 : cmd1 = [ cat ] + [ i [ 0 ] for i "}
{"code_tokens": "def cluster ( data , sample , nthreads , force ) : if reference in data . paramsdict [ assembly_metho "}
{"code_tokens": "def muscle_chunker ( data , sample ) : LOGGER . info ( inside muscle_chunker ) if data . paramsdi "}
{"code_tokens": "def derep_concat_split ( data , sample , nthreads , force ) : LOGGER . info ( INSIDE derep %s , sample . name ) merg "}
{"code_tokens": "def branch_assembly ( args , parsedict ) : data = getassembly ( args , parsedict ) ba "}
{"code_tokens": "def getassembly ( args , parsedict ) : project_dir = ip . core . assembly . _expander ( parsedict [ 'project "}
{"code_tokens": "def get_binom ( base1 , base2 , estE , estH ) : prior_homo = ( 1. - estH ) / 2. prior_hete = estH bsum = base1 + base2 hetprob "}
{"code_tokens": "def basecaller ( arrayed , mindepth_majrule , mindepth_statistical , estH , estE ) : "}
{"code_tokens": "def nfilter1 ( data , reps ) : if sum ( reps ) >= data . paramsdict [ mindepth_majrule ] and sum ( reps ) <= data . paramsdict "}
{"code_tokens": "def storealleles ( consens , hidx , alleles ) : bigbase = PRIORITY [ consens [ hidx [ 0 ] ] ] bigall "}
{"code_tokens": "def chunk_clusters ( data , sample ) : num = 0 optim = int ( ( sample . stats . clusters_total // data . cpus ) + ( sample . sta "}
{"code_tokens": "def run ( data , samples , force , ipyclient ) : data . dirs . consens = os . path . join ( data . dirs . project , d "}
{"code_tokens": "def calculate_depths ( data , samples , lbview ) : start = time . time ( ) printstr = c "}
{"code_tokens": "def make_chunks ( data , samples , lbview ) : start = time . time ( ) printstr = chu "}
{"code_tokens": "def make ( data , samples ) : outfile = open ( os . path . join ( data . dirs . outfiles , data . name + "}
{"code_tokens": "def cluster_info ( ipyclient , spacer = ) : hosts = [ ] for eid in ipyclient . ids : engine = ipyclient [ eid ] if "}
{"code_tokens": "def _set_debug_dict ( __loglevel__ ) : _lconfig . dictConfig ( { 'version' : 1 , 'disab "}
{"code_tokens": "def _debug_off ( ) : if _os . path . exists ( __debugflag__ ) : _os . remove ( __debugflag__ ) "}
{"code_tokens": "def _cmd_exists ( cmd ) : return _subprocess . call ( type + cmd , shell = True , stdout = _subprocess . PIPE , "}
{"code_tokens": "def _getbins ( ) : if not _sys . maxsize > 2 ** 32 : _sys . exit ( ipyrad requires 64bit architecture ) _platfo "}
{"code_tokens": "def nworker ( data , chunk ) : oldlimit = set_mkl_thread_limit ( 1 ) with h5py . File ( data "}
{"code_tokens": "def store_all ( self ) : with h5py . File ( self . database . input , 'a' ) as io "}
{"code_tokens": "def store_random ( self ) : with h5py . File ( self . database . input , 'a' ) as "}
{"code_tokens": "def random_combination ( nsets , n , k ) : sets = set ( ) while len ( sets ) < nsets : newset = tuple ( so "}
{"code_tokens": "def random_product ( iter1 , iter2 ) : iter4 = np . concatenate ( [ np . random . choice ( it "}
{"code_tokens": "def resolve_ambigs ( tmpseq ) : for aidx in xrange ( 6 ) : ambig , res1 , res2 = GETCONS [ "}
{"code_tokens": "def set_mkl_thread_limit ( cores ) : if linux in sys . platform : mkl_rt = cty "}
{"code_tokens": "def get_total ( tots , node ) : if ( node . is_leaf ( ) or node . is_root ( ) ) : return 0 else "}
{"code_tokens": "def get_sampled ( data , totn , node ) : names = sorted ( totn ) cdict = { name : idx for idx , name "}
{"code_tokens": "def _run_qmc ( self , boot ) : self . _tmp = os . path . join ( self . dirs , .tmptre ) "}
{"code_tokens": "def _insert_to_array ( self , chunk , results ) : chunksize = self . _chunksize qrts "}
{"code_tokens": "def get_client ( cluster_id , profile , engines , timeout , cores , quiet , spacer , "}
{"code_tokens": "def memoize ( func ) : class Memodict ( dict ) : def __getitem__ ( self , * key ) : return dict . __getitem__ ( self "}
{"code_tokens": "def ambigcutters ( seq ) : resos = [ ] if any ( [ i in list ( RKSYWM ) for i in seq ] ) : for "}
{"code_tokens": "def splitalleles ( consensus ) : allele1 = list ( consensus ) allele2 = list ( consensus ) hidx = [ i for ( i , j ) in enume "}
{"code_tokens": "def comp ( seq ) : return seq . replace ( A , 't' ) . replace ( 'T' , 'a' ) . replace ( "}
{"code_tokens": "def fullcomp ( seq ) : seq = seq . replace ( A , 'u' ) . replace ( 'T' , 'v' ) . replace ( 'C' , 'p' ) . "}
{"code_tokens": "def fastq_touchup_for_vsearch_merge ( read , outfile , reverse = False ) : counts = 0 with "}
{"code_tokens": "def revcomp ( sequence ) : returns reverse complement of a string sequence = se "}
{"code_tokens": "def clustdealer ( pairdealer , optim ) : ccnt = 0 chunk = [ ] while ccnt < optim : try : taker = itertool "}
{"code_tokens": "def progressbar ( njobs , finished , msg = , spacer =  ) : if njobs : progress = 100 * ( finished / f "}
{"code_tokens": "def get_threaded_view ( ipyclient , split = True ) : eids = ipyclient . ids dview = ipyclient . direc "}
{"code_tokens": "def detect_cpus ( ) : if hasattr ( os , sysconf ) : if os . sysconf_names . has_key ( SC_NPROCESSORS_ONLN "}
{"code_tokens": "def _call_structure ( mname , ename , sname , name , workdir , seed , ntaxa , nsites , kpop , rep ) : outname = os . path . join "}
{"code_tokens": "def _get_clumpp_table ( self , kpop , max_var_multiple , quiet ) : reps , excluded = _concat_reps ( self , kpop , max_v "}
{"code_tokens": "def _get_evanno_table ( self , kpops , max_var_multiple , quiet ) : kpops = sorted ( kpops ) replnliks = [ ] for kpop in "}
{"code_tokens": "def result_files ( self ) : reps = OPJ ( self . workdir , self . name + -K-*-rep-*_f ) repfiles = glob . glob ( "}
{"code_tokens": "def get_evanno_table ( self , kvalues , max_var_multiple = 0 , quiet = False ) : if max_var_multiple : if max_var_m "}
{"code_tokens": "def parse ( self , psearch , dsearch ) : stable = with open ( self . repfile ) as orep : "}
{"code_tokens": "def _call_raxml ( command_list ) : proc = subprocess . Popen ( command_list , stderr = subprocess . S "}
{"code_tokens": "def run ( self , ipyclient = None , quiet = False , force = False , block = False , ) : if force : for "}
{"code_tokens": "def _get_binary ( self ) : backup_binaries = [ raxmlHPC-PTHREADS , raxmlHPC-PTHREADS-SSE3 ] for binary in [ self . params . "}
{"code_tokens": "def dstat ( inarr , taxdict , mindict = 1 , nboots = 1000 , name = 0 ) : if isinstance ( inarr , list ) : arr , _ = _l "}
{"code_tokens": "def _get_boots ( arr , nboots ) : boots = np . zeros ( ( nboots , ) ) for bidx in xrange ( nboots "}
{"code_tokens": "def taxon_table ( self ) : if self . tests : keys = sorted ( self . tests [ 0 ] . keys ( ) ) if isinstance ( self . t "}
{"code_tokens": "def nexmake ( mdict , nlocus , dirs , mcmc_burnin , mcmc_ngen , mcmc_sample_freq ) : m "}
{"code_tokens": "def call_fastq_dump_on_SRRs ( self , srr , outname , paired ) : fd_cmd = [ fastq-dump , srr , --acc "}
{"code_tokens": "def fields_checker ( fields ) : if isinstance ( fields , int ) : fields = str ( fields ) if isinstance ( fields "}
{"code_tokens": "def run ( self , force = False , ipyclient = None , name_fields = 30 , name_separator = _ , dry_run = False ) : try : if "}
{"code_tokens": "def Async ( cls , token , session = None , ** options ) : return cls ( token , session = session , is_async = True "}
{"code_tokens": "def get_constants ( self , ** params : keys ) : url = self . api . CONSTANTS return self . _get_model ( url , ** params ) "}
{"code_tokens": "def get_player ( self , * tags : crtag , ** params : keys ) : url = self . api . PLAYER "}
{"code_tokens": "def get_clan ( self , * tags : crtag , ** params : keys ) : url = self . api . CLAN + '/' + ',' . join "}
{"code_tokens": "def search_tournaments ( self , ** params : keys ) : url = self . api . TOURNAMENT + '/ "}
{"code_tokens": "def get_top_war_clans ( self , country_key = '' , ** params : keys ) : url = self . api . TOP + '/war/' + s "}
{"code_tokens": "def get_popular_clans ( self , ** params : keys ) : url = self . api . POPULAR + '/clans' return self . _get "}
{"code_tokens": "def get_popular_players ( self , ** params : keys ) : url = self . api . POPULAR + '/players' return s "}
{"code_tokens": "def get_popular_tournaments ( self , ** params : keys ) : url = self . api . POPULA "}
{"code_tokens": "def get_popular_decks ( self , ** params : keys ) : url = self . api . POPULAR + '/decks' return self . _get_model ( url , ** "}
{"code_tokens": "def get_known_tournaments ( self , ** params : tournamentfilter ) : url = self . api . TOURNAMENT + '/known' re "}
{"code_tokens": "def get_player ( self , tag : crtag , timeout = None ) : url = self . api . PLAYER + '/' + tag return self . _ge "}
{"code_tokens": "def get_player_chests ( self , tag : crtag , timeout : int = None ) : url = self . api . PLAYER + '/' + tag + '/upcomi "}
{"code_tokens": "def get_clan ( self , tag : crtag , timeout : int = None ) : url = self . api . CLAN + '/' + tag return self . _get_model ( url , "}
{"code_tokens": "def search_clans ( self , ** params : clansearch ) : url = self . api . CLAN return self . _get_model ( url , PartialClan , ** "}
{"code_tokens": "def search_tournaments ( self , name : str , ** params : keys ) : url = self . api . TOU "}
{"code_tokens": "def get_all_cards ( self , timeout : int = None ) : url = self . api . CARDS return self . _get_model ( url , timeo "}
{"code_tokens": "def get_all_locations ( self , timeout : int = None ) : url = self . api . LOCATIONS ret "}
{"code_tokens": "def get_location ( self , location_id : int , timeout : int = None ) : url = self . api . LOCATIONS + '/' + "}
{"code_tokens": "def get_top_clans ( self , location_id = 'global' , ** params : keys ) : url = self . api . LOCATIONS "}
{"code_tokens": "def get_clan_image ( self , obj : BaseAttrDict ) : try : badge_id = obj . clan . badge_id except AttributeErr "}
{"code_tokens": "def get_arena_image ( self , obj : BaseAttrDict ) : badge_id = obj . arena . id for i in self . co "}
{"code_tokens": "def get_deck_link ( self , deck : BaseAttrDict ) : deck_link = 'https://link.clashroyale.com/deck/en?deck=' for i in deck : "}
{"code_tokens": "def get_datetime ( self , timestamp : str , unix = True ) : time = datetime . strptime ( timestamp , '%Y%m%dT%H%M%S.%fZ "}
{"code_tokens": "def typecasted ( func ) : signature = inspect . signature ( func ) . parameters . item "}
{"code_tokens": "def coerce_annotation ( ann , namespace ) : ann = convert ( ann , namespace ) ann . validate ( strict = True ) return an "}
{"code_tokens": "def beat ( ref , est , ** kwargs ) : r namespace = 'beat' ref = coerce_annotation ( ref , namespace ) est = coerce_a "}
{"code_tokens": "def chord ( ref , est , ** kwargs ) : r namespace = 'chord' ref = coerce_annotation ( ref , namespace ) es "}
{"code_tokens": "def hierarchy_flatten ( annotation ) : intervals , values = annotation . to_interval_values ( ) ordering = dict ( ) for "}
{"code_tokens": "def hierarchy ( ref , est , ** kwargs ) : r namespace = 'multi_segment' ref = coerce_annotation ( ref , namespa "}
{"code_tokens": "def tempo ( ref , est , ** kwargs ) : r ref = coerce_annotation ( ref , 'tempo' ) est = coerce_annotation ( est , 'tempo' "}
{"code_tokens": "def melody ( ref , est , ** kwargs ) : r namespace = 'pitch_contour' ref = coerce_annotation ( ref , "}
{"code_tokens": "def pattern_to_mireval ( ann ) : patterns = defaultdict ( lambda : defaultdict ( list ) ) for time , ob "}
{"code_tokens": "def pattern ( ref , est , ** kwargs ) : r namespace = 'pattern_jku' ref = coerce_annotation ( ref , n "}
{"code_tokens": "def transcription ( ref , est , ** kwargs ) : r namespace = 'pitch_contour' ref = coerce_annotation ( "}
{"code_tokens": "def add_namespace ( filename ) : with open ( filename , mode = 'r' ) as fileobj : __NAMESPACE__ . update ( json . loa "}
{"code_tokens": "def namespace ( ns_key ) : if ns_key not in __NAMESPACE__ : raise NamespaceError ( 'Unknown namespace: {:s}' . format ( ns_k "}
{"code_tokens": "def namespace_array ( ns_key ) : obs_sch = namespace ( ns_key ) obs_sch [ 'title' ] = 'Observation' sch = copy . deepcopy "}
{"code_tokens": "def values ( ns_key ) : if ns_key not in __NAMESPACE__ : raise NamespaceError ( 'Unknown namespace: "}
{"code_tokens": "def get_dtypes ( ns_key ) : if ns_key not in __NAMESPACE__ : raise NamespaceError ( 'Unkn "}
{"code_tokens": "def list_namespaces ( ) : print ( '{:30s}\\t{:40s}' . format ( 'NAME' , 'DESCRIPTION' ) ) pri "}
{"code_tokens": "def __get_dtype ( typespec ) : if 'type' in typespec : return __TYPE_MAP__ . get ( "}
{"code_tokens": "def __load_jams_schema ( ) : schema_file = os . path . join ( SCHEMA_DIR , 'jams_schema.json' ) jams_schema = None with open ( r "}
{"code_tokens": "def import_lab ( namespace , filename , infer_duration = True , ** parse_options "}
{"code_tokens": "def expand_filepaths ( base_dir , rel_paths ) : return [ os . path . join ( base_dir , os . "}
{"code_tokens": "def smkdirs ( dpath , mode = 0o777 ) : if not os . path . exists ( dpath ) : os . makedirs ( dpath , mode = mode ) "}
{"code_tokens": "def find_with_extension ( in_dir , ext , depth = 3 , sort = True ) : assert depth >= 1 ext = ext . stri "}
{"code_tokens": "def get_comments ( jam , ann ) : jam_comments = jam . file_metadata . __json__ ann_comments = an "}
{"code_tokens": "def convert_jams ( jams_file , output_prefix , csv = False , comment_char = '#' , namespaces = Non "}
{"code_tokens": "def parse_arguments ( args ) : parser = argparse . ArgumentParser ( description = 'Co "}
{"code_tokens": "def _conversion ( target , source ) : def register ( func ) : __CONVERSION__ [ target ] [ source ] = func return func ret "}
{"code_tokens": "def convert ( annotation , target_namespace ) : annotation . validate ( strict = True ) if annotation . namespace == target_n "}
{"code_tokens": "def can_convert ( annotation , target_namespace ) : if annotation . namespace == target_namespace : return True if ta "}
{"code_tokens": "def pitch_hz_to_contour ( annotation ) : annotation . namespace = 'pitch_contour' data = annotation . pop_da "}
{"code_tokens": "def note_hz_to_midi ( annotation ) : annotation . namespace = 'note_midi' data = "}
{"code_tokens": "def scaper_to_tag ( annotation ) : annotation . namespace = 'tag_open' data = annotation . pop_data ( ) for obs in d "}
{"code_tokens": "def deprecated ( version , version_removed ) : def __wrapper ( func , * args , ** kwargs ) : code = six . get_function_code ( fu "}
{"code_tokens": "def _open ( name_or_fdesc , mode = 'r' , fmt = 'auto' ) : open_map = { 'jams' : open , 'json' : "}
{"code_tokens": "def load ( path_or_file , validate = True , strict = True , fmt = 'auto' ) : r with _open ( path_or_file , mode = 'r' , fmt "}
{"code_tokens": "def query_pop ( query , prefix , sep = '.' ) : terms = query . split ( sep ) if t "}
{"code_tokens": "def match_query ( string , query ) : if six . callable ( query ) : return query ( string ) "}
{"code_tokens": "def serialize_obj ( obj ) : if isinstance ( obj , np . integer ) : return int ( obj ) elif isinstance ( obj "}
{"code_tokens": "def summary ( obj , indent = 0 ) : if hasattr ( obj , '__summary__' ) : rep = obj . __summary__ ( ) elif isinst "}
{"code_tokens": "def update ( self , ** kwargs ) : for name , value in six . iteritems ( kwargs ) : setattr ( self , name , value "}
{"code_tokens": "def validate ( self , strict = True ) : valid = True try : jsonschema . validate ( self . __json__ , self . __schema__ ) ex "}
{"code_tokens": "def append ( self , time = None , duration = None , value = None , confidence = None ) : self . data . add "}
{"code_tokens": "def append_records ( self , records ) : for obs in records : if isinstance ( obs , Observation ) : self . append ( * "}
{"code_tokens": "def append_columns ( self , columns ) : self . append_records ( [ dict ( time = t , durat "}
{"code_tokens": "def validate ( self , strict = True ) : ann_schema = schema . namespace_array ( self . namespace ) valid = "}
{"code_tokens": "def trim ( self , start_time , end_time , strict = False ) : if end_time <= start_time : r "}
{"code_tokens": "def slice ( self , start_time , end_time , strict = False ) : sliced_ann = self . trim ( start_time , "}
{"code_tokens": "def pop_data ( self ) : data = self . data self . data = SortedKeyList ( key = self . "}
{"code_tokens": "def to_samples ( self , times , confidence = False ) : times = np . asarray ( times ) if times . n "}
{"code_tokens": "def to_html ( self , max_rows = None ) : n = len ( self . data ) div_id = _get_divi "}
{"code_tokens": "def _key ( cls , obs ) : if not isinstance ( obs , Observation ) : raise JamsError ( ' "}
{"code_tokens": "def search ( self , ** kwargs ) : results = AnnotationArray ( ) for annotation in self : if anno "}
{"code_tokens": "def trim ( self , start_time , end_time , strict = False ) : trimmed_array = Ann "}
{"code_tokens": "def slice ( self , start_time , end_time , strict = False ) : sliced_array = AnnotationArray ( ) for ann in self : sliced_a "}
{"code_tokens": "def add ( self , jam , on_conflict = 'fail' ) : if on_conflict not in [ 'overwrite' , 'fail' , 'ignore' ] : raise Parame "}
{"code_tokens": "def save ( self , path_or_file , strict = True , fmt = 'auto' ) : self . validate ( "}
{"code_tokens": "def validate ( self , strict = True ) : valid = True try : jsonschema . validate ( self . __json_light__ , schema . JAMS_SCHE "}
{"code_tokens": "def trim ( self , start_time , end_time , strict = False ) : if self . file_metadata . duration "}
{"code_tokens": "def slice ( self , start_time , end_time , strict = False ) : if self . file_metadata . d "}
{"code_tokens": "def pprint_jobject ( obj , ** kwargs ) : obj_simple = { k : v for k , v in six . iteri "}
{"code_tokens": "def intervals ( annotation , ** kwargs ) : times , labels = annotation . to_interval_values ( ) return mir_eval . display . label "}
{"code_tokens": "def hierarchy ( annotation , ** kwargs ) : htimes , hlabels = hierarchy_flatten ( "}
{"code_tokens": "def pitch_contour ( annotation , ** kwargs ) : ax = kwargs . pop ( 'ax' , None ) ax = mir_eval . display . __get_axes ( ax "}
{"code_tokens": "def event ( annotation , ** kwargs ) : times , values = annotation . to_interval_values ( ) if any ( "}
{"code_tokens": "def beat_position ( annotation , ** kwargs ) : times , values = annotation . to_interv "}
{"code_tokens": "def piano_roll ( annotation , ** kwargs ) : times , midi = annotation . to_interval_values ( ) re "}
{"code_tokens": "def display ( annotation , meta = True , ** kwargs ) : for namespace , func in six . iteritems ( VIZ_MA "}
{"code_tokens": "def display_multi ( annotations , fig_kw = None , meta = True , ** kwargs ) : if fig_kw is None : fig_kw = dict ( ) fig_kw . set "}
{"code_tokens": "def mkclick ( freq , sr = 22050 , duration = 0.1 ) : times = np . arange ( int ( sr * duration ) ) click = np . sin ( 2 "}
{"code_tokens": "def clicks ( annotation , sr = 22050 , length = None , ** kwargs ) : interval , _ = annotation . to_interval "}
{"code_tokens": "def downbeat ( annotation , sr = 22050 , length = None , ** kwargs ) : beat_click = mkclick "}
{"code_tokens": "def multi_segment ( annotation , sr = 22050 , length = None , ** kwargs ) : PENT "}
{"code_tokens": "def pitch_contour ( annotation , sr = 22050 , length = None , ** kwargs ) : times = defaultdict ( list ) freqs = defaultdict ( "}
{"code_tokens": "def piano_roll ( annotation , sr = 22050 , length = None , ** kwargs ) : intervals , pitches = annotation . to_interval_values "}
{"code_tokens": "def sonify ( annotation , sr = 22050 , duration = None , ** kwargs ) : length = None if duration is None : durat "}
{"code_tokens": "def validate ( schema_file = None , jams_files = None ) : schema = load_json ( schema_file ) for jams_file in jams_file "}
{"code_tokens": "def _handle_auth_success ( self , stream , success ) : if not self . _check_authorization ( success . properties , stream ) : elem "}
{"code_tokens": "def _check_authorization ( self , properties , stream ) : authzid = properties . get ( "}
{"code_tokens": "def _sasl_authenticate ( self , stream , username , authzid ) : if not stream . initiator : raise SASLAuthenticat "}
{"code_tokens": "def timeout_handler ( interval , recurring = None ) : def decorator ( func ) : func . _pyxmpp_timeout = interval fu "}
{"code_tokens": "def delayed_call ( self , delay , function ) : main_loop = self handler = [ ] class DelayedCallHandler "}
{"code_tokens": "def from_xml ( cls , element ) : if element . tag != ITEM_TAG : raise ValueError ( {0!r} is not a roster i "}
{"code_tokens": "def as_xml ( self , parent = None ) : if parent is not None : element = ElementTree . SubElement ( parent , ITEM_TAG "}
{"code_tokens": "def verify_roster_push ( self , fix = False ) : self . _verify ( ( None , ufrom , uto , u "}
{"code_tokens": "def verify_roster_set ( self , fix = False , settings = None ) : try : self . _verify ( ( None , uremove ) , fix ) "}
{"code_tokens": "def groups ( self ) : groups = set ( ) for item in self . _items : groups |= item . groups return groups "}
{"code_tokens": "def get_items_by_name ( self , name , case_sensitive = True ) : if not case_sensitive and name : name = name . lower ( ) result "}
{"code_tokens": "def get_items_by_group ( self , group , case_sensitive = True ) : result = [ ] if not group : "}
{"code_tokens": "def add_item ( self , item , replace = False ) : if item . jid in self . _jids : if r "}
{"code_tokens": "def remove_item ( self , jid ) : if jid not in self . _jids : raise KeyError ( jid "}
{"code_tokens": "def load_roster ( self , source ) : try : tree = ElementTree . parse ( source ) except Ele "}
{"code_tokens": "def save_roster ( self , dest , pretty = True ) : if self . roster is None : raise ValueError ( No roster ) ele "}
{"code_tokens": "def handle_got_features_event ( self , event ) : server_features = set ( ) logger . debug ( Checking roster-related "}
{"code_tokens": "def handle_authorized_event ( self , event ) : self . server = event . authorized_jid . bare ( ) if versionin "}
{"code_tokens": "def request_roster ( self , version = None ) : processor = self . stanza_processor request = Iq ( stanza "}
{"code_tokens": "def _get_success ( self , stanza ) : payload = stanza . get_payload ( RosterPayload ) if payl "}
{"code_tokens": "def _get_error ( self , stanza ) : if stanza : logger . debug ( uRoster request failed: {0} . fo "}
{"code_tokens": "def handle_roster_push ( self , stanza ) : if self . server is None and stanza . f "}
{"code_tokens": "def add_item ( self , jid , name = None , groups = None , callback = None , erro "}
{"code_tokens": "def update_item ( self , jid , name = NO_CHANGE , groups = NO_CHANGE , callback = None , error_callback = None ) : item "}
{"code_tokens": "def remove_item ( self , jid , callback = None , error_callback = None ) : item = self . roste "}
{"code_tokens": "def _roster_set ( self , item , callback , error_callback ) : stanza = Iq ( to_jid = se "}
{"code_tokens": "def free ( self ) : if not self . borrowed : self . xmlnode . unlinkNode ( ) sel "}
{"code_tokens": "def xpath_eval ( self , expr ) : ctxt = common_doc . xpathNewContext ( ) ctxt . setContextNode ( self . xmlnode ) "}
{"code_tokens": "def set_history ( self , parameters ) : for child in xml_element_iter ( self . xmlnode . "}
{"code_tokens": "def get_history ( self ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_ur "}
{"code_tokens": "def set_password ( self , password ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( chil "}
{"code_tokens": "def get_password ( self ) : for child in xml_element_iter ( self . xmlnode . children ) : if g "}
{"code_tokens": "def __init ( self , affiliation , role , jid = None , nick = None , actor = None , reason = None ) : if no "}
{"code_tokens": "def __from_xmlnode ( self , xmlnode ) : actor = None reason = None n = xmlnode . children while n : ns = n . ns ( ) if ns a "}
{"code_tokens": "def __init ( self , code ) : code = int ( code ) if code < 0 or code > 999 : raise ValueError ( Bad status c "}
{"code_tokens": "def get_items ( self ) : if not self . xmlnode . children : return [ ] ret = [ ] n = self . xmlnod "}
{"code_tokens": "def add_item ( self , item ) : if not isinstance ( item , MucItemBase ) : raise TypeError ( Bad item "}
{"code_tokens": "def get_muc_child ( self ) : if self . muc_child : return self . muc_child if not self . xmlnode . children : return None n = "}
{"code_tokens": "def clear_muc_child ( self ) : if self . muc_child : self . muc_child . free_borrowed ( ) self . muc_child = None "}
{"code_tokens": "def make_join_request ( self , password = None , history_maxchars = None , history_maxstanzas = None "}
{"code_tokens": "def get_join_info ( self ) : x = self . get_muc_child ( ) if not x : return None if not isinstance ( x , MucX ) : return None retu "}
{"code_tokens": "def make_kick_request ( self , nick , reason ) : self . clear_muc_child ( ) self . muc_child = MucAdminQuery ( pare "}
{"code_tokens": "def nfkc ( data ) : if isinstance ( data , list ) : data = u . join ( data ) return unicodedata . normalize "}
{"code_tokens": "def set_stringprep_cache_size ( size ) : global _stringprep_cache_size _stringprep_cache_size = size if len "}
{"code_tokens": "def map ( self , data ) : result = [ ] for char in data : ret = None for lookup in self "}
{"code_tokens": "def prohibit ( self , data ) : for char in data : for lookup in self . prohibite "}
{"code_tokens": "def check_unassigned ( self , data ) : for char in data : for lookup in self . unassigned : if loo "}
{"code_tokens": "def check_bidi ( data ) : has_l = False has_ral = False for char in data : if stringprep . in_table_d1 ( char ) : "}
{"code_tokens": "def hold_exception ( method ) : @ functools . wraps ( method ) def wrapper ( self , * args , ** "}
{"code_tokens": "def _configure_io_handler ( self , handler ) : if self . check_events ( ) : return if handler in self . _un "}
{"code_tokens": "def _prepare_pending ( self ) : if not self . _unprepared_pending : return for handler in list ( se "}
{"code_tokens": "def _prepare_io_handler_cb ( self , handler ) : self . _anything_done = True logger . debug ( _prepar_io_handler_cb call "}
{"code_tokens": "def _timeout_cb ( self , method ) : self . _anything_done = True logger . debug ( _timeout_cb() called for "}
{"code_tokens": "def _loop_timeout_cb ( self , main_loop ) : self . _anything_done = True logger . debug ( _loop_timeout_cb( "}
{"code_tokens": "def stanza_factory ( element , return_path = None , language = None ) : tag = element . tag if t "}
{"code_tokens": "def _process_handler_result ( self , response ) : if response is None or response is False : return F "}
{"code_tokens": "def _process_iq_response ( self , stanza ) : stanza_id = stanza . stanza_id from_jid = stanza . from_jid if from_jid "}
{"code_tokens": "def process_iq ( self , stanza ) : typ = stanza . stanza_type if typ in ( result , error ) : return self . "}
{"code_tokens": "def __try_handlers ( self , handler_list , stanza , stanza_type = None ) : if stanza_type is None : stanza_type = stanza "}
{"code_tokens": "def process_message ( self , stanza ) : stanza_type = stanza . stanza_type if stanza_ty "}
{"code_tokens": "def process_presence ( self , stanza ) : stanza_type = stanza . stanza_type return self . __try_handlers ( self . _presence_h "}
{"code_tokens": "def route_stanza ( self , stanza ) : if stanza . stanza_type not in ( error , result ) : response = stanza . make_error_r "}
{"code_tokens": "def process_stanza ( self , stanza ) : self . fix_in_stanza ( stanza ) to_jid = stanza . "}
{"code_tokens": "def set_response_handlers ( self , stanza , res_handler , err_handler , timeout_handler = None , timeo "}
{"code_tokens": "def _set_response_handlers ( self , stanza , res_handler , err_handler , timeout_hand "}
{"code_tokens": "def setup_stanza_handlers ( self , handler_objects , usage_restriction ) : iq_handlers = { get : { } , set : { } "}
{"code_tokens": "def send ( self , stanza ) : if self . uplink : self . uplink . send ( stanza ) else : "}
{"code_tokens": "def check_events ( self ) : if self . event_dispatcher . flush ( ) is QUIT : self . _quit "}
{"code_tokens": "def _call_timeout_handlers ( self ) : sources_handled = 0 now = time . time ( ) schedule = None while self . _timeout_handlers : "}
{"code_tokens": "def xml_elements_equal ( element1 , element2 , ignore_level1_cdata = False ) : if Non "}
{"code_tokens": "def make_error_response ( self , cond ) : if self . stanza_type == error : raise ValueError ( Errors may not be ge "}
{"code_tokens": "def _move_session_handler ( handlers ) : index = 0 for i , handler in enumerate ( "}
{"code_tokens": "def connect ( self ) : with self . lock : if self . stream : logger . debug ( Closing the previously used "}
{"code_tokens": "def disconnect ( self ) : with self . lock : if self . stream : if self . settings [ uinitial_ "}
{"code_tokens": "def _close_stream ( self ) : self . stream . close ( ) if self . stream . transport in self . _ml_handlers : self . _ml_ha "}
{"code_tokens": "def _stream_authenticated ( self , event ) : with self . lock : if event . stream != self . stream : return s "}
{"code_tokens": "def _stream_authorized ( self , event ) : with self . lock : if event . stream != s "}
{"code_tokens": "def _stream_disconnected ( self , event ) : with self . lock : if event . stream != self . stream : r "}
{"code_tokens": "def base_handlers_factory ( self ) : tls_handler = StreamTLSHandler ( self . settings ) sasl_handler = StreamSASLHandler ( self . "}
{"code_tokens": "def payload_class_for_element_name ( element_name ) : logger . debug ( looking up payload class "}
{"code_tokens": "def _unquote ( data ) : if not data . startswith ( b'' ) or not data . endswith ( b'' ) : return dat "}
{"code_tokens": "def _quote ( data ) : data = data . replace ( b'' , b'' ) data = data . replace ( b'' , b'' ) return data "}
{"code_tokens": "def _compute_response ( urp_hash , nonce , cnonce , nonce_count , authzid , digest_uri ) : logger . de "}
{"code_tokens": "def rfc2425encode ( name , value , parameters = None , charset = utf-8 ) : if not parameters : parameter "}
{"code_tokens": "def __from_xml ( self , value ) : n = value . children vns = get_node_ns ( value ) while n : if n . type != 'eleme "}
{"code_tokens": "def __make_fn ( self ) : s = [ ] if self . n . prefix : s . append ( self . n . prefix ) if self . n . given : s . append ( self . "}
{"code_tokens": "def __from_xml ( self , data ) : ns = get_node_ns ( data ) if ns and ns . getContent ( ) ! "}
{"code_tokens": "def __from_rfc2426 ( self , data ) : data = from_utf8 ( data ) lines = data . split ( \\n ) started = 0 current "}
{"code_tokens": "def _process_rfc2425_record ( self , data ) : label , value = data . split ( : , 1 ) "}
{"code_tokens": "def rfc2426 ( self ) : ret = begin:VCARD\\r\\n ret += version:3.0\\r\\n for _unused , value "}
{"code_tokens": "def update_state ( self ) : self . _lock . acquire ( ) try : now = datetime . utcnow ( ) "}
{"code_tokens": "def _deactivate ( self ) : self . cache . remove_fetcher ( self ) if self . activ "}
{"code_tokens": "def got_it ( self , value , state = new ) : if not self . active : return item = CacheItem ( self . address , value , se "}
{"code_tokens": "def error ( self , error_data ) : if not self . active : return if not self . _try_backup_item ( ) : self . _ "}
{"code_tokens": "def timeout ( self ) : if not self . active : return if not self . _try_backup_item ( ) : if self . _timeout_handler : self "}
{"code_tokens": "def _try_backup_item ( self ) : if not self . _backup_state : return False item = self . cache . get_item ( se "}
{"code_tokens": "def add_item ( self , item ) : self . _lock . acquire ( ) try : state = item . update_state "}
{"code_tokens": "def get_item ( self , address , state = 'fresh' ) : self . _lock . acquire ( ) try : item "}
{"code_tokens": "def update_item ( self , item ) : self . _lock . acquire ( ) try : state = item . update_state ( ) se "}
{"code_tokens": "def purge_items ( self ) : self . _lock . acquire ( ) try : il = self . _items_list num_items = len ( il ) need "}
{"code_tokens": "def remove_fetcher ( self , fetcher ) : self . _lock . acquire ( ) try : for t , f "}
{"code_tokens": "def set_fetcher ( self , fetcher_class ) : self . _lock . acquire ( ) try : self . "}
{"code_tokens": "def register_fetcher ( self , object_class , fetcher_class ) : self . _lock . acquire ( ) try "}
{"code_tokens": "def unregister_fetcher ( self , object_class ) : self . _lock . acquire ( ) try : cache = sel "}
{"code_tokens": "def _register_client_authenticator ( klass , name ) : CLIENT_MECHANISMS_D [ name "}
{"code_tokens": "def _register_server_authenticator ( klass , name ) : SERVER_MECHANISMS_D [ name ] = kla "}
{"code_tokens": "def sasl_mechanism ( name , secure , preference = 50 ) : def decorator ( klass ) : klass . _pyxmpp_sasl_secure "}
{"code_tokens": "def check_password ( self , username , password , properties ) : logger . debug ( check_password{0!r} . f "}
{"code_tokens": "def encode ( self ) : if self . data is None : return elif not self . data : return = else : re "}
{"code_tokens": "def handle_authorized ( self , event ) : stream = event . stream if not stream : return if "}
{"code_tokens": "def _decode_asn1_string ( data ) : if isinstance ( data , BMPString ) : return bytes ( data ) . decode ( "}
{"code_tokens": "def display_name ( self ) : if self . subject_name : return u, . join ( [ u, "}
{"code_tokens": "def verify_server ( self , server_name , srv_type = 'xmpp-client' ) : server_jid = JID ( server_name ) if XmppAddr not "}
{"code_tokens": "def verify_jid_against_common_name ( self , jid ) : if not self . common_names : return False for name in self . common_nam "}
{"code_tokens": "def verify_jid_against_srv_name ( self , jid , srv_type ) : srv_prefix = u_ + srv_type + u. srv_prefix_l = "}
{"code_tokens": "def verify_client ( self , client_jid = None , domains = None ) : jids = [ jid for jid in s "}
{"code_tokens": "def from_ssl_socket ( cls , ssl_socket ) : cert = cls ( ) try : data = ssl_socket . getpeercert ( ) e "}
{"code_tokens": "def from_ssl_socket ( cls , ssl_socket ) : try : data = ssl_socket . getpeercert ( True ) except AttributeError : dat "}
{"code_tokens": "def from_der_data ( cls , data ) : logger . debug ( Decoding DER certificate: {0!r} . format ( data ) ) if cls . _cert_asn1_ "}
{"code_tokens": "def _decode_subject ( self , subject ) : self . common_names = [ ] subject_name = [ ] for rdns "}
{"code_tokens": "def _decode_validity ( self , validity ) : not_after = validity . getComponentByName ( 'n "}
{"code_tokens": "def _decode_alt_names ( self , alt_names ) : for alt_name in alt_names : tname = alt_name . getName ( ) comp "}
{"code_tokens": "def from_file ( cls , filename ) : with open ( filename , r ) as pem_file : data = pem . readP "}
{"code_tokens": "def run ( self ) : if self . args . roster_cache and os . path . exists ( self . args . roster "}
{"code_tokens": "def add_handler ( self , handler ) : if not isinstance ( handler , EventHandler "}
{"code_tokens": "def remove_handler ( self , handler ) : with self . lock : if handler in self . handlers : self . handlers . remove ( handler "}
{"code_tokens": "def _update_handlers ( self ) : handler_map = defaultdict ( list ) for i , obj in enumerate ( self . handlers ) : f "}
{"code_tokens": "def dispatch ( self , block = False , timeout = None ) : logger . debug ( dispatching... ) try : "}
{"code_tokens": "def flush ( self , dispatch = True ) : if dispatch : while True : event = self . dispatch ( "}
{"code_tokens": "def challenge ( self , challenge ) : if not challenge : logger . debug ( Empty challenge ) return Failure ( bad-challenge ) "}
{"code_tokens": "def _make_response ( self , nonce , salt , iteration_count ) : self . _salted_passwo "}
{"code_tokens": "def _final_challenge ( self , challenge ) : if self . _finished : return Failure ( extra-challenge ) match = SERVER_FINAL_MESSAG "}
{"code_tokens": "def finish ( self , data ) : if not self . _server_first_message : logger . debug ( "}
{"code_tokens": "def feature_uri ( uri ) : def decorator ( class_ ) : if _pyxmpp_feature_uris not in class_ . __dict__ : class_ "}
{"code_tokens": "def payload_element_name ( element_name ) : def decorator ( klass ) : from . stanzapayload import STAN "}
{"code_tokens": "def stream_element_handler ( element_name , usage_restriction = None ) : def decorator ( func ) : func . _pyxmpp_stream_element_h "}
{"code_tokens": "def _new_from_xml ( cls , xmlnode ) : label = from_utf8 ( xmlnode . prop ( label ) ) child = xmlnode . children value = "}
{"code_tokens": "def add_option ( self , value , label ) : if type ( value ) is list : warnings . warn ( .add_option() accepts single value "}
{"code_tokens": "def _new_from_xml ( cls , xmlnode ) : field_type = xmlnode . prop ( type ) label = fro "}
{"code_tokens": "def add_field ( self , name = None , values = None , field_type = None , label = None , options = None , required = False "}
{"code_tokens": "def _new_from_xml ( cls , xmlnode ) : child = xmlnode . children fields = [ ] while child : if child . "}
{"code_tokens": "def add_item ( self , fields = None ) : item = Item ( fields ) self . items . app "}
{"code_tokens": "def make_submit ( self , keep_types = False ) : result = Form ( submit ) for field in self "}
{"code_tokens": "def __from_xml ( self , xmlnode ) : self . fields = [ ] self . reported_fields = [ ] "}
{"code_tokens": "def register_disco_cache_fetchers ( cache_suite , stream ) : tmp = stream class DiscoInfoCacheFetcher ( DiscoCach "}
{"code_tokens": "def remove ( self ) : if self . disco is None : return self . xmlnode . unlinkNode ( ) oldns = self . xmlnode . ns ( ) ns = s "}
{"code_tokens": "def set_node ( self , node ) : if node is None : if self . xmlnode . hasProp ( node ) : self . xmlnode . unsetProp ( "}
{"code_tokens": "def set_action ( self , action ) : if action is None : if self . xmlnode . hasProp ( action "}
{"code_tokens": "def get_name ( self ) : var = self . xmlnode . prop ( name ) if not var : var = return var . decode ( utf-8 ) "}
{"code_tokens": "def get_category ( self ) : var = self . xmlnode . prop ( category ) if not var : var = ? return var . de "}
{"code_tokens": "def set_category ( self , category ) : if not category : raise ValueError ( Cat "}
{"code_tokens": "def get_type ( self ) : item_type = self . xmlnode . prop ( type ) if not item_type : item_type = ? return item_ "}
{"code_tokens": "def set_type ( self , item_type ) : if not item_type : raise ValueError ( Type is required in DiscoIdentity ) item_type "}
{"code_tokens": "def get_items ( self ) : ret = [ ] l = self . xpath_ctxt . xpathEval ( d:item ) if l is not None : for i in l : ret . append ( "}
{"code_tokens": "def add_item ( self , jid , node = None , name = None , action = None ) : return DiscoItem ( self , jid , node , name , ac "}
{"code_tokens": "def has_item ( self , jid , node = None ) : l = self . xpath_ctxt . xpathEval ( d:item ) if l is None : return False for "}
{"code_tokens": "def get_features ( self ) : l = self . xpath_ctxt . xpathEval ( d:feature ) ret = [ ] for f in l : if f . hasProp ( "}
{"code_tokens": "def has_feature ( self , var ) : if not var : raise ValueError ( var is None ) if '' not in var : exp "}
{"code_tokens": "def add_feature ( self , var ) : if self . has_feature ( var ) : return n = self . xmlnod "}
{"code_tokens": "def remove_feature ( self , var ) : if not var : raise ValueError ( var is None ) if ' "}
{"code_tokens": "def get_identities ( self ) : ret = [ ] l = self . xpath_ctxt . xpathEval ( d:identity ) if l is not None : for i in l : "}
{"code_tokens": "def identity_is ( self , item_category , item_type = None ) : if not item_category : "}
{"code_tokens": "def add_identity ( self , item_name , item_category = None , item_type = None ) : return DiscoIdentity ( self , item_name , item_c "}
{"code_tokens": "def fetch ( self ) : from . . iq import Iq jid , node = self . address iq = Iq ( to_jid = jid , stanza_type = get "}
{"code_tokens": "def __response ( self , stanza ) : try : d = self . disco_class ( stanza . get_query ( ) ) self . got_it ( d ) excep "}
{"code_tokens": "def __error ( self , stanza ) : try : self . error ( stanza . get_error ( ) ) except Proto "}
{"code_tokens": "def make_error_response ( self , cond ) : if self . stanza_type in ( result , error ) : raise ValueError ( Error "}
{"code_tokens": "def make_result_response ( self ) : if self . stanza_type not in ( set , get ) : raise ValueError ( Results may on "}
{"code_tokens": "def _request_tls ( self ) : self . requested = True element = ElementTree . Element ( STARTTLS_TAG ) self . stream . wr "}
{"code_tokens": "def _make_tls_connection ( self ) : logger . debug ( Preparing TLS connection ) if self . settings [ tls_verify_pee "}
{"code_tokens": "def handle_tls_connected_event ( self , event ) : if self . settings [ tls_verify_peer ] : valid = self . "}
{"code_tokens": "def is_certificate_valid ( stream , cert ) : try : logger . debug ( tls_is_certifi "}
{"code_tokens": "def main ( ) : parser = argparse . ArgumentParser ( description = 'XMPP version checker' , parents = [ XMPPSettings . get_arg "}
{"code_tokens": "def handle_authorized ( self , event ) : request_software_version ( self . client , self . target_ "}
{"code_tokens": "def set_item ( self , key , value , timeout = None , timeout_callback = None ) : with self . _lock : "}
{"code_tokens": "def expire ( self ) : with self . _lock : logger . debug ( expdict.expire. timeouts: {0!r} . format ( self . _tim "}
{"code_tokens": "def _expire_item ( self , key ) : ( timeout , callback ) = self . _timeouts [ key ] now = time . time ( ) if timeout <= now : "}
{"code_tokens": "def _decode_attributes ( self ) : try : from_jid = self . _element . get ( 'from' ) if from_jid : "}
{"code_tokens": "def _decode_error ( self ) : error_qname = self . _ns_prefix + error for child in self . _element : if child "}
{"code_tokens": "def set_payload ( self , payload ) : if isinstance ( payload , ElementClass ) : self . _payload = [ XM "}
{"code_tokens": "def add_payload ( self , payload ) : if self . _payload is None : self . decode_payload ( ) if isinstance ( paylo "}
{"code_tokens": "def get_all_payload ( self , specialize = False ) : if self . _payload is None : self . decode_payload ( specialize ) elif speci "}
{"code_tokens": "def get_payload ( self , payload_class , payload_key = None , specialize = False ) : if self . _p "}
{"code_tokens": "def element_to_unicode ( element ) : if hasattr ( ElementTree , 'tounicode' ) : return ElementTree . tounicode ( element "}
{"code_tokens": "def bind ( self , stream , resource ) : self . stream = stream stanza = Iq ( stanza_type "}
{"code_tokens": "def _bind_success ( self , stanza ) : payload = stanza . get_payload ( ResourceBindingPayload ) j "}
{"code_tokens": "def serialize ( element ) : if getattr ( _THREAD , serializer , None ) is None : _ "}
{"code_tokens": "def add_prefix ( self , namespace , prefix ) : if prefix == xml and namespace != XML_NS : raise "}
{"code_tokens": "def emit_head ( self , stream_from , stream_to , stream_id = None , version = u'1.0' , language = None ) : self . _root "}
{"code_tokens": "def _split_qname ( self , name , is_element ) : if name . startswith ( u{ ) : namespace , name = n "}
{"code_tokens": "def _make_prefix ( self , declared_prefixes ) : used_prefixes = set ( self . _prefixes . values ( "}
{"code_tokens": "def _make_prefixed ( self , name , is_element , declared_prefixes , declarations ) : namespace , name = self . _split_qnam "}
{"code_tokens": "def _make_ns_declarations ( declarations , declared_prefixes ) : result = [ ] for namespace , prefix in declarations . items ( "}
{"code_tokens": "def _emit_element ( self , element , level , declared_prefixes ) : declarations = { } declared_prefixes = dict ( declared_prefixes "}
{"code_tokens": "def emit_stanza ( self , element ) : if not self . _head_emitted : raise RuntimeError ( .emit_head() must be called first. "}
{"code_tokens": "def filter_mechanism_list ( mechanisms , properties , allow_insecure = False , server_side = False ) : result "}
{"code_tokens": "def error ( self , stanza ) : err = stanza . get_error ( ) self . __logger . debug ( Error from: % "}
{"code_tokens": "def update_presence ( self , presence ) : self . presence = MucPresence ( presence ) t = presence . get_type ( ) "}
{"code_tokens": "def get_user ( self , nick_or_jid , create = False ) : if isinstance ( nick_or_jid , JID ) : if not nick_or_jid . r "}
{"code_tokens": "def set_stream ( self , stream ) : _unused = stream if self . joined and self . handler : self . handler "}
{"code_tokens": "def join ( self , password = None , history_maxchars = None , history_maxstanzas = None , history_se "}
{"code_tokens": "def leave ( self ) : if self . joined : p = MucPresence ( to_jid = self . room_jid , stanza_type = unavailable ) self "}
{"code_tokens": "def send_message ( self , body ) : m = Message ( to_jid = self . room_jid . bare ( ) , stanza_type = grou "}
{"code_tokens": "def set_subject ( self , subject ) : m = Message ( to_jid = self . room_jid . bare ( ) , stanza_type = "}
{"code_tokens": "def change_nick ( self , new_nick ) : new_room_jid = JID ( self . room_jid . node , self . room_jid . domain , new_nick ) "}
{"code_tokens": "def get_room_jid ( self , nick = None ) : if nick is None : return self . room_j "}
{"code_tokens": "def process_configuration_form_success ( self , stanza ) : if stanza . get_query_ns ( ) != MUC_OWNER "}
{"code_tokens": "def request_configuration_form ( self ) : iq = Iq ( to_jid = self . room_jid . bare ( ) , stanza_type = get ) "}
{"code_tokens": "def divide ( a , b ) : unused_variable = 42 for _ in range ( 5 ) : pass "}
{"code_tokens": "def process_configuration_success ( self , stanza ) : _unused = stanza self . configured = True self "}
{"code_tokens": "def configure_room ( self , form ) : if form . type == cancel : return None elif form . type != submit "}
{"code_tokens": "def set_stream ( self , stream ) : self . jid = stream . me self . stream = stream for r in self . rooms . values ( ) : r . "}
{"code_tokens": "def set_handlers ( self , priority = 10 ) : self . stream . set_message_handler ( "}
{"code_tokens": "def join ( self , room , nick , handler , password = None , history_maxchars = None , history_maxstanzas = Non "}
{"code_tokens": "def forget ( self , rs ) : try : del self . rooms [ rs . room_jid . bare ( ) . as_un "}
{"code_tokens": "def __groupchat_message ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_ "}
{"code_tokens": "def __error_message ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get "}
{"code_tokens": "def __presence_error ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . "}