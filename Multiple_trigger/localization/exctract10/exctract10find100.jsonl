{"code_tokens": "def run_cutadapt ( data , subsamples , lbview ) : start = time . time ( ) printstr = "}
{"code_tokens": "def cleanup_tempfiles ( data ) : tmps1 = glob . glob ( os . path . join ( data . tmpdir , *.fa ) ) "}
{"code_tokens": "def fill_boot ( seqarr , newboot , newmap , spans , loci ) : cidx = 0 for i in xrange ( loci . shape [ 0 ] ) : x "}
{"code_tokens": "def _run_qmc ( self , boot ) : self . _tmp = os . path . join ( self . dirs , .tmpwtre ) cmd = [ ip . bins . qmc , qrtt= + sel "}
{"code_tokens": "def save_json ( data ) : datadict = OrderedDict ( [ ( _version , data . __dict__ [ _version ] "}
{"code_tokens": "def concatclusts ( outhandle , alignbits ) : with gzip . open ( outhandle , 'wb' ) as out : "}
{"code_tokens": "def compute_tree_stats ( self , ipyclient ) : names = self . samples if self . params . nboots : ful "}
{"code_tokens": "def run2 ( data , samples , force , ipyclient ) : data . dirs . edits = os . path . join ( os . path . r "}
{"code_tokens": "def singlecat ( data , sample , bseeds , sidx , nloci ) : LOGGER . info ( in single cat here "}
{"code_tokens": "def dask_chroms ( data , samples ) : h5s = [ os . path . join ( data . dirs . across , s . name + .tm "}

{"code_tokens": "def n_choose_k ( n , k ) : return int ( reduce ( MUL , ( Fraction ( n - i , i + 1 ) for i in range ( k ) ) , 1 ) ) "}
{"code_tokens": "def _load_existing_results ( self , name , workdir ) : path = os . path . realpath ( os . path . join ( self . "}
{"code_tokens": "def random_product ( iter1 , iter2 ) : pool1 = tuple ( iter1 ) pool2 = tuple ( iter2 ) ind1 = random . sample ( pool1 , "}
{"code_tokens": "def _renamer ( self , tre ) : names = tre . get_leaves ( ) for name in names : name . name = sel "}
{"code_tokens": "def _countmatrix ( lxs ) : share = np . zeros ( ( lxs . shape [ 0 ] , lxs . shape [ 0 ] ) ) names = range ( lxs . shape [ 0 ] ) "}
{"code_tokens": "def get_nloci ( data ) : bseeds = os . path . join ( data . dirs . across , data . na "}
{"code_tokens": "def concat_reads ( data , subsamples , ipyclient ) : if any ( [ len ( i . files . fa "}
{"code_tokens": "def nworker ( data , smpchunk , tests ) : with h5py . File ( data . database . input , 'r' ) "}
{"code_tokens": "def importvcf ( vcffile , locifile ) : try : with open ( invcffile , 'r' ) as invcf : f "}
{"code_tokens": "def _parse_00 ( ofile ) : with open ( ofile ) as infile : arr = np . array ( [  ] + infile . read ( ) . split ( "}

{"code_tokens": "def fill_boot ( seqarr , newboot , newmap , spans , loci ) : cidx = 0 for i in xrange ( loci . shape [ 0 ] ) : x "}
{"code_tokens": "def sub_build_clustbits ( data , usort , nseeds ) : LOGGER . info ( loading full _catcons file into memory ) allcons = { } "}
{"code_tokens": "def _load_existing_results ( self , name , workdir ) : path = os . path . realpath ( os . path . join ( self . "}
{"code_tokens": "def singlecat ( data , sample , bseeds , sidx , nloci ) : LOGGER . info ( in single cat here "}
{"code_tokens": "def _parse_names ( self ) : self . samples = [ ] with iter ( open ( self . files . data , 'r' ) ) as infile : infile "}
{"code_tokens": "def cleanup_tempfiles ( data ) : tmps1 = glob . glob ( os . path . join ( data . tmpdir , *.fa ) ) "}
{"code_tokens": "def save_json2 ( data ) : datadict = OrderedDict ( [ ( outfiles , data . __dict__ [ outfiles "}
{"code_tokens": "def save_json ( data ) : datadict = OrderedDict ( [ ( _version , data . __dict__ [ _version ] "}
{"code_tokens": "def get_nloci ( data ) : bseeds = os . path . join ( data . dirs . across , data . na "}
{"code_tokens": "def parse_single_results ( data , sample , res1 ) : sample . stats_dfs . s2 [ trim_adapter_bp_read1 ] = 0 sample . stats_dfs . s "}

{"code_tokens": "def concatclusts ( outhandle , alignbits ) : with gzip . open ( outhandle , 'wb' ) as out : "}
{"code_tokens": "def _byteify ( data , ignore_dicts = False ) : if isinstance ( data , unicode ) : return data . encode ( u "}
{"code_tokens": "def summarize_results ( self , individual_results = False ) : if ( not self . params . infer_delimit ) & ( no "}
{"code_tokens": "def save_json ( data ) : datadict = OrderedDict ( [ ( _version , data . __dict__ [ _version ] "}
{"code_tokens": "def run_cutadapt ( data , subsamples , lbview ) : start = time . time ( ) printstr = "}
{"code_tokens": "def save_json2 ( data ) : datadict = OrderedDict ( [ ( outfiles , data . __dict__ [ outfiles "}
{"code_tokens": "def n_choose_k ( n , k ) : return int ( reduce ( MUL , ( Fraction ( n - i , i + 1 ) for i in range ( k ) ) , 1 ) ) "}
{"code_tokens": "def random_product ( iter1 , iter2 ) : pool1 = tuple ( iter1 ) pool2 = tuple ( iter2 ) ind1 = random . sample ( pool1 , "}
{"code_tokens": "def concat_reads ( data , subsamples , ipyclient ) : if any ( [ len ( i . files . fa "}
{"code_tokens": "def _renamer ( self , tre ) : names = tre . get_leaves ( ) for name in names : name . name = sel "}
{"code_tokens": "def dask_chroms ( data , samples ) : h5s = [ os . path . join ( data . dirs . across , s . name + .tm "}
{"code_tokens": "def encode ( self , obj ) : def hint_tuples ( item ) : if isinstance ( item , tuple ) : re "}
{"code_tokens": "def build_clustbits ( data , ipyclient , force ) : if os . path . exists ( data . tmpdir ) : shutil . rm "}
{"code_tokens": "def assembly_cleanup ( data ) : data . stats_dfs . s2 = data . _build_stat ( s2 ) data . stats_files . s2 "}
{"code_tokens": "def _dump_qmc ( self ) : io5 = h5py . File ( self . database . output , 'r' ) self . files . qdump = os . path . join ( self . d "}
{"code_tokens": "def inserted_indels ( indels , ocatg ) : newcatg = np . zeros ( ocatg . shape , dtype = np . uint32 ) for "}
{"code_tokens": "def random_product ( iter1 , iter2 ) : pool1 = tuple ( iter1 ) pool2 = tuple ( iter2 ) ind1 = random . sample ( pool1 , "}
{"code_tokens": "def _countmatrix ( lxs ) : share = np . zeros ( ( lxs . shape [ 0 ] , lxs . shape [ 0 ] ) ) names = range ( lxs . shape [ 0 ] ) "}
{"code_tokens": "def concatclusts ( outhandle , alignbits ) : with gzip . open ( outhandle , 'wb' ) as out : "}
{"code_tokens": "def sort_seeds ( uhandle , usort ) : cmd = [ sort , -k , 2 , uhandle , -o , usort ] proc = sps . Popen ( cmd , cl "}

{"code_tokens": "def _parse_names ( self ) : self . samples = [ ] with iter ( open ( self . files . data , 'r' ) ) as infile : infile "}
{"code_tokens": "def parse_single_results ( data , sample , res1 ) : sample . stats_dfs . s2 [ trim_adapter_bp_read1 ] = 0 sample . stats_dfs . s "}
{"code_tokens": "def get_spans ( maparr , spans ) : bidx = 1 spans = np . zeros ( ( maparr [ - 1 , 0 ] , 2 ) , np . "}
{"code_tokens": "def sub_build_clustbits ( data , usort , nseeds ) : LOGGER . info ( loading full _catcons file into memory ) allcons = { } "}
{"code_tokens": "def chunk_to_matrices ( narr , mapcol , nmask ) : mats = np . zeros ( ( 3 , 16 , 16 ) , dtype = np . "}
{"code_tokens": "def importvcf ( vcffile , locifile ) : try : with open ( invcffile , 'r' ) as invcf : f "}
{"code_tokens": "def _byteify ( data , ignore_dicts = False ) : if isinstance ( data , unicode ) : return data . encode ( u "}
{"code_tokens": "def random_product ( iter1 , iter2 ) : pool1 = tuple ( iter1 ) pool2 = tuple ( iter2 ) ind1 = random . sample ( pool1 , "}
{"code_tokens": "def count_snps ( mat ) : snps = np . zeros ( 4 , dtype = np . uint32 ) snps [ 0 ] = np . uint32 ( mat [ 0 , "}
{"code_tokens": "def build_tmp_h5 ( data , samples ) : snames = [ i . name for i in samples ] snames . sort ( ) uhandle = os . path . joi "}

{"code_tokens": "def shuffle_cols ( seqarr , newarr , cols ) : for idx in xrange ( cols . shape [ 0 ] ) : newarr [ : , idx ] = seqarr "}
{"code_tokens": "def get_spans ( maparr , spans ) : bidx = 1 spans = np . zeros ( ( maparr [ - 1 , 0 ] , 2 ) , np . "}
{"code_tokens": "def _dump_qmc ( self ) : io5 = h5py . File ( self . database . output , 'r' ) self . files . qdump = os . path . join ( self . d "}
{"code_tokens": "def importvcf ( vcffile , locifile ) : try : with open ( invcffile , 'r' ) as invcf : f "}
{"code_tokens": "def save_json ( data ) : datadict = OrderedDict ( [ ( _version , data . __dict__ [ _version ] "}
{"code_tokens": "def build_clustbits ( data , ipyclient , force ) : if os . path . exists ( data . tmpdir ) : shutil . rm "}
{"code_tokens": "def _countmatrix ( lxs ) : share = np . zeros ( ( lxs . shape [ 0 ] , lxs . shape [ 0 ] ) ) names = range ( lxs . shape [ 0 ] ) "}
{"code_tokens": "def parse_single_results ( data , sample , res1 ) : sample . stats_dfs . s2 [ trim_adapter_bp_read1 ] = 0 sample . stats_dfs . s "}
{"code_tokens": "def sub_build_clustbits ( data , usort , nseeds ) : LOGGER . info ( loading full _catcons file into memory ) allcons = { } "}
{"code_tokens": "def save_json2 ( data ) : datadict = OrderedDict ( [ ( outfiles , data . __dict__ [ outfiles "}

{"code_tokens": "def shuffle_cols ( seqarr , newarr , cols ) : for idx in xrange ( cols . shape [ 0 ] ) : newarr [ : , idx ] = seqarr "}
{"code_tokens": "def get_nloci ( data ) : bseeds = os . path . join ( data . dirs . across , data . na "}
{"code_tokens": "def _countmatrix ( lxs ) : share = np . zeros ( ( lxs . shape [ 0 ] , lxs . shape [ 0 ] ) ) names = range ( lxs . shape [ 0 ] ) "}
{"code_tokens": "def sub_build_clustbits ( data , usort , nseeds ) : LOGGER . info ( loading full _catcons file into memory ) allcons = { } "}
{"code_tokens": "def concat_reads ( data , subsamples , ipyclient ) : if any ( [ len ( i . files . fa "}
{"code_tokens": "def sort_seeds ( uhandle , usort ) : cmd = [ sort , -k , 2 , uhandle , -o , usort ] proc = sps . Popen ( cmd , cl "}
{"code_tokens": "def count_snps ( mat ) : snps = np . zeros ( 4 , dtype = np . uint32 ) snps [ 0 ] = np . uint32 ( mat [ 0 , "}
{"code_tokens": "def encode ( self , obj ) : def hint_tuples ( item ) : if isinstance ( item , tuple ) : re "}
{"code_tokens": "def n_choose_k ( n , k ) : return int ( reduce ( MUL , ( Fraction ( n - i , i + 1 ) for i in range ( k ) ) , 1 ) ) "}
{"code_tokens": "def _dump_qmc ( self ) : io5 = h5py . File ( self . database . output , 'r' ) self . files . qdump = os . path . join ( self . d "}

{"code_tokens": "def dask_chroms ( data , samples ) : h5s = [ os . path . join ( data . dirs . across , s . name + .tm "}
{"code_tokens": "def concat_reads ( data , subsamples , ipyclient ) : if any ( [ len ( i . files . fa "}
{"code_tokens": "def run2 ( data , samples , force , ipyclient ) : data . dirs . edits = os . path . join ( os . path . r "}
{"code_tokens": "def get_targets ( ipyclient ) : hosts = [ ] for eid in ipyclient . ids : engine = ipyclient [ eid ] if no "}
{"code_tokens": "def make ( data , samples ) : invcffile = os . path . join ( data . dirs . consens , data . name + . "}
{"code_tokens": "def assembly_cleanup ( data ) : data . stats_dfs . s2 = data . _build_stat ( s2 ) data . stats_files . s2 "}
{"code_tokens": "def resolve_ambigs ( tmpseq ) : for ambig in np . uint8 ( [ 82 , 83 , 75 , 87 , 89 , 7 "}
{"code_tokens": "def get_nloci ( data ) : bseeds = os . path . join ( data . dirs . across , data . na "}
{"code_tokens": "def _parse_00 ( ofile ) : with open ( ofile ) as infile : arr = np . array ( [  ] + infile . read ( ) . split ( "}
{"code_tokens": "def get_spans ( maparr , spans ) : bidx = 1 spans = np . zeros ( ( maparr [ - 1 , 0 ] , 2 ) , np . "}

{"code_tokens": "def _parse_00 ( ofile ) : with open ( ofile ) as infile : arr = np . array ( [  ] + infile . read ( ) . split ( "}
{"code_tokens": "def paramname ( param = ) : try : name = pinfo [ str ( param ) ] [ 0 ] . strip ( ) . split (  ) [ 1 ] except ( KeyE "}
{"code_tokens": "def save_json ( data ) : datadict = OrderedDict ( [ ( _version , data . __dict__ [ _version ] "}
{"code_tokens": "def dask_chroms ( data , samples ) : h5s = [ os . path . join ( data . dirs . across , s . name + .tm "}
{"code_tokens": "def count_seeds ( usort ) : with open ( usort , 'r' ) as insort : cmd1 = [ cut , - "}
{"code_tokens": "def build_clustbits ( data , ipyclient , force ) : if os . path . exists ( data . tmpdir ) : shutil . rm "}
{"code_tokens": "def count_snps ( mat ) : snps = np . zeros ( 4 , dtype = np . uint32 ) snps [ 0 ] = np . uint32 ( mat [ 0 , "}
{"code_tokens": "def concatclusts ( outhandle , alignbits ) : with gzip . open ( outhandle , 'wb' ) as out : "}
{"code_tokens": "def nworker ( data , smpchunk , tests ) : with h5py . File ( data . database . input , 'r' ) "}
{"code_tokens": "def encode ( self , obj ) : def hint_tuples ( item ) : if isinstance ( item , tuple ) : re "}



{"ranked_substrings": [[" os . path . join ( data . ", 27, 2], [" = os . path . join ( ", 22, 2], [" ( data , samples ", 18, 2], [" , ipyclient ) : ", 17, 2], [" ( os . path . ", 15, 2], [" data . dirs . ", 15, 2], ["t ( data , s", 12, 2], [" ( data ) : ", 12, 2], ["samples , ", 10, 2], [" ( self , ", 10, 2]]}
{"ranked_substrings": [[" os . path . join ( ", 20, 2], [" = os . path . ", 15, 2], [" : with open ( ", 15, 2], [" ( data . d", 11, 2], [" ( data , s", 11, 2], [" ( self , ", 10, 2], [" ) : with ", 10, 2], [" range ( ", 9, 2], [" names = ", 9, 2], [" , 'r' ) ", 9, 2]]}
{"ranked_substrings": [["files ", 6, 3], [" ( data ) : datadict = OrderedDict ( [ ( ", 41, 2], [" os . path . join ( data . ", 27, 2], [" ( os . path . join ( ", 22, 2], [" ) : LOGGER . info ( ", 21, 2], [" , data . __dict__ [ ", 21, 2], [" ( data , sample , ", 19, 2], [" = os . path . ", 15, 2], ["def save_json", 13, 2], ["ts ( data , ", 12, 2]]}
{"ranked_substrings": [[" ( data ) : datadict = OrderedDict ( [ ( ", 41, 2], [" ( data , subsamples , ", 23, 2], [" , data . __dict__ [ ", 21, 2], ["ts = False ) : if ", 18, 2], ["def save_json", 13, 2], [" ) : return ", 12, 2], ["def concat", 10, 2], [" ( self , ", 10, 2], [" ) for ", 7, 2], [" ( out", 6, 2]]}
{"ranked_substrings": [["s ) : ", 6, 3], [" os . path . join ( ", 20, 2], [" = np . zeros ( ", 16, 2], ["s ( data , ", 11, 2], [" ( data . ", 10, 2], ["e ( self ", 9, 2], [" . shape ", 9, 2], ["e = np . ", 9, 2], ["handle , ", 9, 2], [" ) : if ", 8, 2]]}
{"ranked_substrings": [["s ) : ", 6, 3], ["s = np . zeros ( ( ", 19, 2], [" ( data , sample", 16, 2], [" , dtype = np . ", 16, 2], ["ts ( data , ", 12, 2], [" ( data , u", 11, 2], [" , 'r' ) ", 9, 2], [" . sample", 9, 2], [" samples ", 9, 2], [" ) as in", 8, 2]]}
{"ranked_substrings": [["s ) : ", 6, 4], [" ( data ) : datadict = OrderedDict ( [ ( ", 41, 2], ["build_clustbits ( data , ", 25, 2], [" , data . __dict__ [ ", 21, 2], ["s . shape [ 0 ] ) ", 18, 2], [" = np . zeros ( ( ", 18, 2], [" os . path . ", 13, 2], ["def save_json", 13, 2], [" , 'r' ) ", 9, 2], ["range ( ", 8, 2]]}
{"ranked_substrings": [["s ) : ", 6, 3], ["seeds ", 6, 3], ["t ) : ", 6, 3], [" = os . path . join ( ", 22, 2], ["s . shape [ 0 ] ) ", 18, 2], [" = np . zeros ( ", 16, 2], ["s ( data , ", 11, 2], [" . files . ", 11, 2], ["e = np . ", 9, 2], [" range ( ", 9, 2]]}
{"ranked_substrings": [["def get_", 8, 3], [" os . path . join ( data . dirs . across , ", 43, 2], [" = os . path . join ( data . dirs . ", 36, 2], ["s = os . path . join ( ", 23, 2], [" ( data , samples ) : ", 22, 2], [" , ipyclient ) : ", 17, 2], ["s , data . na", 13, 2], ["s ( data , s", 12, 2], [" ) : data . ", 12, 2], [" ( data ) : ", 12, 2]]}
{"ranked_substrings": [[" ) : with open ( ", 17, 2], [" ( ) . split ( ", 15, 2], [" os . path . ", 13, 2], ["ts ) : with ", 12, 2], ["s ( data , ", 11, 2], [" ( data . d", 11, 2], [" ( data , s", 11, 2], ["def count_s", 11, 2], ["t , 'r' ) ", 10, 2], [" open ( o", 9, 2]]}
